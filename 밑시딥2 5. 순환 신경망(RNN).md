# 밑시딥2 📂5. 순환 신경망(RNN)

지금까지 배운 신경망은 흐름이 단방향인 신경망인 **피드포워드(Feed Forward)** 유형이다. 

피드포워드는 시계열 데이터를 잘 다루지 못해 패턴을 충분히 학습할 수 없는 문제점이 있다.

그래서 **순환 신경망(Recurrent Neural Network)**이 등장한다.

이번 시간에는 피드포워드 신경망의 문제점을 알아보고, RNN의 장점과 구조를 알아보자.



## 1. 확률과 언어 모델

word2vec를 복습한 다음 자연어에 관한 현상을 확률을 사용해 기술하고 언어를 확률로 다루는 언어 모델에 대해 살펴보자.



### 1.1 word2vec을 확률 관점에서 바라보다

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-1.png" alt="fig 5-1" style="zoom:50%;" /> 

여기서 W_t-1과 W_t+1이 주어졌을 때 타깃이 W_t가 될 확률을 수식으로 나타내면 아래와 같다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-1.png" alt="e 5-1" style="zoom:50%;" /> 

그렇다면 윈도우를 좌우 비대칭으로 설정하면 어떻게 되는지 생각해보자. 맥락을 왼쪽 윈도우만으로 한정해보면 확률 수식과 교차 엔트로피 오차는 아래와 같다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-2.png" alt="e 5-2" style="zoom:50%;" /> 

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-3.png" alt="e 5-3" style="zoom:50%;" /> 

CBOW 모델의 목적은 맥락으로부터 타깃을 정확히 추측하는 것으로 그 부산물로 단어의 의미가 인코딩된 `단어의 분산 표현`을 얻을 수 있다.

그렇다면 저 확률은 어디에 쓰이는 것일까라는 물음에서 <u>'언어 모델'</u>이 등장한다.



### 1.2 언어 모델

> 단어 나열에 확률을 부여

기계 번역과 음성 인식이 대표적인 언어 모델 사용의 예이다. 



언어 모델을 수식으로 설명하면 m개의 단어로 된 문장을 예로 들어본다.

단어가 w1, ... , w_m의 순서로 출현할 확률을 동시 확률 P(w1,...,w_m)라고 하면 사후 확률을 사용하여 다음과 같이 쓸수 있다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-4.png" alt="e 5-4" style="zoom:50%;" /> 

여기서 파이(Π)는 모든 원소를 곱하는 '총곱'을 뜻한다. 위의 결과는 확률의 곱셈정리로 유도할 수 있다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-5.png" alt="e 5-5" style="zoom:50%;" />

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-6.png" alt="e 5-6" style="zoom:50%;" />

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-7.png" alt="e 5-7" style="zoom:50%;" />



🔥 사후 확률은 타깃단어보다 왼쪽에 있는 모든 단어를 맥락으로 했을 때의 확률이다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-3.png" alt="fig 5-3" style="zoom:50%;" />



### 1.3 CBOW 모델을 언어 모델로?

맥락을 왼쪽 2개의 단어로 한정하여 근사적으로 나타내면 아래와 같다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-8.png" alt="e 5-8" style="zoom:50%;" />

맥락의 크기는 임의 길이로 설정할 수 있지만 결국 특정 길이로 **`고정`**된다.

왼쪽 10개의 단어를 맥락으로 CBOW모델을 만든다면 맥락보다 더 왼쪽에 있는 단어의 정보는 무시된다.

따라서 CBOW 모델에서는 맥락 안의 단어 순서가 무시된다는 한계가 있다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />



보다시피 맥락의 단어들은 은닉층에서 더해져 맥락의 순서는 무시된다. 

따라서 맥락의 단어 순서도 고려한 모델이 바람직할 것이다.

👉 여기서 맥락의 정보를 기억하는 메커니즘을 갖춘 RNN이 등장한다.

RNN을 사용하면 시계열 데이터를 처리할 수 있다.



---





## 2. RNN이란

> Recurrent Neural Network로 순환하는 신경망으로 해석된다.



### 2.1 순환하는 신경망

**`순환`**이란 반복해서 될돌아간다는 의미로 <u>닫힌 경로</u>가 필요하다.

RNN의 특징은 데이터가 순환한다는 것이며, 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신될 수 있다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-6.png" alt="fig 5-6" style="zoom:50%;" /> 

여기서 t는 시각으로 문장을 예로들면 각 단어의 분산 표현이 X_t이며 분산 표현이다.

지면 관계상 RNN 계층을 그리는 방식을 다음과 같이 변경한다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-7.png" alt="fig 5-7" style="zoom:50%;" />



### 2.2 순환 구조 펼치기

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-8.png" alt="fig 5-8" style="zoom:50%;" />

각 시각의 RNN 계층은 그 계층으로의 입력과 1개 전의 RNN 계층으로부터의 출력을 받는다.

이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 수식은 아래와 같다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-9.png" alt="e 5-9" style="zoom:50%;" /> 

W_x : x를 출력 h로 변환하는 가중치

W_h : 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치

이를 보면 현재의 출력(h_t)은 한 시각 이전 출력(h_t-1)에 기초해 계산되며 RNN은 h라는 상태를 가지고 계속 갱신된다고 해석할 수 있다.



### 2.3 BPTT

> Backpropagation Through Time(BPTT) : 시간 방향으로 펼친 신경망의 <u>오차역전파법</u>

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-10.png" alt="fig 5-10" style="zoom:50%;" />

순환 구조를 펼친 후의 RNN에서는 오차역전파법을 적용할 수 있다.

하지만 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가하고

기울기가 불안정해지는 문제점이 있다.



### 2.4 Truncated BPTT

> 신경망 연결을 적당한 길이로 끊어 작은 신경망 여러 개로  만들어 오차역전파법을 수행

정확히는 역전파의 연결만 끊고 순전파의 연결은 그대로 유지해야한다.

구체적인 예를 들어보자.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-11.png" alt="fig 5-11" style="zoom:50%;" />

지금까지 본 신경망은 미니배치 학습을 위해 데이터를 무작위로 선택했지만

RNN에서는 데이터를 순서대로 입력해야한다.



<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-12.png" alt="fig 5-12" style="zoom:50%;" /> 

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-13.png" alt="fig 5-13" style="zoom:50%;" /> 

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-14.png" alt="fig 5-14" style="zoom:50%;" />

이처럼 순전파의 연결을 유지하면서 블록 단위로 오차역전파법을 적용할 수 있다.



### 2.5 Truncated BPTT의 미니배치 학습

그렇다면 미니배치 학습 시 각각의 미니배치가 어떤 식으로 이루어지는가

데이터를 주는 `시작 위치`를 각 미니배치의 시작 위치로 **옮겨줘야**한다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-15.png" alt="fig 5-15" style="zoom:50%;" /> 

각 미니배치의 시작 위치를 오프셋으로 옮겨준 후 순서대로 제공하면 된다.

또한 데이터를 순서대로 입력하다가 끝에 도달하면 다시 처음부터 입력한다.



---





## 3. RNN 구현

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-17.png" alt="fig 5-17" style="zoom:50%;" /> 

xs, hs와 같이 각각 하나로 묶으면 옆으로 늘어선 일련의 계층을 하나의 계층으로 간주할 수 있다.

T개 단계분의 작업을 한꺼번에 처리하는 계층을 'Time RNN 계층'이다.

🔥 PYTHON 구현에서는 RNN 한 단계를 처리하는 클래스를 RNN이라 부르며 T개 단계의 처리를 한꺼번에 수행하는 계층을 TimeRNN의 클래스로 완성시킨다.



### 3.1  RNN 계층 구현

RNN의 순저파 식은 아래와 같다. 여기서 데이터를 미니배치로 모아 처리하여 X_t, h_t에는 각 샘플 데이터를 행 방향에 저장한다

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-10.png" alt="e 5-10" style="zoom:50%;" />



- RNN 클래스의 초기화와 순전파 메서드

  ```python
  class RNN:
      # 초기화 메서드
    def __init__(self, Wx, Wh, b):
          # 가중치 2개, 편향
          self.params = [Wx, Wh, b]
          self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
          self.cache = None # 역전파 계산 시 중간 데이터를 담을 변수
  
      # 입력 x와 전 RNN 계층에서 넘어오는 h
      def forward(self, x, h_prev):
          Wx, Wh, b = self.params
          t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b
          h_next = np.tanh(t)
  
          self.cache = (x, h_prev, h_next)
          return h_next
  ```
  
- RNN 클래스의 역전파 메서드

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-20-1616809736432.png" alt="fig 5-20" style="zoom:50%;" />

  ```python
  def backward(self, dh_next):
      Wx, Wh, b = self.params
      x, h_prev, h_next = self.cache
  
      dt = dh_next * (1 - h_next ** 2)
      db = np.sum(dt, axis=0)
      dWh = np.dot(h_prev.T, dt)
      dh_prev = np.dot(dt, Wh.T)
      dWx = np.dot(x.T, dt)
      dx = np.dot(dt, Wx.T)
  
      self.grads[0][...] = dWx
      self.grads[1][...] = dWh
      self.grads[2][...] = db
  
      return dx, dh_prev
  ```

  

### 3.2 Time RNN 계층 구현

> Time RNN 계층은 T개의 RNN계층으로 구성된다.

RNN 계층의 은닉 상태 h 를 인스턴스 변수로 유지한다. 

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-22.png" alt="fig 5-22" style="zoom:50%;" />

- Time RNN 초기화 메서드

  ```python
  class TimeRNN:
      def __init__(self, Wx, Wh, b, stateful=False):
          # stateful : 은닉 상태를 유지할 것인지 => 순전파를 끈힞 않고 전파할 것인가
          self.params = [Wx, Wh, b]
          self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
          self.layers = None # 다수의 RNN 계층을 리스트로 저장하는 변수
  
          self.h, self.dh = None, None
          # h : forward 메서드를 불렀을 떄의 마지막 RNN 계층의 은닉 상태를 저장
          # dh : backward 메서드를 불렀을 때 하나 앞 블록의 은닉 상태의 기울기를 저장
          self.stateful = stateful
          
      # 확장성 고려 메서드
      # 은닉 상태를 설정하는 메서드
      def set_state(self, h):
          self.h = h
  
      # 은닉 상태를 초기화 
      def reset_state(self):
          self.h = None
  ```

  

- Time RNN 순전파

  ```python
  def forward(self, xs):
      # xs : T개 분량의 시계열 데이터를 하나로 모은 것
      Wx, Wh, b = self.params
      N, T, D = xs.shape
      # N : 미니배치 크기, D :입력 벡터의 차원 수
      D, H = Wx.shape
  
      self.layers = []
      hs = np.empty((N, T, H), dtype='f')
      # 출력값 담을 변수
  
      if not self.stateful or self.h is None:
          self.h = np.zeros((N, H), dtype='f')
          # 은닉 상태 h는 처음 호출 시(self.h is None)에는 원소가 모두 0인 영행렬로 초기화
  
      for t in range(T):
          layer = RNN(*self.params)
          # RNN 계층이 각 시각 t의 은닉 상태 h를 계산하고 이를 hs에 해당 인덱스의 값으로 설정한다.
          self.h = layer.forward(xs[:, t, :], self.h)
          hs[:, t, :] = self.h
          self.layers.append(layer)
          
      return hs
  ```

  

- Time RNN 역전파

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-23.png" alt="fig 5-23" style="zoom:50%;" />

  Truncated BPTT를 수행하기 때문에 이 블록의 이전 시각 역전파는 필요하지 않다.

  하지만 seq2seq에 필요하기 때문에 이전 시각의 은닉 상태 기울기는 따로 저정한다.

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-24.png" alt="fig 5-24" style="zoom:50%;" />

  기울기는 '한 시각 뒤(미래)' 계층으로부터의 기울기가 전해진다.

  🔥 RNN 계층의 순전파에서는 출력이 2개로 분기된다!! 순전파 시 분기했을 경우, 그 역전파에서는 각 기울기가 합산되어 전해진다. 따라서 합산된 기울기 (dh_t + dh_next)가 입력된다.

  ```python
      def backward(self, dhs):
          Wx, Wh, b = self.params
          N, T, H = dhs.shape
          D, H = Wx.shape
  
          dxs = np.empty((N, T, D), dtype='f')
          # 하류로 보낼 기울기를 담을 변수 생성 
          
          dh = 0
          grads = [0, 0, 0]
          for t in reversed(range(T)):
              layer = self.layers[t]
              dx, dh = layer.backward(dhs[:, t, :] + dh)
              dxs[:, t, :] = dx
  
              for i, grad in enumerate(layer.grads):
                  grads[i] += grad # 각 RNN 계층의 가중치 기울기를 합산한다
  
          for i, grad in enumerate(grads):
              self.grads[i][...] = grad 
          self.dh = dh
  
          return dxs
  ```

  

---





## 4. 시계열 데이터 처리 계층 구현

> RNN을 사용하여 '언어 모델'을 구현해보자.
>
> RNNLM : RNN Language Model



### 4.1 RNNLM의 전체 그림

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-25.png" alt="fig 5-25" style="zoom:50%;" />

왼쪽은 RNNLM의 계층 구성, 오른쪽은 이를 시간축으로 펼친 신경망

여기서 Ebedding 계층에서는 단어 id를 단어의 분산 표현으로 변환하고 그 분산 표현이 RNN 계층으로 입력된다.

RNN 계층이 위로 출력한 은닉 상태는 Affine계층을 거쳐 Softmax계층으로 전해진다.

구체적인 예로 'You say goodbye and I say hello'를 입력데이터로 사용해보자.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-26.png" alt="fig 5-26" style="zoom:50%;" />

🔥 여기서 주목할 것은 **RNN은 과거의 정보를 응집된 은닉 상태 벡터로 저장해두고 그 정보를 Affine계층과 다음 시각의 RNN계층에 전달**한다.

즉, RNNLM은 지금까지 입력된 단어를 `기억`하고 이를 바탕으로 다음 단어를 예측한다.



### 4.2 Time 계층 구현

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-27.png" alt="fig 5-27" style="zoom:50%;" />

- Time이란 이름은 시계열 데이터를 한번에 처리하는 계층을 뜻한다.

- Time Affine 계층

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-28.png" alt="fig 5-28" style="zoom:50%;" />

  Affine 계층을 T개 준비해서 각 시각의 데이터를 개별적으로 처리하면 된다.

  여기서는 행렬 계산으로 한꺼번에 처리하는 방식으로 구현

  ```python
  class TimeAffine:
      def __init__(self, W, b):
          self.params = [W, b]
          self.grads = [np.zeros_like(W), np.zeros_like(b)]
          self.x = None
  
      def forward(self, x):
          N, T, D = x.shape
          W, b = self.params
  
          rx = x.reshape(N*T, -1) # 한번에 행렬로 처리
          out = np.dot(rx, W) + b
          self.x = x
          return out.reshape(N, T, -1)
  
      def backward(self, dout):
          x = self.x
          N, T, D = x.shape
          W, b = self.params
  
          dout = dout.reshape(N*T, -1)
          rx = x.reshape(N*T, -1)
  
          db = np.sum(dout, axis=0)
          dW = np.dot(rx.T, dout)
          dx = np.dot(dout, W.T)
          dx = dx.reshape(*x.shape)
  
          self.grads[0][...] = dW
          self.grads[1][...] = db
  
          return dx
  ```

  

- Time Embedding 계층

   Embedding 계층을 T개 준비해서 각 시각의 데이터를 개별적으로 처리하면 된다.



- Softmax 계층

  Cross Entropy Error 계층도 함께 구현

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-29.png" alt="fig 5-29" style="zoom:50%;" />

  x는 아래층에서 전해진 '점수'를 나타내며 t는 '정답 레이블'을 나타낸다.

  여기서 손실의 수식은 다음과 같다.

  <img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-11.png" alt="e 5-11" style="zoom:50%;" /> 

  하지만 책에서는 시계열에 대한 평균을 구한다. 데이터 1개당 평균 손실을 구해 최종으로 출력한다.

  ```python
  class TimeSoftmaxWithLoss:
      def __init__(self):
          self.params, self.grads = [], []
          self.cache = None
          self.ignore_label = -1
  
      def forward(self, xs, ts):
          N, T, V = xs.shape
  
          if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우
              ts = ts.argmax(axis=2)
  
          mask = (ts != self.ignore_label)
  
          # 배치용과 시계열용을 정리(reshape)
          xs = xs.reshape(N * T, V)
          ts = ts.reshape(N * T)
          mask = mask.reshape(N * T)
  
          ys = softmax(xs)
          ls = np.log(ys[np.arange(N * T), ts])
          ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정
          loss = -np.sum(ls)
          loss /= mask.sum() # 평균 구한다.
  
          self.cache = (ts, ys, mask, (N, T, V))
          return loss
  
      def backward(self, dout=1):
          ts, ys, mask, (N, T, V) = self.cache
  
          dx = ys
          dx[np.arange(N * T), ts] -= 1
          dx *= dout
          dx /= mask.sum()
          dx *= mask[:, np.newaxis] #ignore_label에 해당하는 데이터는 기울기를 0으로 설정
  
          dx = dx.reshape((N, T, V))
  
          return dx
  ```

  

---





## 5. RNNLM 학습과 평가



### 5.1 RNNLM  구현

RNNLM에서 사용하는 신경망을 SimpleRnnlm으로 구현한다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-30.png" alt="fig 5-30" style="zoom:50%;" /> 

- 초기화 코드

  각 계층에서 사용하는 매개변수를 초기화하고 필요한 계층을 생성한다.

  ```python
  class SimpleRnnlm:
      def __init__(self, vocab_size, wordvec_size, hidden_size):
          V, D, H = vocab_size, wordvec_size, hidden_size
          rn = np.random.randn
  
          # 가중치 초기화
          embed_W = (rn(V, D) / 100).astype('f')
          rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')
          rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')
          rnn_b = np.zeros(H).astype('f')
          affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
          affine_b = np.zeros(V).astype('f')
  
          # 계층 생성
          self.layers = [
              TimeEmbedding(embed_W),
              TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True), # 이전 시각의 은닉 상태를 계층한다.
              TimeAffine(affine_W, affine_b)
              # Xavier 초깃값 이용
          ]
          self.loss_layer = TimeSoftmaxWithLoss()
          self.rnn_layer = self.layers[1]
  
          # 모든 가중치와 기울기를 리스트에 모은다.
          self.params, self.grads = [], []
          for layer in self.layers:
              self.params += layer.params
              self.grads += layer.grads
  ```

  

- 순전파/역전파

  ```python
  def forward(self, xs, ts):
          for layer in self.layers:
              xs = layer.forward(xs)
          loss = self.loss_layer.forward(xs, ts)
          return loss
  
      def backward(self, dout=1):
          dout = self.loss_layer.backward(dout)
          for layer in reversed(self.layers):
              dout = layer.backward(dout)
          return dout
  
      # 신경망의 상태를 초기화 
      def reset_state(self):
          self.rnn_layer.reset_state()
  
  ```

  

### 5.2 언어 모델의 평가

언어 모델은 주어진 정보로부터 다음에 출현할 단어의 확률분포를 출력하며 

이러한 예측 성능을 평가하는 척도로 **`퍼플렉서티(perplexity, 혼란도)`**를 이용한다.

🙄퍼플렉서티 : 확률의 역수, 작을수록 정확도 높다.

👉 분기수(number of branches)로 해석. 다음에 취할 수 있는 선택사항의 수

즉, 퍼플렉서티가 작으면 분기수가 작다는 것이며 이는 출현할 단어의 선택수가 작아진다는 뜻이다.



입력 데이터가 여러개 일대의 공식과 계산을 아래와 같다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-12.png" alt="e 5-12" style="zoom:50%;" /> 

<img src="밑시딥2 5. 순환 신경망(RNN).assets/e 5-13.png" alt="e 5-13" style="zoom:50%;" /> 

N : 데이터의 총개수, t : 원핫 벡터로 나타낸 정답 레이블, y : 확률분포, L : 신경망의 손실

 

### 5.3 RNNLM의 학습 코드

```python
# 하이퍼파라미터 설정
batch_size = 10
wordvec_size = 100
hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수
time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기
lr = 0.1
max_epoch = 100

# 학습 데이터 읽기(전체 중 1000개만)
corpus, word_to_id, id_to_word = ptb.load_data('train')
corpus_size = 1000
corpus = corpus[:corpus_size]
vocab_size = int(max(corpus) + 1)

xs = corpus[:-1]  # 입력
ts = corpus[1:]   # 출력(정답 레이블)
data_size = len(xs)
print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))

# 학습 시 사용하는 변수
max_iters = data_size // (batch_size * time_size)
time_idx = 0
total_loss = 0
loss_count = 0
ppl_list = []

# 모델 생성
model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)
optimizer = SGD(lr)

# 미니배치의 각 샘플의 읽기 시작 위치를 계산
jump = (corpus_size - 1) // batch_size
offsets = [i * jump for i in range(batch_size)]

for epoch in range(max_epoch):
    for iter in range(max_iters):
        # 미니배치 획득
        batch_x = np.empty((batch_size, time_size), dtype='i')
        batch_t = np.empty((batch_size, time_size), dtype='i')
        for t in range(time_size):
            for i, offset in enumerate(offsets):
                batch_x[i, t] = xs[(offset + time_idx) % data_size]
                batch_t[i, t] = ts[(offset + time_idx) % data_size]
            time_idx += 1

        # 기울기를 구하여 매개변수 갱신
        loss = model.forward(batch_x, batch_t)
        model.backward()
        optimizer.update(model.params, model.grads)
        total_loss += loss
        loss_count += 1

    # 에폭마다 퍼플렉서티 평가
    ppl = np.exp(total_loss / loss_count)
    print('| 에폭 %d | 퍼플렉서티 %.2f'
          % (epoch+1, ppl))
    ppl_list.append(float(ppl))
    total_loss, loss_count = 0, 0
```

기존과 다른 점은 '데이터 제공 방법'과 '퍼플렉서티 계산' 부분이다.

🙄 데이터 제공 방법 : Truncated BPTT 방식으로 학습 수행한다. 따라서 데이터는 순차적으로 주고 각각의 미니배치에서 데이터를 읽는 시작 위치를 조정해야 한다. offsets의 각 원소에 데이터를 읽는 시작 위치가 담긴다.

<img src="밑시딥2 5. 순환 신경망(RNN).assets/fig 5-33.png" alt="fig 5-33" style="zoom:50%;" /> 

🔥 현재의 모델로는 큰 말뭉치를 처리할 수 없기때문에 다음 장에서는 이런 문제를 개선한다.



### 5.4 RNNLM의 Trainer 클래스

```python
# 하이퍼파라미터 설정
batch_size = 10
wordvec_size = 100
hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수
time_size = 5  # RNN을 펼치는 크기
lr = 0.1
max_epoch = 100

# 학습 데이터 읽기
corpus, word_to_id, id_to_word = ptb.load_data('train')
corpus_size = 1000  # 테스트 데이터셋을 작게 설정
corpus = corpus[:corpus_size]
vocab_size = int(max(corpus) + 1)
xs = corpus[:-1]  # 입력
ts = corpus[1:]  # 출력（정답 레이블）

# 모델 생성
model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)
optimizer = SGD(lr)
trainer = RnnlmTrainer(model, optimizer)

trainer.fit(xs, ts, max_epoch, batch_size, time_size)
trainer.plot()
```

1. 미니배치를 순차적으로 만들어
2. 모델의 순전파와 역전파를 호출하고
3. 옵티마이저로 가중치를 갱신하고
4. 퍼플렉서티를 구한다

