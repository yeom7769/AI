# ë°‘ì‹œë”¥2 ğŸ“‚7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±

ì´ë²ˆ ì¥ì—ì„œëŠ” ì–¸ì–´ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ìƒì„±ì„ ìˆ˜í–‰í•  ê²ƒì´ë‹¤.

ë§ë­‰ì¹˜ë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ë‚¸ë‹¤. 

ê·¸ ë‹¤ìŒ ê°œì„ ëœ ì–¸ì–´ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ìƒì„ í•œë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” seq2seqë¼ëŠ” ìƒˆë¡œìš´ ì‹ ê²½ë§ì„ êµ¬í˜„í•´ë³´ì.



## 1. ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±

1ì ˆì—ì„œëŠ” ì–¸ì–´ ëª¨ë¸ë¡œ ë¬¸ì¥ì„ ìƒì„±í•´ë³´ì.



### 1.1 RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±ì˜ ìˆœì„œ

ì–¸ì–´ ëª¨ë¸ì—ì„œ ë¬¸ì¥ì„ ìƒì„±ì‹œí‚¤ëŠ” ìˆœì„œë¥¼ ì•Œì•„ë³´ì. 

'you say goodbye and i say hello'ë¼ëŠ” ë§ë­‰ì¹˜ë¥¼ í•™ìŠµí•œ ì–¸ì–´ ëª¨ë¸ì— 'i'ë¼ëŠ” ë‹¨ì–´ë¥¼ ì…ë ¥í•˜ë©´ 

ë‹¤ìŒê³¼ê°™ì€ í™•ë¥ ë¶„í¬ë¥¼ ì¶œë ¥í•œë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-2.png" alt="fig 7-2" style="zoom: 33%;" /> 

ë‹¤ìŒ ë‹¨ì–´ ìƒì„± ë°©ë²•

- ê²°ì •ì  ë°©ë²• : í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ ì„ íƒ
- í™•ë¥ ì  ë°©ë²• : í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ëŠ” ì„ íƒë˜ê¸° ì‰½ë„ë¡ ì„ íƒ

ì´ë ‡ê²Œ ì„ íƒì„ ë°˜ë³µí•˜ë©´ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. 



### 1.2 ë¬¸ì¥ ìƒì„± êµ¬í˜„

- ë¬¸ì¥ìƒì„± ë©”ì„œë“œ

```python
def generate(self, start_id, skip_ids=None, sample_size=100):
    '''
    start_id : ìµœì´ˆë¡œ ì£¼ëŠ” ë‹¨ì–´ì˜ id
    sample_size : ìƒ˜í”Œë§í•˜ëŠ” ë‹¨ì–´ì˜ ìˆ˜
    skip_ids : ë‹¨ì–´ idì˜ ë¦¬ìŠ¤íŠ¸ì¸ë° ì´ ë¦¬ìŠ¤íŠ¸ì— ì†í•˜ëŠ” ë‹¨ì–´ idëŠ” ìƒ˜í”Œë§ë˜ì§€ ì•Šë„ë¡ í•œë‹¤.
    => ì „ì²˜ë¦¬ëœ ë‹¨ì–´ë¥¼ ìƒ˜í”Œë§í•˜ì§€ ì•Šë„ë¡ í•´ì¤Œ
    '''
    word_ids = [start_id]

    x = start_id
    while len(word_ids) < sample_size:
        x = np.array(x).reshape(1, 1)
        # ê° ë‹¨ì–´ì˜ ì ìˆ˜ ì¶œë ¥
        score = self.predict(x)
        # ì ìˆ˜ë¥¼ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ì •ê·œí™”
        p = softmax(score.flatten())
		# ìƒ˜í”Œë§
        sampled = np.random.choice(len(p), size=1, p=p)
        if (skip_ids is None) or (sampled not in skip_ids):
            x = sampled
            word_ids.append(int(x))

    return word_ids
```

í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê°€ì¤‘ì¹˜  : you s.a. years speculative hart-scott-rodino porter mass-market owning ...

í•™ìŠµí•œ ê°€ì¤‘ì¹˜ : you mean buying. that 's less assuming said mr. jones. inflation-adjusted rates...

=> í•™ìŠµí•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ì½”ë“œê°€ ë” ìì—°ìŠ¤ëŸ½ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì´ë¼ê³  í•  ìˆ˜ ì—†ë‹¤.



### 1.3 ë” ì¢‹ì€ ë¬¸ì¥ìœ¼ë¡œ

ë” ì¢‹ì€ ì–¸ì–´ëª¨ë¸ë¡œ í•™ìŠµì‹œí‚¨ ê²½ìš° ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

the meaning of life is in the midwest at home prices...

the meaning of life is happening...



---





## 2. seq2seq

ì´ë²ˆ ì ˆì—ëŠ” ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ì‹œê³„ì—´ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ì„ ìƒê°í•´ë³´ì.

ì—¬ê¸°ì„œì—ì„œëŠ” 2ê°œì˜ RNNì„ ì‚¬ìš©í•˜ëŠ” seq2seqë¥¼ ì‚¬ìš©í•œë‹¤.



### 2.1 seq2seqì˜ ì›ë¦¬

seq2seq = Encoder-Decoder : ì…ë ¥ ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ê³  ì¸ì½”ë”©ëœ ë°ì´í„°ë¥¼ ë””ì½”ë”©í•œë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-5.png" alt="fig 7-5" style="zoom:50%;" /> 

Encoder : ì¸ì½”ë”©í•œ ì •ë³´ì—ëŠ” ë²ˆì—­ì— í•„ìš”í•œ ì •ë³´ ì‘ì¶•

Decoder : ì‘ì¶•ëœ ì¸ì½”ë”© ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ì°©ì–´ ë¬¸ì¥ ìƒì„±



1ï¸âƒ£ encoder

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-6.png" alt="fig 7-6" style="zoom:50%;" />

encoder ì¶œë ¥ì¸ ë²¡í„° hëŠ” ê³ ì • ê¸¸ì´ ë²¡í„°ì´ë‹¤.

ì¦‰, ì¸ì½”ë”©í•œë‹¤ = ì„ì˜ ê¸¸ì´ì˜ ë¬¸ì¥ì„ ê³ ì • ê¸¸ì´ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…

2ï¸âƒ£ decoder

 <img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-8.png" alt="fig 7-8" style="zoom:50%;" />

LSTM ê³„ì¸µì´ ë²¡í„° hë¥¼ ë°›ëŠ”ë‹¤. <eos>ëŠ” ë¬¸ì¥ ìƒì„±ì˜ ì‹œì‘ì˜ ì‹ í˜¸ì´ë‹¤.



<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-9.png" alt="fig 7-9" style="zoom:50%;" /> 



### 2.2 ì‹œê³„ì—´ ë°ì´í„° ë³€í™˜ìš© ì¥ë‚œê° ë¬¸ì œ

ì¥ë‚œê° ë¬¸ì œ(toy problem) : ë¨¸ì‹ ëŸ¬ë‹ì„ í‰ê°€í•˜ê³ ì ë§Œë“  ê°„ë‹¨í•œ ë¬¸ì œ

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-10.png" alt="fig 7-10" style="zoom:33%;" /> 

ì§€ê¸ˆê¹Œì§€ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í•  í–ˆì§€ë§Œ ì´ë²ˆ ë¬¸ì œì—ì„œëŠ” ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í• í•´ë³´ì.



### 2.3 ê°€ë³€ ê¸¸ì´ ì‹œê³„ì—´ ë°ì´í„°

ì´ë²ˆ ì¥ë‚œê° ë¬¸ì œëŠ” ìƒ˜í”Œë§ˆë‹¤ ë°ì´í„°ì˜ ì‹œê°„ ë°©í–¥ í¬ê¸°ê°€ ë‹¤ë¥´ë‹¤. ì¦‰ ê°€ë³„ ê¸¸ì´ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë‹¤ë£¬ë‹¤.

ë•Œë¬¸ì— í•™ìŠµ ì‹œ 'ë¯¸ë‹ˆë°°ì¹˜' ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” ì¶”ê°€ ì½”ë”©ì´ í•„ìš”í•˜ë‹¤.

ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì´ 'íŒ¨ë”©(padding)'ì´ë‹¤.

**`íŒ¨ë”©(padding)`** : ì˜ë¯¸ ì—†ëŠ” ë°ì´í„°ë¥¼ ì±„ì›Œ ëª¨ë“  ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ê· ì¼í•˜ê²Œ ì²˜ë¦¬í•œë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-11.png" alt="fig 7-11" style="zoom:50%;" /> 

ì´ë²ˆ ë¬¸ì œì—ì„œëŠ” ì…ë ¥ì˜ ìˆ«ìê°€ ì„¸ìë¦¬ ìˆ˜ë¡œ ì •í•˜ê³  ì¶œë ¥ ì•ì—ëŠ” _ë¥¼ ë¶™ì´ê¸°ë¡œ ì•½ì†í•˜ì.

ì´ë ‡ê²Œ ëª¨ë“  ìƒ˜í”Œ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ í†µì¼í•œë‹¤.

ì¡´ì¬í•˜ì§€ ì•Šë˜ íŒ¨ë”©ìš© ë¬¸ìê¹Œì§€ seq2seqê°€ ì²˜ë¦¬í•˜ê²Œ ë˜ë¯€ë¡œ seq2seqì— íŒ¨ë”© ì „ìš© ì²˜ë¦¬ë¥¼ ì¶”ê°€í•´ì•¼í•œë‹¤.



### 2.4 ë§ì…ˆ ë°ì´í„°ì…‹

```python
(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt', seed=1984)
# load_data : í…ìŠ¤íŠ¸ íŒŒì¼ ì½ì–´ì™€ì„œ ë¬¸ì idë¡œ ë³€í™˜í•˜ê³  í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‚˜ëˆ  ë°˜í™˜í•œë‹¤.
char_to_id, id_to_char = sequence.get_vocab()
# get_vocab : ë¬¸ìì™€ ë¬¸ìidì˜ ëŒ€ì‘ ê´€ê³„ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

print(x_train.shape, t_train.shape)
print(x_test.shape, t_test.shape)
# (45000, 7) (45000, 5)
# (5000, 7) (5000, 5)

print(x_train[0])
print(t_train[0])
# [ 3  0  2  0  0 11  5]
# [ 6  0 11  7  5]

print(''.join([id_to_char[c] for c in x_train[0]]))
print(''.join([id_to_char[c] for c in t_train[0]]))
# 71+118
# _189
```



---





## 3. seq2seq êµ¬í˜„

seq2seqëŠ” 2ê°œì˜ RNNì„ ì—°ê²°í•œ ì‹ ê²½ë§ì´ë‹¤. 

ì¦‰, encoderí´ë˜ìŠ¤ì™€ decoderí´ë˜ìŠ¤ë¥¼ ì—°ê²°í•˜ëŠ” seq2seq í´ë˜ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼í•œë‹¤.



### 3.1 Encoder í´ë˜ìŠ¤

![fig 7-14](ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-14.png)

- Embedding ê³„ì¸µ : ë¬¸ìë¥¼ ë¬¸ì ë²¡í„°ë¡œ ë³€í™˜
- LSTM : ì˜¤ë¥¸ìª½ìœ¼ë¡œëŠ” ì€ë‹‰ìƒíƒœ(h_t)ì™€ ì…€(c-t)ì„ ì¶œë ¥, ìœ„ìª½ìœ¼ë¡œ ì¶œë ¥ ì€ë‹‰ ìƒíƒœëŠ” íê¸°ëœë‹¤.

```python
class Encoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        # vocab_size : ì–´íœ˜ ìˆ˜(ë¬¸ìì˜ ì¢…ë¥˜), wordvec_size : ë¬¸ì ë²¡í„°ì˜ ì°¨ì› ìˆ˜
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)

        self.params = self.embed.params + self.lstm.params
        self.grads = self.embed.grads + self.lstm.grads
        self.hs = None

    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        self.hs = hs
        return hs[:, -1, :]

    def backward(self, dh):
        dhs = np.zeros_like(self.hs)
        dhs[:, -1, :] = dh

        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout

```



### 3.2 Decoder í´ë˜ìŠ¤

ê³„ì‚°ë°©ì‹ì—ì„œëŠ” ê²°ì •ì ì¸ ë‹µì„ ìƒì„±í•˜ê³ ì í•˜ë¯€ë¡œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë¬¸ì í•˜ë‚˜ë§Œ ê³ ë¥¸ë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-18.png" alt="fig 7-18" style="zoom:50%;" />

```python
class Decoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.affine = TimeAffine(affine_W, affine_b)

        self.params, self.grads = [], []
        for layer in (self.embed, self.lstm, self.affine):
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, h):
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        out = self.lstm.forward(out)
        score = self.affine.forward(out)
        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        dout = self.lstm.backward(dout)
        dout = self.embed.backward(dout)
        dh = self.lstm.dh
        return dh

    # í•™ìŠµ ì‹œì™€ ë¬¸ì¥ ìƒì„± ì‹œì˜ ë™ì‘ì´ ë‹¤ë¥´ë‹¤.
    def generate(self, h, start_id, sample_size):
        sampled = []
        sample_id = start_id
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array(sample_id).reshape((1, 1))
            out = self.embed.forward(x)
            out = self.lstm.forward(out)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(int(sample_id))

        return sampled
```



### 3.3 seq2seq í´ë˜ìŠ¤

```python
class Seq2seq(BaseModel):
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        self.encoder = Encoder(V, D, H)
        self.decoder = Decoder(V, D, H)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads

    def forward(self, xs, ts):
        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]

        h = self.encoder.forward(xs)
        score = self.decoder.forward(decoder_xs, h)
        loss = self.softmax.forward(score, decoder_ts)
        return loss

    def backward(self, dout=1):
        dout = self.softmax.backward(dout)
        dh = self.decoder.backward(dout)
        dout = self.encoder.backward(dh)
        return dout

    def generate(self, xs, start_id, sample_size):
        h = self.encoder.forward(xs)
        sampled = self.decoder.generate(h, start_id, sample_size)
        return sampled
```



### 3.4 seq2seq í‰ê°€

ì—í­ì„ ê±°ë“­í• ìˆ˜ë¡ ì •ë‹µë¥ ì€ ë†’ì•„ì§„ë‹¤. í•˜ì§€ë§Œ ì •ë‹µë¥ ì€ ë§¤ìš° ë‚®ê¸°ë•Œë¬¸ì— seq2seqë¥¼ ê°œì„ ì‹œì¼œë³´ì.



---





## 4. seq2seq ê°œì„ 

ì†ë„ ê°œì„ 

- ì…ë ¥ ë°ì´í„° ë°˜ì „(reverse)
- ì—¿ë³´ê¸°(peeky)



### 4.1 ì…ë ¥ ë°ì´í„° ë°˜ì „(Reverse)

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-23.png" alt="fig 7-23" style="zoom:50%;" />

ì…ë ¥ ë°ì´í„°ë¥¼ ë°˜ì „ì‹œí‚¤ë©´ í•™ìŠµì§„í–‰ì´ ë¹¨ë¼ì§€ê³  ìµœì¢… ì •í™•ë„ ë˜í•œ ì¢‹ì•„ì§„ë‹¤ê³  í•œë‹¤.

- íŒŒì´ì¬ êµ¬í˜„

```python
# ì…ë ¥ ë°˜ì „ ì—¬ë¶€ ì„¤ì • =============================================
is_reverse = False  # True
if is_reverse:
    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]
# ================================================================
```

ğŸ™„ ì§ê´€ì ìœ¼ë¡œ ê¸°ìš¸ê¸° ì „íŒŒê°€ ì›í™œí•´ì§€ê¸° ë•Œë¬¸ì´ë¼ê³  ì˜ˆìƒ.

ğŸ‘‰ ì…ë ¥ ë¬¸ì¥ì˜ ì²« ë¶€ë¶„ì´ ë°˜ì „ ë•Œë¬¸ì— ë³€í™˜ í›„ ë‹¨ì–´ì™€ ê°€ê¹Œì›Œì ¸ ê¸°ìš¸ê¸°ê°€ ë” ì˜ ì „í•´ì§„ë‹¤ê³  ìƒê°



### 4.2 ì—¿ë³´ê¸°(Peeky)

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-25.png" alt="fig 7-25" style="zoom:50%;" />

h ì•ˆì—ëŠ” decoderì—ê²Œ í•„ìš”í•œ ì •ë³´ ëª¨ë‘ ë‹´ê²¨ ìˆë‹¤. ë•Œë¬¸ì— ì´ ì¤‘ìš”í•œ ì •ë³´ì¸ hë¥¼ ë” í™œìš©í•  ë°©ë²•ì„ ì°¾ëŠ”ë‹¤.



<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-26.png" alt="fig 7-26" style="zoom:50%;" />

Affine ê³„ì¸µê³¼ LSTM ê³„ì¸µì— ì¶œë ¥ hë¥¼ ì „í•´ì£¼ë©´ì„œ ì…ë ¥ ë²¡í„°ê°€ 2ê°œì”© ëœë‹¤.

ì´ëŠ” ì‹¤ì œë¡œ ë‘ ë²¡í„°ê°€ ì—°ê²°ëœ ê²ƒì„ ì˜ë¯¸í•œë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-27.png" alt="fig 7-27" style="zoom: 33%;" /> 

```python
class PeekyDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        # ê°€ì¤‘ì¹˜ í˜•ìƒ ì»¤ì§
        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.affine = TimeAffine(affine_W, affine_b)

        self.params, self.grads = [], []
        for layer in (self.embed, self.lstm, self.affine):
            self.params += layer.params
            self.grads += layer.grads
        self.cache = None

    def forward(self, xs, h):
        N, T = xs.shape
        N, H = h.shape

        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        # ì‹œê³„ì—´ë§Œí¼ ë³µì œí•´ hsì— ì €ì¥
        hs = np.repeat(h, T, axis=0).reshape(N, T, H)
        # hsì™€ embedding ê³„ì¸µì˜ ì¶œë ¥ ì—°ê²°
        out = np.concatenate((hs, out), axis=2)

        out = self.lstm.forward(out)
        # ì´ë¥¼ LSTM ê³„ì¸µì— ì—°ê²°
        out = np.concatenate((hs, out), axis=2)

        score = self.affine.forward(out)
        self.cache = H
        return score
```

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-28.png" alt="fig 7-28" style="zoom:50%;" /> 

í˜„ì¬ê¹Œì§€ ìˆ˜í–‰í•œ ê°œì„ ë³´ë‹¤ ë” í° íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì–´í…ì…˜ ê¸°ìˆ ì€ ë‹¤ìŒ ì¥ì—ì„œ ë°°ìš´ë‹¤.



---





## 5. seq2seqë¥¼ ì´ìš©í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜

- ê¸°ê³„ ë²ˆì—­ : ë‹¤ë¥¸ ì–¸ì–´ì˜ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
- ìë™ ìš”ì•½ : ì§§ê²Œ ìš”ì•½ëœ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
- ì§ˆì˜ì‘ë‹µ : ì‘ë‹µìœ¼ë¡œ ë³€í™˜
- ë©”ì¼ ìë™ ì‘ë‹µ : ë‹µë³€ ê¸€ë¡œ ë³€í™˜



### 5.1 ì±—ë´‡

### 5.2 ì•Œê³ ë¦¬ì¦˜ í•™ìŠµ

ì†ŒìŠ¤ ì½”ë“œë„ ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ ì½”ë”© í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.



### 5.3 ì´ë¯¸ì§€ ìº¡ì…”ë‹

seq2seqëŠ” í…ìŠ¤íŠ¸ ì™¸ì—ë„ ì´ë¯¸ì§€, ìŒì„± ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-31.png" alt="fig 7-31" style="zoom:50%;" />

CNNì˜ ìµœì¢… ì¶œë ¥ì€ íŠ¹ì§• ë§µì´ë‹¤. ì¦‰, 3ì°¨ì›ì´ë¯€ë¡œ ì´ë¥¼ Decoderì˜ LSTMì´ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì „ì²˜ë¦¬í•´ì•¼í•œë‹¤.

íŠ¹ì§• ë§µì„ 1ì°¨ì›ìœ¼ë¡œ í‰íƒ„í™”í•œ í›„ ì…ë ¥ì‹œí‚¨ë‹¤.

<img src="ë°‘ì‹œë”¥2 7. RNNì„ ì‚¬ìš©í•œ ë¬¸ì¥ ìƒì„±.assets/fig 7-32.png" alt="fig 7-32" style="zoom:50%;" />