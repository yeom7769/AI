# ë°‘ì‹œë”¥  ğŸ“‚6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤

> ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’ì„ íƒìƒ‰í•˜ëŠ” <u>ìµœì í™” ë°©ë²•, ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ ì´ˆê¹ƒê°’, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ë°©ë²•</u> ë“±ê³¼ ê°™ì´ ì‹ ê²½ë§ í•™ìŠµì˜ í•µì‹¬ ê°œë…ë“¤ì„ ë°°ì›Œë³´ì.



## 1. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 

> ì‹ ê²½ë§ í•™ìŠµì˜ ëª©í‘œ = ë§¤ê°œë³€ìˆ˜(weight, bias)ì˜ ìµœì ê°’ì„ ì°¾ëŠ” ê²ƒ! ğŸ‘‰ **`ìµœì í™”(optimization)`**

ğŸ˜¥ ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì°¾ê¸° ìœ„í•´ `ê¸°ìš¸ê¸°(ë¯¸ë¶„)`ì„ ì´ìš©í–ˆë‹¤. ì´ëŠ” <u>í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent, SGD)</u> ë°©ë²•ìœ¼ë¡œ, ë³´ë‹¤ ë˜‘ë˜‘í•œ ìµœì í™” ë°©ë²•ì´ ì¡´ì¬í•œë‹¤. ì§€ê¸ˆë¶€í„° SGDì˜ ë‹¨ì ì„ ì‚´í´ë³´ê³  ë‹¤ë¥¸ ìµœì í™” ê¸°ë²•ì„ ì•Œì•„ë³´ì.



### 1.2 í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent, SGD)

> ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œ ê°„ë‹¤. 

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.1.png" alt="e 6.1" style="zoom:50%;" />  

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        # lr = learning rate(í•™ìŠµë¥ ) ==> í•˜ì´í¼íŒŒë¼ë¯¸í„°
        
    def update(self, params, grads):
        for key in params.keys(): # keys = w1, b1, w2, b2, ...
            params[key] -= self.lr * grads[key] 
            
# SGD í˜¸ì¶œ
optimizer = SGD()
optimizer.update(params, grads)
```



### 1.3 SGD ë‹¨ì 

> SGDëŠ” ë‹¨ìˆœí•˜ê³  êµ¬í˜„ì´ ì‰½ì§€ë§Œ ë•Œë¡œëŠ” ë¹„íš¨ìœ¨ì ì¼ ë•Œê°€ ìˆë‹¤.

- ì˜ˆë¥¼ ë“¤ë©´, ì•„ë˜ì™€ ê°™ì€ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ëŠ” ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì. 

  <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.2.png" alt="e 6.2" style="zoom:50%;" /> 

 <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-1.png" alt="fig 6-1" style="zoom:50%;" />

â€‹													 	`[í•¨ìˆ˜ì˜ ê·¸ë˜í”„ì™€ ë“±ê³ ì„ ]`

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-2.png" alt="fig 6-2" style="zoom: 33%;" /> 

â€‹						`[í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°]`

> ê¸°ìš¸ê¸°ë¥¼ ë³´ë©´ yì¶• ë°©í–¥ì€ ë§¤ìš° í¬ê³  xì¶• ë°©í–¥ì€ ë§¤ìš° ì‘ë‹¤. 
>
> ìµœì†Ÿê°’ì´ ë˜ëŠ” ì¥ì†ŒëŠ” ì‹¤ì œë¡œ (0,0)ì´ì§€ë§Œ, ê¸°ìš¸ê¸° ëŒ€ë¶€ë¶„ì€ (0,0) ë°©í–¥ì„ ê°€ë¦¬í‚¤ì§€ ì•ŠëŠ”ë‹¤.

- (-7, 2)ì—ì„œ íƒìƒ‰ì„ ì‹œì‘í•´ë³´ì.

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-3.png" alt="fig 6-3" style="zoom: 33%;" /> 

>  ìµœì†Ÿê°’ì¸ (0,0)ê¹Œì§€ ì§€ê·¸ì¬ê·¸ë¡œ ì´ë™í•˜ì—¬ ë¹„íš¨ìœ¨ì ì´ë‹¤.

##### ğŸ”¥ **`SGD ë‹¨ì ` **: <u>ë¹„ë“±ë°©ì„± í•¨ìˆ˜</u>(ë°©í–¥ì— ë”°ë¼ ì„±ì§ˆì´ ë‹¬ë¼ì§€ëŠ” í•¨ìˆ˜)[íŠ¹ì •í•œ ì¢Œí‘œì—ì„œ ê¸°ìš¸ê¸°ê°€ ê°€ë¥´ì¹˜ëŠ” ì§€ì ì´ ë³€í•˜ëŠ” ê²½ìš°ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒ]ì—ì„œ íƒìƒ‰ ê²½ë¡œê°€ ë¹„íš¨ìœ¨ì ì„ 



### 1.4 ëª¨ë©˜í…€(Momentum)

> 'ìš´ë™ëŸ‰'ì„ ëœ»í•˜ëŠ” ë‹¨ì–´

1. ëª¨ë©˜í…€ ê¸°ë²•ì˜ ìˆ˜ì‹ í‘œí˜„

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.3.png" alt="e 6.3" style="zoom:50%;" /> 

- v : ì†ë„(velocity) ğŸ‘‰ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ í˜ì„ ë°›ì•„ ë¬¼ì²´ê°€ ê°€ì†ë˜ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ„
- Î± : ë¬¼ì²´ê°€ ì•„ë¬´ëŸ° í˜ì„ ë°›ì§€ ì•Šì„ ë•Œ ì„œì„œíˆ í•˜ê°•ì‹œí‚¤ëŠ” ì—­í• 
- Î±v : ì´ì „ íšŒì°¨ì˜ ìˆ˜ì •ëŸ‰

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-4.png" alt="fig 6-4" style="zoom:50%;" /> 

- ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì´ ì¼ì •í•˜ë©´ v ê°€ì†, ê¸°ìš¸ê¸°ì˜ ë°©í–¥ì´ ì¼ì •í•˜ì§€ ì•Šìœ¼ë©´ v ì˜í–¥ ì ìŒ
- ì´ì „ íšŒì°¨ì˜ ìˆ˜ì •ëŸ‰ì— ì˜í–¥ì„ ë°›ì•„ ìˆ˜ì •ëŸ‰ì´ ê¸‰ê²©í•˜ê²Œ ë³€í™”í•˜ëŠ” ê²ƒì„ ë§‰ê³  ë¶€ë“œëŸ½ê²Œ ìˆ˜ì •í•¨

```python
class Momentum:
    # Î±(momentum) í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë” ëŠ˜ì–´ë‚¨
    def __init__(self, lr=0.01, momentum=0.9):
        # learning rate
        self.lr = lr
        # Î± (ë¬¼ì²´ê°€ í˜ ë°›ì§€ ì•Šì„ ë•Œ í•˜ê°•ì‹œí‚¬ìˆ˜ ìˆëŠ” ë³€ìˆ˜)
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items(): # keys : w1, b1, w2, b2, ...
                self.v[key] = np.zeros_like(val) # valì˜ ë°ì´í„° íƒ€ì…ì— í¬ê¸°ê°€ 0ì¸ ë°°ì—´
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            # self.momentum*self.v[key] : ì´ì „ íšŒì°¨ì˜ ê°±ì‹ ëŸ‰
            params[key] += self.v[key]
```

- ëª¨ë©˜í…€ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ë³´ì.

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-5.png" alt="fig 6-5" style="zoom: 33%;" /> 

> xì¶•ì˜ í˜ì€ ì‘ì§€ë§Œ ë°©í–¥ì„ ë³€í•˜ì§€ ì•Šì•„ì„œ í•œ ë°©í–¥ìœ¼ë¡œ ì¼ì •í•˜ê²Œ **`ê°€ì†`** 
>
> yì¶•ì€ í˜ì€ í¬ì§€ë§Œ ìœ„ì•„ë˜ë¡œ ìƒì¶©í•˜ì—¬ ì†ë„ê°€ ì•ˆì •ì ì´ì§€ ì•ŠìŒ



### 1.5 AdaGrad(Adaptive Gradient)

> ìˆ˜ì •ëŸ‰ì´ ìë™ìœ¼ë¡œ ì¡°ì •ë¨ => ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ í•™ìŠµë¥ ì´ ê°ì†Œ
>
> `í•™ìŠµë¥  ê°ì†Œ(learning rate decay)` : í•™ìŠµì„ ì§„í–‰í•˜ë©´ì„œ í•™ìŠµë¥ ì„ ì ì°¨ ì¤„ì—¬ê°€ëŠ” ë°©ë²•

- <u>AdaGrad</u> : ê°ê°ì˜ ë§¤ê°œë³€ìˆ˜ì— ë§ì¶¤í˜• ê°’ì„ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•

  â€‹					ğŸ‘‰ ê°œë³„ ë§¤ê°œë³€ìˆ˜ì— ì ì‘ì ìœ¼ë¡œ(adaptive) í•™ìŠµë¥ ì„ ì¡°ì •



1. AdaGrad ìˆ˜ì‹

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.5.png" alt="e 6.5" style="zoom:50%;" /> 

- Î˜ : í–‰ë ¬ì˜ ì›ì†Œë³„ ê³±ì…ˆ => ì œê³±!

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.6.png" alt="e 6.6" style="zoom:50%;" /> 

- hëŠ” ë¬´ì¡°ê±´ ì¦ê°€! => ê°±ì‹ ëŸ‰ì€ hê°€ ë¶„ëª¨ì— ìˆìœ¼ë¯€ë¡œ ë¬´ì¡°ê±´ ê°ì†Œ!

- ë§¤ê°œë³€ìˆ˜ì˜ ì›ì†Œ ì¤‘ì—ì„œ ë§ì´ ì›€ì§ì¸(í¬ê²Œ ê°±ì‹ ëœ) ì›ì†ŒëŠ” í•™ìŠµë¥ ì´ ë‚®ì•„ì§



2. AdaGrad êµ¬í˜„

   ```python
   class AdaGrad:
       def __init__(self, lr=0.01):
           self.lr = lr
           self.h = None
           
       def update(self, params, grads):
           if self.h is None:
               self.h = {}
               for key, val in params.items():
                   self.h[key] = np.zeros_like(val)
               
           for key in params.keys():
               self.h[key] += grads[key] * grads[key]
               params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
               # 1e-7 : 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´
   ```

- AdaGradë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ë³´ì.

 

> yì¶• ë°©í–¥ì€ ê¸°ìš¸ê¸°ê°€ ì»¤ì„œ ì²˜ìŒì—ëŠ” í¬ê²Œ ì›€ì§ì´ì§€ë§Œ, ê·¸ì— ë¹„ë¡€í•´ ê°±ì‹  ì •ë„ë„ í° í­ìœ¼ë¡œ ì‘ì•„ì§€ë„ë¡ ì¡°ì •ë¨

ğŸ¤·â€â™€ï¸ AdaGradëŠ” ë¬´í•œíˆ í•™ìŠµí•œë‹¤ë©´ ê°±ì‹  ê°’ì€ 0ì´ ë¨. ê·¸ë˜ì„œ ë¨¼ ê³¼ê±°ì˜ ê¸°ìš¸ê¸° ì •ë³´ëŠ” ìŠê³  ìƒˆë¡œìš´ ê¸°ìš¸ê¸° ì •ë³´ë¥¼ í¬ê²Œ ë°˜ì˜í•˜ëŠ” ì§€ìˆ˜ì´ë™í‰ê· (Exponential moving average, EMA)ì„ ì´ìš©í•œ ê¸°ë²•ì¸ `RMSProp` ì‚¬ìš©í•˜ê¸°ë„í•¨.

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-6.png" alt="fig 6-6" style="zoom: 33%;" /> 

3. RMSProp êµ¬í˜„

   > ì•„ë‹¤ê·¸ë¼ë“œì—ì„œ ê°±ì‹ ëŸ‰ ê°ì†Œë¥¼ í†µí•´ í†µí•´ í•™ìŠµì´ ì •ì²´ë˜ëŠ” ë‹¨ì ì„ ê·¹ë³µí•œ ë°©ë²•

     <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.9.png" alt="e 6.9" style="zoom:67%;" />
   
   - Ï (decay_rate) : ì´ì „ ì‹œì ì˜ hë¥¼ ì ë‹¹í•œ ë¹„ìœ¨ë¡œ ê°ì†Œì‹œí‚´ ==> ì˜› ì •ë³´ì¼ìˆ˜ë¡ ë§ì´ ê°ì†Œë¨
   
   ```python
   class RMSprop:
       def __init__(self, lr=0.01, decay_rate = 0.99):
           self.lr = lr
           self.decay_rate = decay_rate
           self.h = None
           
       def update(self, params, grads):
           if self.h is None:
               self.h = {}
               for key, val in params.items():
                   self.h[key] = np.zeros_like(val)
               
           for key in params.keys():
            self.h[key] *= self.decay_rate
               self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]
               params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
   ```
   
   

### 1.6 ğŸ“ŒAdam

> ëª¨ë©˜í…€(ë¶€ë“œëŸ¬ìš´ ê°±ì‹ ) + AdaGrad(ê° ë§¤ê°œë³€ìˆ˜ì˜ ì ì‘ì ì¸ ê°±ì‹ )

1. íŠ¹ì§•

   - í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ 'í¸í–¥ ë³´ì •'ì´ ì§„í–‰ëœë‹¤.

2. Adam êµ¬í˜„

    <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.10.png" alt="e 6.10" style="zoom:50%;" />
   
   ```python
   class Adam:
       def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
           self.lr = lr
           self.beta1 = beta1
           self.beta2 = beta2
           self.iter = 0
           self.m = None
           self.v = None
           
       def update(self, params, grads):
           if self.m is None:
               self.m, self.v = {}, {}
               for key, val in params.items():
                   self.m[key] = np.zeros_like(val)
                   self.v[key] = np.zeros_like(val)
           
           self.iter += 1
           # ê°±ì‹ ì„ ë°˜ë³µí• ìˆ˜ë¡ lr ê°ì†Œ
           lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
           
           for key in params.keys():
               #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
               #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
               self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
               self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
               
               params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
   ```
   

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-7.png" alt="fig 6-7" style="zoom:33%;" /> 

> ëª¨ë©˜í…€ê³¼ ë¹„ìŠ·í•œ íŒ¨í„´ì´ì§€ë§Œ í•™ìŠµì˜ ê°±ì‹  ê°•ë„ë¥¼ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•´ì„œ ì¢Œìš° í”ë“¤ë¦¼ ì ìŒ



### 1.7 ì–´ë–¤ ê°±ì‹  ë°©ë²•ì„ ì´ìš©í•  ê²ƒì¸ê°€?

> SGD vs ëª¨ë©˜í…€ vs AdaGrad vs Adam

- ê°ìì˜ ìƒí™©ì„ ê³ ë ¤í•´ ì—¬ëŸ¬ ê°€ì§€ë¡œ ì‹œë„



### 1.8 MNIST ë°ì´í„°ì…‹ìœ¼ë¡œ ë³¸ ê°±ì‹  ë°©ë²• ë¹„êµ

```python
# 0. MNIST ë°ì´í„° ì½ê¸°==========
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000


# 1. ì‹¤í—˜ìš© ì„¤ì •==========
optimizers = {}
optimizers['SGD'] = SGD()
optimizers['Momentum'] = Momentum()
optimizers['AdaGrad'] = AdaGrad()
optimizers['Adam'] = Adam()

networks = {}
train_loss = {}
for key in optimizers.keys():
    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],output_size=10)
    # hidden_size_listëŠ” ì€ë‹‰ì¸µì´ 4ê°œ ìˆìŒì„ ëœ»í•¨(ê°€ì¤‘ì¹˜ 100ê°œ)
    train_loss[key] = []    


# 2. í›ˆë ¨ ì‹œì‘==========
for i in range(max_iterations):
    # batchì‚¬ì´ì¦ˆì— ë§ê²Œ í• ë‹¹
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in optimizers.keys():
        # ê¸°ìš¸ê¸° êµ¬í•˜ê¸°
        grads = networks[key].gradient(x_batch, t_batch)
        # ê° ìµœì í™”(ê°±ì‹ )ì— ë§ê²Œ ìµœì í™”í•˜ê¸°
        optimizers[key].update(networks[key].params, grads)
    	# loss êµ¬í•˜ê¸°
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
    
    if i % 100 == 0:
        print( "===========" + "iteration:" + str(i) + "===========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))


# 3. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°==========
markers = {"SGD": "o", "Momentum": "x", "AdaGrad": "s", "Adam": "D"}
x = np.arange(max_iterations)
for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
    # smooth_curveëŠ” ë”°ë¡œ ì§€ì •í•œ í•¨ìˆ˜ : ë¶€ë“œëŸ¬ìš´ ê³¡ì„  ê·¸ë˜í”„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 1)
plt.legend()
plt.show()
```

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/Figure_1.png" alt="Figure_1" style="zoom:72%;" /> 

> ì£¼ì˜í•  ì . í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë‹¤ë¼ ê²°ê³¼ëŠ” ë‹¬ë¼ì§„ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ SGDë³´ë‹¤ ë‹¤ë¥¸ ì„¸ ê¸°ë²•ì´ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ë©° ì •í™•ë„ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚œë‹¤.



---



## 2. âœ¨ê°€ì¤‘ì¹˜ì˜ ì´ˆê¹ƒê°’

> ê°€ì¤‘ì¹˜ì˜ ì´ˆê¹ƒê°’ì„ ë¬´ì—‡ìœ¼ë¡œ ì„¤ì •í•˜ëŠëƒëŠ” ì‹ ê²½ë§ í•™ìŠµì—ì„œ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. 



### 2.1 ì´ˆê¹ƒê°’ì„ 0ìœ¼ë¡œ í•˜ë©´?

ğŸ”¥ **`ê°€ì¤‘ì¹˜ ê°ì†Œ(weight decay)`** : ì˜¤ë²„í”¼íŒ…ì„ ì–µì œí•´ ë²”ìš© ì„±ëŠ¥ì„ ë†’ì´ëŠ” í…Œí¬ë‹‰

â€‹															ğŸ‘‰ ê°€ì¤‘ì¹˜ ê°’ì´ ì‘ì•„ì§€ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ë²• 

======> ê°€ì¤‘ì¹˜ ê°’ì„ ì‘ê²Œ í•˜ë©´ ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤!

======> ê°€ì¤‘ì¹˜ëŠ” ì…ë ¥ì˜ ì˜í–¥ë ¥ì„ ì œì–´í•˜ê¸° ë•Œë¬¸ì—!!

â€‹				 ê°€ì¤‘ì¹˜ê°€ ì‘ë‹¤ë©´ ì…ë ¥ì˜ ì˜í–¥ë ¥ì„ ì‘ê²Œí•˜ê¸° ë•Œë¬¸ì— íŠ¹ì • ë°ì´í„°ì— ì˜¤ë²„í”¼íŒ…ë˜ì§€ ì•ŠìŒ!



â“ <u>ê·¸ë ‡ë‹¤ë©´ ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ì„ ëª¨ë‘ 0ìœ¼ë¡œ í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ</u> â“

ğŸ†– ê°€ì¤‘ì¹˜ë¥¼ <u>ê· ì¼í•œ ê°’</u>ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì˜¤ì°¨ì—­ì „íŒŒë²•ì—ì„œ ëª¨ë“  ê°€ì¤‘ì¹˜ì˜ ê°’ì´ ë˜‘ê°™ì´ ê°±ì‹ ëœë‹¤. 



### 2.2 ì€ë‹‰ì¸µì˜ í™œì„±í™”ê°’ ë¶„í¬

> ì€ë‹‰ì¸µì˜ í™œì„±í™”ê°’(í™œì„±í™” í•¨ìˆ˜ì˜ ì¶œë ¥ ë°ì´í„°)ì˜ ë¶„í¬ë¥¼ í†µí•´ ì‹ ê²½ë§ í•™ìŠµì´ íš¨ìœ¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- ì€ë‹‰ì¸µì˜ í™œì„±í™”ê°’ ë¶„í¬ë¥¼ íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ê·¸ë ¤ë³´ì.

  ```python
  # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  
  # ReLU í™œì„±í™” í•¨ìˆ˜
  def ReLU(x):
      return np.maximum(0, x)
  
  # tanh í™œì„±í™” í•¨ìˆ˜
  def tanh(x):
      return np.tanh(x)
  
  # ì…ë ¥ ë°ì´í„° ë¬´ì‘ìœ„ë¡œ ìƒì„±
  x = np.random.randn(1000, 100)  # 1000ê°œì˜ ë°ì´í„°
  node_num = 100  # ê° ì€ë‹‰ì¸µì˜ ë…¸ë“œ(ë‰´ëŸ°) ìˆ˜
  hidden_layer_size = 5  # ì€ë‹‰ì¸µ 5ê°œ
  activations = {}  # í™œì„±í™” ê²°ê³¼ ì €ì¥
  
  # ê° ì€ë‹‰ì¸µ íƒìƒ‰
  for i in range(hidden_layer_size):
      # ì•ì— ê³„ì‚°í•œ x*wì˜ ê°’ì„ ë°›ì•„ì„œ í•´ë‹¹ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ ê³±í•´ì„œ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì•ì˜ ê³„ì‚°ê°’ ë¶ˆëŸ¬ì˜´
      if i != 0:
          x = activations[i-1]
  
      # ì´ˆê¹ƒê°’ ì‹¤í—˜
      # í‘œì¤€í¸ì°¨ê°€ 1ì¸ ì •ê·œë¶„í¬
      w = np.random.randn(node_num, node_num) * 1 # (1)
      # í‘œì¤€í¸ì°¨ê°€ 0.01ì¸ ì •ê·œë¶„í¬
      w = np.random.randn(node_num, node_num) * 0.01 # (2)
      # Xavier ì´ˆê¹ƒê°’
      w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num) # (3)  
      # He ì´ˆê¹ƒê°’(ReLU íŠ¹í™” ì´ˆê¹ƒê°’)
      w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # (4)
  
      a = np.dot(x, w)
      z = sigmoid(a)
      # z = ReLU(a)
      # z = tanh(a)
  	
      # í™œì„±í™” ê°’
      activations[i] = z
  
  # íˆìŠ¤í† ê·¸ë¨ ê·¸ë¦¬ê¸°
  for i, a in activations.items():
      plt.subplot(1, len(activations), i+1)
      plt.title(str(i+1) + "-layer")
      if i != 0: plt.yticks([], [])
      plt.hist(a.flatten(), 30, range=(0,1))
  plt.show()
  ```



(1) í‘œì¤€í¸ì°¨ê°€ 1ì¸ ì •ê·œë¶„í¬

â€‹	<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-10.png" alt="fig 6-10" style="zoom:50%;" /> 

> í™œì„±í™”ê°’ë“¤ì´ 0, 1ì— ì¹˜ìš°ì³ ë¶„í¬ ==> ì—­ì „íŒŒì˜ ê¸°ìš¸ê¸° ê°’ì´ ì‘ì•„ì§€ë‹¤ê°€ ì‚¬ë¼ì§ 

> **`ê¸°ìš¸ê¸° ì†Œì„¤(gradient vanishing)`** ë°œìƒ!

  <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/Figure_2.png" alt="Figure_2" style="zoom:50%;" />

[Sigmoid í™œì„±í™”í•¨ìˆ˜ëŠ” input ê°’ì´ 0ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ë¯¸ë¶„ê°’ì€ 0ìœ¼ë¡œ ì‘ì•„ì§€ë¯€ë¡œ backward propagationì—ì„œ ê¸°ìš¸ê¸° ê°’ë„ ë§¤ìš° ì‘ì•„ì§]



(2) í‘œì¤€í¸ì°¨ê°€ 0.01ì¸ ì •ê·œë¶„í¬

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-11.png" alt="fig 6-11" style="zoom:50%;" /> 

> í™œì„±í™”ê°’ë“¤ì´ í•œê³³ìœ¼ë¡œ ì¹˜ìš°ì¹¨ ==> ë‹¤ìˆ˜ì˜ ë‰´ëŸ°ì´ ê°™ì€ ê°’ì„ ì¶œë ¥ ==> ë‹¤ìˆ˜ì˜ ë‰´ëŸ° ì˜ë¯¸ X
>
> **`í‘œí˜„ë ¥ ì œí•œ`** ë°œìƒ!

ğŸ™„ ìœ„ì˜ ê²½ìš°ë“¤ì„ ë³´ì•˜ì„ ë•Œ, í™œì„±í™”ê°’ì€ <u>ì ë‹¹íˆ ê³¨ê³ ë£¨ ë¶„í¬</u>ë˜ì–´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.



(3) âœ¨ **`Xavier ì´ˆê¹ƒê°’`**

> ê° ì¸µì˜ í™œì„±í™”ê°’ë“¤ì„ ê´‘ë²”ìœ„í•˜ê²Œ ë¶„í¬ì‹œí‚¤ê¸° ìœ„í•œ ì´ˆê¹ƒê°’ ì„¤ì •

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.11.png" alt="e 6.11" style="zoom:67%;" /> n : ì• ê³„ì¸µì˜ ë…¸ë“œ ìˆ˜

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-13.png" alt="fig 6-13" style="zoom:50%;" /> 

- sigmoid í•¨ìˆ˜ ëŒ€ì‹  tanhí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ ì¼ê·¸ëŸ¬ì§„ ëª¨ì–‘ì„ ê°œì„ í•  ìˆ˜ ìˆìŒ => tanh í•¨ìˆ˜ëŠ” (0,0) ëŒ€ì¹­ì´ê¸° ë•Œë¬¸


<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/Figure_3.png" alt="Figure_3" style="zoom:50%;" /> 



### 2.3 ReLUë¥¼ ì‚¬ìš©í•  ë•Œì˜ ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’

> Xavier ì´ˆê¹ƒê°’ì€ í™œì„±í™” í•¨ìˆ˜ê°€ ì„ í˜•ì¸ ê²ƒì„ ì „ì œ ==> ReLUì— íŠ¹í™”ëœ ì´ˆê¹ƒê°’ ì´ìš© ê¶Œì¥!!
>
> ğŸ‘‰ **`He ì´ˆê¹ƒê°’`**

```python
# He ì´ˆê¹ƒê°’(ReLU íŠ¹í™” ì´ˆê¹ƒê°’)
w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # (4)
```

- ì§ê°ì ìœ¼ë¡œ ReLUëŠ” ìŒì˜ ì˜ì—­ì´ 0ì´ë¼ì„œ ë” ë„“ê²Œ ë¶„í¬ì‹œí‚¤ê¸° ìœ„í•´ Xavier ì´ˆê¹ƒê°’ë³´ë‹¤ 2ë°°ì˜ ê³„ìˆ˜ í•„ìš”

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-14.png" alt="fig 6-14" style="zoom:50%;" /> 



### 2.4 MNIST ë°ì´í„°ì…‹ìœ¼ë¡œ ë³¸ ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’ ë¹„êµ

```python
# 1. ì‹¤í—˜ìš© ì„¤ì •(ê°€ì¤‘ì¹˜ ì´ˆê¹ƒê°’)==========
weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}
optimizer = SGD(lr=0.01) # ìµœì í™” SGD

networks = {}
train_loss = {}
for key, weight_type in weight_init_types.items():
    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100], output_size=10, weight_init_std=weight_type) # ì´ˆê¹ƒê°’ì„ ë”•ì…”ë„ˆë¦¬ valueê°’ìœ¼ë¡œ ë°›ìŒ
    train_loss[key] = []


# 2. í›ˆë ¨ ì‹œì‘==========
for i in range(max_iterations):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in weight_init_types.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizer.update(networks[key].params, grads)
    
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
```

```python
def __init_weight(self, weight_init_std):
    """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    Parameters
    ----------
    weight_init_std : ê°€ì¤‘ì¹˜ì˜ í‘œì¤€í¸ì°¨ ì§€ì •ï¼ˆe.g. 0.01ï¼‰
        'relu'ë‚˜ 'he'ë¡œ ì§€ì •í•˜ë©´ 'He ì´ˆê¹ƒê°’'ìœ¼ë¡œ ì„¤ì •
        'sigmoid'ë‚˜ 'xavier'ë¡œ ì§€ì •í•˜ë©´ 'Xavier ì´ˆê¹ƒê°’'ìœ¼ë¡œ ì„¤ì •
    """
    all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
    for idx in range(1, len(all_size_list)):
        scale = weight_init_std
        if str(weight_init_std).lower() in ('relu', 'he'):
            scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUë¥¼ ì‚¬ìš©í•  ë•Œì˜ ê¶Œì¥ ì´ˆê¹ƒê°’
        elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):
            scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidë¥¼ ì‚¬ìš©í•  ë•Œì˜ ê¶Œì¥ ì´ˆê¹ƒê°’
        self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])
        self.params['b' + str(idx)] = np.zeros(all_size_list[idx])
```

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-15.png" alt="fig 6-15" style="zoom: 33%;" /> 

> ê¸°ë³¸ ì¸ìë¡œ í™œì„±í™” í•¨ìˆ˜ë¥¼ reluë¡œ ì„¤ì •í•¨. ë•Œë¬¸ì— xavierê³¼ heì˜ í•™ìŠµì´ ì›”ë“±íˆ ì¢‹ìŒ.



---



## 3. ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)

> ê° ì¸µì´ í™œì„±í™”ë¥¼ ì ë‹¹íˆ í¼ëœ¨ë¦¬ë„ë¡ ê°•ì œí•˜ëŠ” ê²ƒ



### 3.1 ë°°ì¹˜ ì •ê·œí™” ì•Œê³ ë¦¬ì¦˜

1. íŠ¹ì§•
   - í•™ìŠµ ì†ë„ ê°œì„ 
   - ì´ˆê¹ƒê°’ì— í¬ê²Œ ì˜ì¡´ X
   - ì˜¤ë²„í”¼íŒ… ì–µì œ



2. ë°°ì¹˜ ì •ê·œí™”(Batch Norm) ê³„ì¸µ

   >  ë°ì´í„° ë¶„í¬ë¥¼ ì •ê·œí™”

   <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-16.png" alt="fig 6-16" style="zoom:50%;" /> 

   - ë°ì´í„° ë¶„í¬ê°€ í‰ê·  0, ë¶„ì‚° 1ì´ ë˜ë„ë¡ ì •ê·œí™”í•˜ì.

   - ì •ê·œí™” ìˆ˜ì‹

     <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.7.png" alt="e 6.7" style="zoom:50%;" /> 

     > Î¼ : í‰ê·  /  Ïƒ : ë¶„ì‚°

   - ê³ ìœ í•œ í™•ëŒ€(scale) ì™€ ì´ë™(shift) ë³€í™˜ ìˆ˜í–‰

     <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.8.png" alt="e 6.8" style="zoom:50%;" /> 

     > Î³=1, Î²=0ìœ¼ë¡œ ì‹œì‘í•´ì„œ í•™ìŠµí•˜ë©´ì„œ ì í•©í•œ ê°’ìœ¼ë¡œ ì¡°ì •

   <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-17.png" alt="fig 6-17" style="zoom:50%;" /> 



### 3.2 ë°°ì¹˜ ì •ê·œí™”ì˜ íš¨ê³¼

> MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜í•´ë³¸ ê²°ê³¼, í•™ìŠµ ì†ë„ë¥¼ ë†’ì¸ë‹¤ëŠ” ê²°ë¡ ì´ ë„ì¶œë¨

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-19.png" alt="fig 6-19" style="zoom:50%;" /> 

- ë°°ì¹˜ ì •ê·œí™” êµ¬í˜„

```python
class BatchNormalization:
    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):
        self.gamma = gamma
        self.beta = beta
        self.momentum = momentum
        self.input_shape = None # í•©ì„±ê³± ê³„ì¸µ(convolutional layer)ì€ 4ì°¨ì›, ì™„ì „ì—°ê²° ê³„ì¸µ(fully connected layer/ affine ê³„ì¸µ)ì€ 2ì°¨ì›  

        # ì‹œí—˜í•  ë•Œ ì‚¬ìš©í•  í‰ê· ê³¼ ë¶„ì‚°
        self.running_mean = running_mean
        self.running_var = running_var  
        
        # backward ì‹œì— ì‚¬ìš©í•  ì¤‘ê°„ ë°ì´í„°
        self.batch_size = None
        self.xc = None
        self.std = None
        self.dgamma = None # Î³ ë¯¸ë¶„ê°’
        self.dbeta = None # Î² ë¯¸ë¶„ê°’

    def forward(self, x, train_flg=True):
        self.input_shape = x.shape
        if x.ndim != 2:
            N, C, H, W = x.shape
            x = x.reshape(N, -1)

        out = self.__forward(x, train_flg)
        
        return out.reshape(*self.input_shape)
            
    def __forward(self, x, train_flg):
        if self.running_mean is None:
            N, D = x.shape
            self.running_mean = np.zeros(D)
            self.running_var = np.zeros(D)
                        
        if train_flg: # ë°°ì¹˜ ì •ê·œí™” í•œë‹¤ë©´ => ì…ë ¥ ê°’ë“¤ì„ ì •ê·œí™”
            mu = x.mean(axis=0)
            xc = x - mu
            var = np.mean(xc**2, axis=0)
            std = np.sqrt(var + 10e-7)
            xn = xc / std # ì •ê·œí™” ìˆ˜ì‹ 
            
            self.batch_size = x.shape[0]
            self.xc = xc
            self.xn = xn
            self.std = std
            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            
        else:
            xc = x - self.running_mean
            xn = xc / ((np.sqrt(self.running_var + 10e-7)))
            
        out = self.gamma * xn + self.beta 
        return out

    def backward(self, dout):
        if dout.ndim != 2:
            N, C, H, W = dout.shape
            dout = dout.reshape(N, -1)

        dx = self.__backward(dout)

        dx = dx.reshape(*self.input_shape)
        return dx

    def __backward(self, dout): # Î³, Î²ì˜ ë¯¸ë¶„ê°’ êµ¬í•´ì„œ ì ì ˆí•œ ê°’ìœ¼ë¡œ ê°±ì‹ 
        dbeta = dout.sum(axis=0)
        dgamma = np.sum(self.xn * dout, axis=0)
        dxn = self.gamma * dout
        dxc = dxn / self.std
        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)
        dvar = 0.5 * dstd / self.std
        dxc += (2.0 / self.batch_size) * self.xc * dvar
        dmu = np.sum(dxc, axis=0)
        dx = dxc - dmu / self.batch_size
        
        self.dgamma = dgamma
        self.dbeta = dbeta
        
        return dx
```



---



## 4. ë°”ë¥¸ í•™ìŠµì„ ìœ„í•´

> ì‹ ê²½ë§ì´ í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ì ì‘ë˜ì–´ ë²”ìš© ì„±ëŠ¥ì„ ë‚®ì¶”ëŠ” ê²ƒì„ **`ì˜¤ë²„í”¼íŒ…`**ì´ë¼ê³  í•œë‹¤.



### 4.1 ì˜¤ë²„í”¼íŒ…

1. ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚˜ëŠ” ê²½ìš°
   - ë§¤ê°œë³€ìˆ˜ê°€ ë§ê³  í‘œí˜„ë ¥ì´ ë†’ì€ ëª¨ë¸
   - í›ˆë ¨ë°ì´í„°ê°€ ì ìŒ

- ìœ„ì˜ ìš”ê±´ì„ ì¶©ì¡±ì‹œì¼œ ì˜¤ë²„í”¼íŒ…ì„ ì¼ìœ¼ì¼œë³´ì.

  ```python
  # MNIST ë°ì´í„°ì…‹ì˜ í›ˆë ¨ë°ì´í„°ë¥¼ 300ê°œë§Œ ì‚¬ìš©í•˜ê³ , 7ì¸µì˜ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•´ë³´ì.
  (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)
  
  # ì˜¤ë²„í”¼íŒ…ì„ ì¬í˜„í•˜ê¸° ìœ„í•´ í•™ìŠµ ë°ì´í„° ìˆ˜ë¥¼ ì¤„ì„ 300ê°œë§Œ ë“¤ê³ ì˜´
  x_train = x_train[:300]
  t_train = t_train[:300]
  
  # weight decayï¼ˆê°€ì¤‘ì¹˜ ê°ì‡ ï¼‰ ì„¤ì • =======================
  weight_decay_lambda = 0 # weight decay(ê°€ì¤‘ì¹˜ ê°ì†Œ)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²½ìš°
  weight_decay_lambda = 0.1 # weight decay(ê°€ì¤‘ì¹˜ ê°ì†Œ)ë¥¼ ì‚¬ìš©í•œ ê²½ìš°, lambda = 0.1ë¡œ ì„¤ì •
  # ====================================================
  
  network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,weight_decay_lambda=weight_decay_lambda)
  optimizer = SGD(lr=0.01) # í•™ìŠµë¥ ì´ 0.01ì¸ SGDë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
  
  max_epochs = 201
  train_size = x_train.shape[0] # 300
  batch_size = 100
  
  train_loss_list = []
  train_acc_list = []
  test_acc_list = []
  
  iter_per_epoch = max(train_size / batch_size, 1) # 3
  epoch_cnt = 0
  
  for i in range(1000000000):
      batch_mask = np.random.choice(train_size, batch_size)
      x_batch = x_train[batch_mask]
      t_batch = t_train[batch_mask]
  
      grads = network.gradient(x_batch, t_batch)
      optimizer.update(network.params, grads)
  
      if i % iter_per_epoch == 0:
          train_acc = network.accuracy(x_train, t_train)
          test_acc = network.accuracy(x_test, t_test)
          train_acc_list.append(train_acc)
          test_acc_list.append(test_acc)
  
          print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))
  
          epoch_cnt += 1
          if epoch_cnt >= max_epochs:
              break
  ```

    

  <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/Figure_4.png" alt="Figure_4" style="zoom:50%;" /> <img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/Figure_5.png" alt="Figure_5" style="zoom:50%;" />

> ì™¼ìª½ ê·¸ë˜í”„ì²˜ëŸ¼ ì •í™•ë„ê°€ í¬ê²Œ ë²Œì–´ì§€ëŠ” ê²ƒì€ í›ˆë ¨ë°ì´í„°ì—ë§Œ ì ì‘í•´ë²„ë¦° ê²°ê³¼ì´ë‹¤.

> ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ëŠ” ê°€ì¤‘ì¹˜ ê°ì†Œë¥¼ ì‚¬ìš©í•œ ê²½ìš°ì´ë‹¤.



### 4.2 ê°€ì¤‘ì¹˜ ê°ì†Œ(weight decay)

> ê°€ì¤‘ì¹˜ê°€ ì‘ì„ìˆ˜ë¡ ì˜¤ë²„í”¼íŒ…ì€ ì–µì œëœë‹¤.  ê°€ì¤‘ì¹˜ì˜ `ì œê³± ë…¸ë¦„(L2 norm)`ì„ <u>ì†ì‹¤í•¨ìˆ˜ì— ë”í•˜ë©´</u> ê°€ì¤‘ì¹˜ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ì–µì œí•  ìˆ˜ ìˆë‹¤.  => ê°€ì¤‘ì¹˜ê°€ í´ìˆ˜ë¡ íŒ¨ë„í‹°(ì†ì‹¤ì— ëŒ€í•œ)ë¥¼ ì¤€ë‹¤.

* ì œê³± ë…¸ë¦„(L2 norm) : ê° ì›ì†Œì˜ ì œê³±ë“¤ì„ ë”í•œ ê²ƒ

```python
def loss(self, x, t, train_flg=False):
"""ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤.
	Parameters
	----------
	x : ì…ë ¥ ë°ì´í„°
	t : ì •ë‹µ ë ˆì´ë¸” 
"""

y = self.predict(x, train_flg)

weight_decay = 0
for idx in range(1, self.hidden_layer_num + 2):
    W = self.params['W' + str(idx)]
    weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2) # ê°€ì¤‘ì¹˜ ê°ì†Œ ì‚¬ìš©í•˜ì—¬ ì˜¤ë²„í”¼íŒ… ë§‰ìŒ

return self.last_layer.forward(y, t) + weight_decay # lossì— ê°€ì¤‘ì¹˜ ê°ì†Œ ë”í•´ì¤Œ 
```

> Î»(weight_decay_lambda) : ì •ê·œí™”ì˜ ì„¸ê¸°ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.12.png" alt="e 6.12" style="zoom:67%;" /> 

- ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•  ê²ƒê°™ë‹¤.

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/e 6.13.png" alt="e 6.13" style="zoom:67%;" /> 

> ê°€ì¤‘ì¹˜ ê°ì†Œë€ ê°€ì¤‘ì¹˜ì˜ `í¬ê¸°`ê°€ ê°ì†Œëœë‹¤ëŠ” ëœ»ìœ¼ë¡œ ì´í•´!!



### 4.3 ë“œë¡­ì•„ì›ƒ(Dropout)

> ë‰´ëŸ°ì„ ì„ì˜ë¡œ ì‚­ì œí•˜ë©´ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•

```python
class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None

    def forward(self, x, train_flg=True):
        if train_flg:
            # xì™€ í˜•ìƒì´ ê°™ì€ ë°°ì—´ì„ ë¬´ì‘ìœ„ë¡œ ìƒì„± 
            # dropout_ratioë³´ë‹¤ í° ê°’ë“¤ì€ Trueë¡œ í‘œì‹œ, ì‘ì€ ê°’ë“¤ì€ Falseë¡œ í‘œì‹œ
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)

    def backward(self, dout):
        # ìˆœì „íŒŒë•Œ í†µê³¼í•œ ë‰´ëŸ°ì€ ì—­ì „íŒŒë•Œë„ í†µê³¼í•œë‹¤.
        return dout * self.mask
```

<img src="ë°‘ì‹œë”¥ 6. í•™ìŠµ ê´€ë ¨ ê¸°ìˆ ë“¤.assets/fig 6-23.png" alt="fig 6-23" style="zoom:50%;" /> 

> í‘œí˜„ë ¥ì„ ë†’ì´ë©´ì„œ ì˜¤ë²„í”¼íŒ…ì„ ì–µì œí•˜ëŠ” ë°©ë²•

- ì•™ìƒë¸” í•™ìŠµ(ensemble learning) : ê°œë³„ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¶œë ¥ì„ í‰ê· ë‚´ì–´ ì¶”ë¡ í•˜ëŠ” ë°©ì‹

  > ë“œë¡­ì•„ì›ƒì€ ì•™ìƒë¸” í•™ìŠµê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ì–»ëŠ”ë‹¤.



---



## 5. ì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ ì°¾ê¸°

> í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ì ì ˆíˆ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§ˆ ê²ƒì´ë‹¤.
>
> í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°’ì„ ìµœëŒ€í•œ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì.



### 5.1 ê²€ì¦ ë°ì´í„°(validation data)

> í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•ŒëŠ” ì‹œí—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì„œ ì•ˆ ëœë‹¤.

â€‹	ğŸ¤·â€â™€ï¸ why? í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì´ ì‹œí—˜ ë°ì´í„°ì—  ì˜¤ë²„í”¼íŒ…ë˜ê¸° ë•Œë¬¸

â€‹			ğŸ‘‰ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ìš© ë°ì´í„° = validation data

| data        | ì—­í•                      |
| ----------- | ------------------------ |
| í›ˆë ¨ ë°ì´í„° | ë§¤ê°œë³€ìˆ˜ í•™ìŠµ            |
| ê²€ì¦ ë°ì´í„° | í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„±ëŠ¥ í‰ê°€ |
| ì‹œí—˜ ë°ì´í„° | ì‹ ê²½ë§ì˜ ë²”ìš© ì„±ëŠ¥ í‰ê°€  |

```python
# 20%ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„í• 
validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)

# train, validationìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì „ì— ë°ì´í„° shuffle
x_train, t_train = shuffle_dataset(x_train, t_train)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]
```

```python
def shuffle_dataset(x, t):
    permutation = np.random.permutation(x.shape[0]) 
    # 0~x.shape[0]-1ì˜ ìˆ«ìë“¤ì´ ì„ì„
    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]
    # permutation ì¡°í•©
    t = t[permutation]

    return x, t
```



### 5.2 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

> í•µì‹¬ : í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ `ìµœì ê°’` ì´ ì¡´ì¬í•˜ëŠ” ë²”ìœ„ë¥¼ ì¡°ê¸ˆì”© ì¤„ì—¬ê°„ë‹¤ëŠ” ê²ƒ

1. ëŒ€ëµì ì¸ ë²”ìœ„ë¥¼ ì„¤ì •
2. ê·¸ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ê³¨ë¼ëƒ„(ìƒ˜í”Œë§)
3. ì •í™•ë„ë¥¼ í‰ê°€

- ë”¥ëŸ¬ë‹ í•™ìŠµì—ëŠ” ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ì—í­ ì‘ê²Œ í•˜ì—¬ ì‹œê°„ì„ ë‹¨ì¶•í•˜ëŠ” ê²ƒì´ íš¨ê³¼ì 



### 5.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” êµ¬í˜„í•˜ê¸°

> MNIS ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ í•™ìŠµë¥ ê³¼ ê°€ì¤‘ì¹˜ ê°ì†Œì˜ ê³„ìˆ˜ë¥¼ ìµœì í™”í•´ë³´ì.

```python
# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¬´ì‘ìœ„ íƒìƒ‰======================================
optimization_trial = 100
results_val = {}
results_train = {}
for _ in range(optimization_trial):
    # íƒìƒ‰í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ ì§€ì •===============
    weight_decay = 10 ** np.random.uniform(-8, -4) # 10^(-8) ~10^(-4) ì¤‘ ìƒ˜í”Œë§
    lr = 10 ** np.random.uniform(-6, -2) # 10^(-6) ~ 10^(-2)
    # ================================================

    val_acc_list, train_acc_list = __train(lr, weight_decay)
    print("val acc:" + str(val_acc_list[-1]) + " | lr:" + str(lr) + ", weight decay:" + str(weight_decay))
    key = "lr:" + str(lr) + ", weight decay:" + str(weight_decay)
    results_val[key] = val_acc_list
    results_train[key] = train_acc_list
```
