# 밑시딥2 📂8. 어텐션

이전 장에서는 RNN과 seq2seq에 대해 배워보았다.

이번 장에서는 seq2seq 가능성과 RNN 가능성에 대해 깊게 살펴본다.

목표는 어텐션의 구조를 코드 수준에서 이해하는 것, 실제 문제에 적용해보는 것이다.



## 1. 어텐션의 구조

seq2seq는 다양하게 응용할 수 있다. 지금까지 배운 seq2seq를 한층 더 강력하게 하는 **`어텐션 매커니즘`**을 알아보자.

📍 어텐션 매커니즘 : seq2seq가 필요한 정보에 주목할 수 있도록 만들고 seq2seq의 문제점 개선이 가능하다.



### 1.1 seq2seq의 문제점

seq2seq에서는 Encoder가 시계열 데이터를 인코딩한다. 이 인코딩된 정보를 Decoder로 전달한다. 이때 정보는 '고정 길이의 벡터'이다.

고정 길이 벡터는 입력 문장의 길이에 관계없이 항상 같은 길이의 벡터로 변환한다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-1.png" alt="fig 8-1" style="zoom:50%;" />



🙄 이 때, 필요한 정보가 벡터에 다 담기지 못하는 문제점 발생!



### 1.2 Encoder 개선

기존 seq2seq 구조는 LSTM 계층의 마지막 은닉 상태만 Decoder 에 전달했다.

🌟**<u>Encoder의 출력 길이는 입력 문장의 길이에 따라 바꿔주는 것이 좋다.</u>**

<img src="밑시딥2 8. 어텐션.assets/fig 7-9.png" alt="fig 7-9" style="zoom:50%;" />

 <img src="밑시딥2 8. 어텐션.assets/fig 8-2.png" alt="fig 8-2" style="zoom:50%;" />

🙄 개선된 seq2seq의 구조에서 LSTM 계층의 은닉상태에는 어떠한 정보가 담겨 있나?

각 시각의 은닉 상태에는 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다.

Encoder가 출력하는 hs 행렬은 각 단어에 해당하는 벡터들의 집합이라고 생각할 수 있다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-3.png" alt="fig 8-3" style="zoom:50%;" /> 



### 1.3 Decoder 개선1️⃣

<img src="밑시딥2 8. 어텐션.assets/fig 8-4.png" alt="fig 8-4" style="zoom:50%;" /> 

Decoder는 Encoder의 정보인 hs를 전달받아 시계열 변환을 담당한다.



기존 seq2seq의 decoder 계층 구성은 아래와 같다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-5.png" alt="fig 8-5" style="zoom:50%;" /> 

Encoder의 LSTM 계층의 마지막 은닉 상태를 Decoder의 LSTM 계층의 첫 은닉 상태로 설정한다.

때문에 **<u>hs 전부를 활용</u>**할 수 있도록 decoder를 개선시켜야한다.

우리는 번역을 할 때 단어 하나마다의 의미를 생각한다. 즉, 어떤 단어에 주목하여 그 단어의 변환을 수시로 반복한다.

그렇다면 seq2seq에게 '입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가'라는 대응 관계를 학습시킬 수 있는가?



우리의 목표는 <u>필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행하는 것</u>이다. 이 구조를 **<u>어텐션</u>**이라 부른다.

이번 장에서 구현하는 신경망 계층의 구성은 아래와 같다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-6.png" alt="fig 8-6" style="zoom:50%;" />

추가된 '어떤 계산'을 수행하기 위해서는 2가지의 입력을 받아야한다. 

하나는 encoder의 hs, 다른 하나는 시각별 LSTM계층의 은닉 상태이다.

어텐션 구조로 부터 도출하고 싶은 것은 단어들의 얼라인먼트(단어의 대응 관계를 나타내는 정보) 추출이다.

즉, 각 시각에서 decoder에 입력된 단어와 대응 관계인 단어의 벡터를 hs에서 선택하겠다는 뜻이다.

🙄 이러한 선택의 과정을 '어떤 계산'을 통해 구현하는 것이다. 여기서 문제점은 선택한느 작업은 미분을 할 수 없다는 것이다.

👉 문제의 해결 방법은 간단하다. '모든 것을 선택'하는 것이다. 이때 각 단어의 중요도를 나타내는 가중치를 별도로 계산하다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-7.png" alt="fig 8-7" style="zoom:50%;" /> 

<img src="밑시딥2 8. 어텐션.assets/fig 8-8.png" alt="fig 8-8" style="zoom:50%;" /> 

그림과 같이 단어 벡터의 가중합을 계산하고 그 결과를 '맥락 벡터'라고 부른다.

```python
T, H = 5, 4
hs = np.random.randn(T, H)
a= np.array([0.8, 0.1, 0.03, 0.05, 0.02])
# 중요도

ar = a.reshape(5, 1).repeat(4, axis = 1)
# (5, 4)
t=hs*ar # 원소별 곱
# (5, 4)
c = np.sum(t, axis=0)
# (4, )
```

<img src="밑시딥2 8. 어텐션.assets/fig 8-9.png" alt="fig 8-9" style="zoom:50%;" /> 

<img src="밑시딥2 8. 어텐션.assets/fig 8-10.png" alt="fig 8-10" style="zoom:50%;" /> 



- 미니배치 처리용 가중합 구현

```python
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
a= np.random.randn(N, T)
# 중요도

ar = a.reshape(N, T, 1).repeat(H, axis = 2)
t=hs*ar # 원소별 곱
# (10, 5, 4)
c = np.sum(t, axis=1)
# (10, 4)
```

<img src="밑시딥2 8. 어텐션.assets/fig 8-11.png" alt="fig 8-11" style="zoom:50%;" /> 



- 역전파 구현

```python
class WeightSum:
    def __init__(self):
        self.params, self.grads = [], []
        self.cache = None

    def forward(self, hs, a):
        N, T, H = hs.shape

        ar = a.reshape(N, T, 1) #.repeat(T, axis=1)
        t = hs * ar
        c = np.sum(t, axis=1)

        self.cache = (hs, ar)
        return c

    def backward(self, dc):
        hs, ar = self.cache
        N, T, H = hs.shape
        # sum의 역전파는 repeat
        dt = dc.reshape(N, 1, H).repeat(T, axis=1)
        dar = dt * hs
        dhs = dt * ar
        
        # repeat의 역전파는 sum
        da = np.sum(dar, axis=2)

        return dhs, da
```



### 1.4 Decoder 개선2️⃣

🙄 위에서 설정한 가중치 a는 어떻게 구해야하는지 알아보자.

우선 decoder의 첫번째 시각 LSTM 계층이 은닉 상태 벡터를 출력할 떄가지의 처리부터 살펴본다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-12.png" alt="fig 8-12" style="zoom:50%;" /> 

decoder의 LSTM 계층의 은닉 상태를 h라고 한다. 이 h가 hs의 각 단어 벡터와 얼마나 비슷한지를 수치로 나타내야한다.

간단한 방법은 벡터의 내적을 이용하는 것이다.

내적의 수식은 아래와 같다.

<img src="밑시딥2 8. 어텐션.assets/e 8-1.png" alt="e 8-1" style="zoom:50%;" /> 

내적은 두 벡터가 얼마나 같은 방향을 향하고 있는지(유사도)를 나타낸다.

내적을 이용한 벡터 유사도 산출 처리는 아래와 같이 나타낼 수 있다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-13.png" alt="fig 8-13" style="zoom:50%;" /> 

```python
N, T, H = 10, 5, 4
hs = np.random.randn(N, T, H)
h = np.random.randn(N, T)
hr = a.reshape(N, T, 1).repeat(H, axis = 1)

t=hs*ar
# (10, 5, 4)
s = np.sum(t, axis=2)
# (10, 5)

softmax = Softmax()
a = softmax.forward(s)
# (10, 5)
```

<img src="밑시딥2 8. 어텐션.assets/fig 8-15.png" alt="fig 8-15" style="zoom:50%;" /> 

```python
class AttentionWeight:
    def __init__(self):
        self.params, self.grads = [], []
        self.softmax = Softmax()
        self.cache = None

    def forward(self, hs, h):
        N, T, H = hs.shape

        hr = h.reshape(N, 1, H) # 브로드캐스트 사용하는 경우
        t = hs * hr
        s = np.sum(t, axis=2)
        a = self.softmax.forward(s)

        self.cache = (hs, hr)
        return a

    def backward(self, da):
        hs, hr = self.cache
        N, T, H = hs.shape

        ds = self.softmax.backward(da)
        dt = ds.reshape(N, T, 1).repeat(H, axis=2)
        dhs = dt * hr
        dhr = dt * hs
        dh = np.sum(dhr, axis=1)

        return dhs, dh
```



### 1.5 Decoder 개선3️⃣

앞에서 구현하는 weight sum 과 attention weight 계층을 하나로 결합해본다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-16.png" alt="fig 8-16" style="zoom:50%;" /> 

```python
class Attention:
    def __init__(self):
        self.params, self.grads = [], []
        self.attention_weight_layer = AttentionWeight()
        self.weight_sum_layer = WeightSum()
        self.attention_weight = None

    def forward(self, hs, h):
        a = self.attention_weight_layer.forward(hs, h)
        out = self.weight_sum_layer.forward(hs, a)
        self.attention_weight = a
        return out

    def backward(self, dout):
        dhs0, da = self.weight_sum_layer.backward(dout)
        dhs1, dh = self.attention_weight_layer.backward(da)
        dhs = dhs0 + dhs1
        return dhs, dh
```

<img src="밑시딥2 8. 어텐션.assets/fig 8-18.png" alt="fig 8-18" style="zoom:50%;" />

마지막으로 시계열 방향으로 펼쳐진 다수의 Attention 계층을 Time Ateention 계층으로 모아 구현한다.

```python
class TimeAttention:
    def __init__(self):
        self.params, self.grads = [], []
        self.layers = None
        self.attention_weights = None

    def forward(self, hs_enc, hs_dec):
        N, T, H = hs_dec.shape # T : Attention 계층의 수
        out = np.empty_like(hs_dec)
        self.layers = []
        self.attention_weights = []

        for t in range(T):
            layer = Attention()
            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])
            self.layers.append(layer)
            self.attention_weights.append(layer.attention_weight)

        return out

    def backward(self, dout):
        N, T, H = dout.shape
        dhs_enc = 0
        dhs_dec = np.empty_like(dout)

        for t in range(T):
            layer = self.layers[t]
            dhs, dh = layer.backward(dout[:, t, :])
            dhs_enc += dhs
            dhs_dec[:,t,:] = dh

        return dhs_enc, dhs_dec
```



---





## 2. 어텐션을 갖춘 seq2seq 구현



### 2.1 Encoder 구현

```python
class AttentionEncoder(Encoder): # Encoder 상속
    def forward(self, xs):
        xs = self.embed.forward(xs)
        hs = self.lstm.forward(xs)
        return hs # 모든 은닉상태를 반환

    def backward(self, dhs):
        dout = self.lstm.backward(dhs)
        dout = self.embed.backward(dout)
        return dout
```



### 2.2 Decoder 구현

<img src="밑시딥2 8. 어텐션.assets/fig 8-21.png" alt="fig 8-21" style="zoom:50%;" /> 

```python
class AttentionDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn

        embed_W = (rn(V, D) / 100).astype('f')
        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b = np.zeros(4 * H).astype('f')
        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')
        affine_b = np.zeros(V).astype('f')

        self.embed = TimeEmbedding(embed_W)
        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
        self.attention = TimeAttention()
        self.affine = TimeAffine(affine_W, affine_b)
        layers = [self.embed, self.lstm, self.attention, self.affine]

        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, xs, enc_hs):
        h = enc_hs[:,-1]
        self.lstm.set_state(h)

        out = self.embed.forward(xs)
        dec_hs = self.lstm.forward(out)
        
        # Time Attention 계층의 출력과 LSTM 계층의 출력을 연결
        c = self.attention.forward(enc_hs, dec_hs)
        out = np.concatenate((c, dec_hs), axis=2)
        
        score = self.affine.forward(out)

        return score

    def backward(self, dscore):
        dout = self.affine.backward(dscore)
        N, T, H2 = dout.shape
        H = H2 // 2

        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]
        denc_hs, ddec_hs1 = self.attention.backward(dc)
        ddec_hs = ddec_hs0 + ddec_hs1
        dout = self.lstm.backward(ddec_hs)
        dh = self.lstm.dh
        denc_hs[:, -1] += dh
        self.embed.backward(dout)

        return denc_hs

    # 새로운 단어열 생성
    def generate(self, enc_hs, start_id, sample_size):
        sampled = []
        sample_id = start_id
        h = enc_hs[:, -1]
        self.lstm.set_state(h)

        for _ in range(sample_size):
            x = np.array([sample_id]).reshape((1, 1))

            out = self.embed.forward(x)
            dec_hs = self.lstm.forward(out)
            c = self.attention.forward(enc_hs, dec_hs)
            out = np.concatenate((c, dec_hs), axis=2)
            score = self.affine.forward(out)

            sample_id = np.argmax(score.flatten())
            sampled.append(sample_id)

        return sampled
```



### 2.3 seq2seq 구현

```python
class AttentionSeq2seq(Seq2seq): # seq2seq 상속
    def __init__(self, vocab_size, wordvec_size, hidden_size):
        args = vocab_size, wordvec_size, hidden_size
        self.encoder = AttentionEncoder(*args)
        self.decoder = AttentionDecoder(*args)
        self.softmax = TimeSoftmaxWithLoss()

        self.params = self.encoder.params + self.decoder.params
        self.grads = self.encoder.grads + self.decoder.grads
```



---





## 3. 어텐션 평가

날짜 형식을 변경하는 문제로 어텐션을 갖춘 seq2seq 효과를 확인해본다.



### 3.1 날짜 형식 변환 문제

<img src="밑시딥2 8. 어텐션.assets/fig 8-22.png" alt="fig 8-22" style="zoom:50%;" /> 

<img src="밑시딥2 8. 어텐션.assets/fig 8-23.png" alt="fig 8-23" style="zoom:50%;" /> 



### 3.2 어텐션을 갖춘 seq2seq의 학습

```python
# 데이터 읽기
(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')
char_to_id, id_to_char = sequence.get_vocab()

# 입력 문장 반전
x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]

# 하이퍼파라미터 설정
vocab_size = len(char_to_id)
wordvec_size = 16
hidden_size = 256
batch_size = 128
max_epoch = 10
max_grad = 5.0

model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)
# model = Seq2seq(vocab_size, wordvec_size, hidden_size)
# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)

optimizer = Adam()
trainer = Trainer(model, optimizer)

acc_list = []
for epoch in range(max_epoch):
    trainer.fit(x_train, t_train, max_epoch=1,
                batch_size=batch_size, max_grad=max_grad)

    correct_num = 0
    for i in range(len(x_test)):
        question, correct = x_test[[i]], t_test[[i]]
        verbose = i < 10
        correct_num += eval_seq2seq(model, question, correct,
                                    id_to_char, verbose, is_reverse=True)

    acc = float(correct_num) / len(x_test)
    acc_list.append(acc)
```

<img src="밑시딥2 8. 어텐션.assets/fig 8-25.png" alt="fig 8-25" style="zoom: 33%;" /> <img src="밑시딥2 8. 어텐션.assets/fig 8-26.png" alt="fig 8-26" style="zoom:33%;" />





### 3.3 어텐션 시각화

Attention 계층은 각 시각의 어텐션 가중치를 인스턴스 변수로 보관하고 있다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-27.png" alt="fig 8-27" style="zoom:50%;" />

시각화를 통해 모델이 수행하는 순간에 어디에 집중하는 확인할 수 있다.



---





## 4. 어텐션에 관한 남은 이야기



### 4.1 양방향 RNN

앞장까지 배운 Encoder 계층에 집중해본다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-29.png" alt="fig 8-29" style="zoom:50%;" /> 

LSTM을 양방향으로 처리하는 방법을 생각할 수 있다. 이것이 양방향 LSTM기술이다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-30.png" alt="fig 8-30" style="zoom:50%;" /> 

각 시각에서는 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉 상태로 처리한다.

이렇게 양방향으로 은닉 상태를 처리하면 각 단어에 대응하는 은닉 상태 벡터에는 양쪽 방향으로부터의 정보를 집약할 수 있다.



### 4.2 어텐션 계층 사용 방법

어텐션 계층의 위치는 자유롭게 설정할 수 있다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-31.png" alt="fig 8-31" style="zoom:50%;" /> 

<img src="밑시딥2 8. 어텐션.assets/fig 8-32.png" alt="fig 8-32" style="zoom:50%;" /> 



### 4.3 seq2seq 심층화와 skip 연결

복잡한 문제 풀이를 위해서는 더 높은 성능의 어텐션을 갖춘 seq2seq가 필요한다.

LSTM 계층을 깊게 쌓는 방법이 있다.

깊게 층을 쌓으면 표현력 높은 모델을 만들 수 있다.

<img src="밑시딥2 8. 어텐션.assets/fig 8-33.png" alt="fig 8-33" style="zoom:50%;" /> 

층을 깊게 할 떄 사용되는 중요한 기법 중 skip 연결이 있다. 

<img src="밑시딥2 8. 어텐션.assets/fig 8-34.png" alt="fig 8-34" style="zoom:50%;" /> 

skip 연결은 계층을 건너뒤는 연결이다. skip 연결의 접속부에서는 2개의 출력이 더해진다. 

(더해지는 것의 역전파에서 기울기 소실되지 않고 전파된다.)



---





## 5. 어텐션 응용

어텐션의 아이디어는 범용적으로 활용이 가능하다.



### 5.1 구글 신경망 기계 번역(GNMT)

<img src="밑시딥2 8. 어텐션.assets/fig 8-35.png" alt="fig 8-35" style="zoom:50%;" /> 

- LSTM 다층화, 양방향 LSTM, skip 연결 등



### 5.2 트랜스포머

RNN의 단점으로 병렬 처리를 들 수 있다.

이전 시각에 계산한 결과를 이용하여 순서대로 계산하여야하므로 병렬 계산이 불가능하다.

때문에 RNN을 없애는 연구가 활발히 이뤄지고 있다. 그 중하나가 트랜스포머이다.

트랜스포머 : 셀프어텐션(하나의 시계열 데이터를 대상으로 한 어텐션)을 사용한 모델

<img src="밑시딥2 8. 어텐션.assets/fig 8-37.png" alt="fig 8-37" style="zoom:50%;" /> 

<img src="밑시딥2 8. 어텐션.assets/fig 8-38.png" alt="fig 8-38" style="zoom:50%;" /> 

셀프어텐션은 두 입력선 모두 하나의 시계열 데이터로부터 나오기때문에 하나의 시계열 데이터 내에서의 원소 간 대응 관계가 구해진다.



### 5.3 뉴럴 튜링 머신(NTM)

NTM : 외부 메모리를 읽고 쓰면서 시계열 데이터를 처리한다.

어텐션을 통해 encoder, decoder는 메모리 조작같은 작업을 수행한다.

즉, encoder가 필요한 정보를 메모리에 쓰고, decoder는 그 메모리로부터 필요한 정보를 읽어 들인다.

RNN의 외부에 정보 정보 저장용 메모리 기능을 배치하고 어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법이다.

컨트롤러라는 모듈은정보를 처리하는 모듈로 신경망을 이용한다.

메모리는 정보를 쓰거나 불필요한 정보를 지우는 능력이 있다. 

<img src="밑시딥2 8. 어텐션.assets/fig 8-41.png" alt="fig 8-41" style="zoom:50%;" /> 

NTM은 컴퓨터 메모리 조작을 모방하기 위해 2개의 어텐션을 이용한다.

콘텐츠 기반 어텐션 + 위치 기반 어텐션(입력 벡터와 비슷한 벡터를 메모리에서 찾는 용도)

