# 밑시딥 📂3. 신경망(Neural network)

> 앞에서 배운 퍼셉트론으로 우리는 복잡한 함수도 표현할 수 있음을 배웠다. 하지만 퍼셉트론의 가중치의 설정은 인간이 수동으로 정했다는 점을 잊으면 안된다.
>
> 이러한 문제를 해결하기 위해 신경망에 대해 살펴보자.
>
> ✨**`신경망은 가중치의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 말한다.`**



## 1. 퍼셉트론에서 신경망으로

<img src="밑시딥 3. 신경망.assets/3.PNG" alt="3" style="zoom:67%;" /> 

- 신경망의 표현은 위와 같다. 입력층, 은닉층, 출력층으로 나뉜다.



### 1.1 활성화 함수(Activation function)

>  단층 퍼셉트론 수식을 통해 활성화 함수에 대해 알아보자.

- 단층 퍼셉트론 수식

  `y=b + w1x1 + w2x2` (b=bias, w=weight/가중치)

  👉 bias : 뉴런이 얼마나 쉽게 활성화되는가

  👉 weight : 각 신호의 영향력 제어

  - 위의 수식을 더 간결한 형태로 바꾸기 위해 **활성화 함수 h(x)**를 사용하자.

    `y=h( b + w1x1 + w2x2 )`

    👉 **활성화 함수 h(x)** : 입력이 0을 넘으면 1 반환, 아니면 0 반환

    ​	    ✅ 입력 신호의 총합을 출력 신호로 변환하는 함수. 입력 신호의 총합이 활성화를 일으키는지 정하는 역할

  - 입력의 총합을 a라하면,

    `a=b + w1x1 + w2x2` , `y=h(a)`



## 2. 📌활성화 함수 종류

> 지금까지 배웠던 활성화 함수는 임계값을 경계로 출력이 바뀌는 계단 함수(step function)이다.
>
> 그렇다면 다른 활성화 함수를 적용하면 어떤 퍼셉트론이 될까? 신경망에서 이용하는 활성화 함수를 알아보자.

### 2.1 계단 함수

`h(x)=0,1`

- 위의 식을 구현하기 위해 아래와 같은 코드를 만든다. 

  ```python
  import numpy as np
  
  def step_function(x):
      
      # x는 배열이지만 y는 [False, True, True]의 bool 배열이 됨
      y=x >0 
      
      # bool 배열을 int형으로 출력하기 위해 자료형 변환하는 메서드 astype 사용
      return y.astype(np.int)
  
  x=np.array([-1,1,2])
  print(step_function(x))
  
  # [0,1,1]
  ```

   

### 2.2 시그모이드 함수(sigmoid function)

> 's' 자 모양이라는 뜻. 

`h(x)=1/(1-exp(-x))`

- 위의 식을 구현하기 위해 아래와 같은 코드를 만든다. 

  ```python
  def sigmoid(x):
      return 1/(1+np.exp(-x))
  
  x=np.arange(-5.0, 5.0, 0.1)
  y=sigmoid(x)
  
  plt.plot(x,y)
  plt.ylim(-0.1 ,1.1) # y축 범위 설정
  plt.show()
  ```

 <img src="밑시딥 3. 신경망.assets/3.1.PNG" alt="3.1" style="zoom: 80%;" />

#### 🙋‍♀️ 여기서 계단 함수와 시그모이드 함수를 비교해보자.

- 차이점
  - **`매끄러움`** : 입력에 따라 출력이 연속적으로 변화
  - **`연속적인 실수 값`** 

- 공통점

  - 입력이 커지면 1에 가까워지고 입력이 작으면 0에 가까워짐 

    👉 입력이 중요하면 큰값을 출력, 그 반대면 작은값 출력

    👉 출력의 한계가 1과 0임

  - **`비선형 함수`** 



### 2.3 ReLU 함수

> Rectified Linear Unit. 최근에는 ReLU 함수 주로 이용

`0과 x 중 큰 값을 출력`

```python
def ReLU(x):
    return np.maximum(0,x) # 0이하일 때는 0, 0이상일 때는 x(본인)을 출력

x=np.arange(-5.0, 5.0, 0.1)
y3=ReLU(x)
```

<img src="밑시딥 3. 신경망.assets/3.2.PNG" alt="3.2" style="zoom:80%;" /> 



## 3. 다차원 배열의 계산

> 넘파이의 다차원 배열을 통해 신경망의 효율적인 구현 가능

- np.ndim() : 배열의 차원수
- .shape() : 형상(원소 개수). 항상 튜플로 반환!!



### 3.1 행렬

> 2차원 배열 = 행렬(matrix). 행=가로/열=세로

- 🌟**`np.dot(a,b)`** : 행렬의 곱. 1차원 배열이 입력으로 들어오면 벡터, 2차원 배열이 들어오면 행렬 곱을 계산

🔥행렬의 곱에서 중요한 점은 `대응하는 차원의 원소 수를 일치시키는 것`



## 4. 3층 신경망 구현하기

> 3층 신경망을 순방향(forward) 처리 방법으로 파이썬 구현을 해보자.

```python
import numpy as np

# 시그모이드 활성화 함수 생성
def sigmoid(x):
    return 1/(1+np.exp(-x))

# 본인을 반환하는 함수 생성
def identity_function(x):
    return x

# 신경망 가중치와 편향 값 dict 형태로 할당
def network():
    
    network={}
    
    network['W1']=np.array([[0.1, 0.3, 0.5],[0.2, 0.4, 0.6]])
    network['B1']=np.array([0.1, 0.2, 0.3])
    network['W2']=np.array([ [0.1,0.4], [0.2,0.5], [0.3,0.6]])
    network['B2']=np.array([ 0.1, 0.2 ])
    network['W3']=np.array([ [0.1,0.3], [0.2,0.4] ])
    network['B3']=np.array([ 0.1, 0.2 ])
    
    return network
# 순방향 처리 함수
def forward(network, x):
    
    W1 ,W2 ,W3 = network['W1'], network['W2'], network['W3']
    B1, B2, B3 = network['B1'], network['B2'], network['B3']
    
    # 0층에서 총합 구하기 
    A1=np.dot(x,W1)+B1
    #[0.3 0.7 1.1]
    
    # 1층 입력값 구하기
    Z1=sigmoid(A1)
    # [0.57444252 0.66818777 0.75026011]
    
    # 1층 총합 구하기
    A2=np.dot(Z1,W2)+B2
    # [0.51615984 1.21402696]
    
    # 2층 입력값 구하기
    Z2=sigmoid(A2)
    # [0.62624937 0.7710107]
    
    # 2층 총합 구하기
    A3=np.dot(Z2,W3)+B3
    # [0.31682708 0.69627909]
    
    # 결과값 구하기
    Y=identity_function(A3)
    # [0.31682708 0.69627909]
    return Y

network=network()

Y=forward(network,np.array([1.0,0.5]))
print(Y)
#[0.31682708 0.69627909]
```



## 5. 출력츨 설계하기

> 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수 달라짐

- 분류(classification) : 데이터가 어느 클래스(class)에 속하느냐 ex) 성별, 혈액형 ...
- 회귀(regression) :  데이터의 수치 예측 ex) 신발사이즈,  몸무게 ... 



### 5.1 항등 함수(identity func) & 소프트맥스 함수(soft max func)

- 항등 함수 : 입력을 그대로 출력

- **`소프트맥스 함수`** 

  `y_k=exp(a_k)/sum(exp(a))` 

  ```python
  def softmax(a):
      exp_a=np.exp(a)
      sum_exp_a=sum(exp_a)
      y=exp_a/sum_exp_a
      
      return y
  ```

   🙋‍♀️ 여기서 문제점은 `오버플로`이다. 오버플로(overflow)란 너무 큰 값은 표현할 수 없는 것을 뜻한다. 지수함수란 쉽게 큰 값을 반환하기 때문이다.  ===> 학습시에는 사용 why?  0~1사이로 일반화해주지 않으면 역전파(weight 업데이트)의 loss 값이 커짐

  🔑 이를 해결하기위해 a의 값을 c(a+c')로 대체한다. c'의 값은 입력 신호 중 최댓값을 이용하는 것이 일반적이다.

  ```python
  def softmax(a):
      
      c=np.max(a) # 입력값 중 최댓값
      exp_a=np.exp(a-c) # 최댓값에서 빼줌 ==> 작은 값으로 exp 이용 가능
      sum_exp_a=np.sum(exp_a)
      y=exp_a/sum_exp_a
      
      return y
  ```



- 소프트맥스 함수의 특징

  🔥**`출력이 0~1 사이이다. 출력의 총합은 1이다`**🔥 ==> `확률`로 해석 가능

  - 신경망 이용한 분류는 일반적으로 가장 큰 출려을 내는 뉴런에 해당하는 클래스로만 인식 

    ==> 지수함수 계산에 드는 자원 낭비 줄이기 위해 출력층의 소프트맥스 함수는 생략하는 것이 일반적



## 6. 손글씨 숫자 인식

> MNIST 데이터셋을 이용하는 손글씨를 숫자로 인식하는 실습을 해보자. 이번에는 매개변수 학습 과정은 생략하고 추론 과정만 구현할 것이다.



### 6.1 MNIST 데이터셋

> train set 60,000장 
>
> test set 10,000장
>
> (28 X 28), 0~255 픽셀



```python
import sys, os
# os 모듈 : 파일과 폴더 만들고 복사하는 등의 운영체제(operation system) 제어 가능
# sys 모듈 : 파이썬 인터프리터 제어 가능
sys.path.append('C:\\Users\\yeomg\\ssafy5\\밑시딥 연습\\deep-learning-from-scratch-master')
# 환경 경로 추가
from dataset.mnist import load_mnist
# dataset.mnist에서 load_mnist import

(x_train, t_train),(x_test, t_test)=load_mnist(flatten=True, normalize=False)
# load_mnist : (train_data, train_label),(test_data, test_label) 로 반환
# flatten : 입력 이미지를 평탄하게(1차원 배열) 만듦. 
## normalize(정규화) : 입력 이미지의 픽셀값을 0~1사이로 정규화
#                    = 데이터를 특정 번위로 변환하는 처리
## 전처리 신경망의 입력 데이터에 특정 변환을 가하는 것
# one-hot-label : 원-핫 인코딩 형태로 저장할 것인지 여부

from PIL import Image
# PIL : 이미지 표시 모듈

def img_show(img):
    pil_img= Image.fromarray(np.uint8(img))
    # fromarray : 넘파이로 저장된 이미지 데이터를 pil용 데이터 객체로 변환
    pil_img.show()
    # 이미지 보여주기
    
img=x_train[0]
label=t_train[0]
print(label)

print(img.shape)
img=img.reshape(28,28)
# flatten로 인해 1차원 배열로 되었기 때문에 28*28 크기로 다시 변형해야함
img_show(img)
```

✨ 여기서 사용 one-hot encoding을 살펴보자.

​		:  categorical data(범주형 자료) 다룰 때, 컴퓨터가 인식할 수 있도록 0,1로 이뤄진 array 데이터로 변형

​		왜 사용?  label로 지정시, label 자체가 영향을 줄 수 있음

​		👉 텐서플로우 라이브러리 사용 ==> tf.one_hot(indices, depth)

​				👉여기서 depth로 카테고리 수 설정. 만약 (indices의 label 수-1) 이면 0으로 설정됨 

​						=> 많은 데이터가 있으면 하나의 카테고리를 줄이는 것도 경제적임

​								(예로 4개의 카테고리에서 3개만 지정해줘도 나머지는 저절로 판별 가능)  



```python
import sys, os
sys.path.append('C:\\Users\\yeomg\\ssafy5\\밑시딥 연습\\deep-learning-from-scratch-master')
from dataset.mnist import load_mnist
import pickle
# 프로그램 실행 중에 특정 객체를 파일로 저장하는 기능


# 데이터를 얻는 함수
def get_data():
    (x_train, t_train),(x_test, t_test)=load_mnist(flatten=True, normalize=True, one_hot_label=False)
    
    return x_test, t_test
    # 정확도만 측정하는 것이기 때문에 text set만 반환
    
# 학습된 가중치 매개변수를 통해 신경망 초기화    
def init_network():
    with open("sample_weight.pkl", 'rb') as f:
        # 가중치와 편향 매개변수가 딕셔너리 변수로 저장
        network=pickle.load(f)
        
    return network

# 신경망 행렬 곱 계산
def predict(network,x):
    W1 ,W2 ,W3 = network['W1'], network['W2'], network['W3']
    B1, B2, B3 = network['b1'], network['b2'], network['b3']
    
    A1=np.dot(x,W1)+B1
    Z1=sigmoid(A1)
    
    A2=np.dot(Z1,W2)+B2
    Z2=sigmoid(A2)
    
    A3=np.dot(Z2,W3)+B3
    Y=softmax(A3)
    # softmax 함수 사용
    
    return Y

x, t =get_data()
network=init_network()

# 정확도 평가
accuracy_cnt=0

# x에 저장된 이미지 데이터 1장식 꺼낸다
for i in range(len(x)):
    y=predict(network, x[i]) # 출력 값 예상
    p=np.argmax(y) # 10개 출력 중에 가장 큰 값 
    if p==t[i]: # 예측 값과 label 값이 일치하면
        accuracy_cnt+=1 # 정확도 +1
        
        
print('accuracy:'+str(float(accuracy_cnt)/len(x)))
# accuracy:0.9352
```



🔥 이미지를 한장씩 처리하는 것을 비효율적임. 큰 배열을 한꺼번에 계산하는 것이 더욱 효율적

​      ====> batch(배치) : 하나로 묶은 입력 데이터

​		but, batch 사이즈가 커지면 컴퓨터 성능이 좋아야함

​		🔥 batch size마다 정확도 달라짐. 왜? 계산할때마다 epoch(batch size) 정확도 달라서

```python
# 정확도 평가
accuracy_cnt=0

# 배치 처리
batch_size=100

# 이미지를 batch_size 씩 묶음
for i in range(0,len(x),batch_size):
    x_batch= x[i:i+batch_size]
    y_batch=predict(network, x_batch) # 출력 값 예상
    p=np.argmax(y_batch, axis=1) #   axis는 1번째 차원을 구성하는 각 원소에서 최댓값의 인덱스를 찾도록 함
    accuracy_cnt+=np.sum(p==t[i:i+batch_size]) # 각 batch의 정답 수를 더함
    # accuracy:0.9352
```

























