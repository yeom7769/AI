# ë°‘ì‹œë”¥ ğŸ“‚4. ì‹ ê²½ë§ í•™ìŠµ

> í•™ìŠµ : í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’ì„ ìë™ìœ¼ë¡œ íšë“í•˜ëŠ” ê²ƒ



## 1. ë°ì´í„° í•™ìŠµ

> ê¸°ê³„í•™ìŠµì—ì„œëŠ” <u>ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ì°¾ê³  ë‹µì„ ì°¾ëŠ”ë‹¤.</u>



### 1.1 ê¸°ê³„í•™ìŠµ ì ‘ê·¼ë²•

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-2.png" alt="fig 4-2" style="zoom:50%;" />

1. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ `íŠ¹ì§•(feature)`ì„ ì¶”ì¶œí•˜ê³  ê·¸ íŠ¹ì§•ì˜ íŒ¨í„´ì„ ê¸°ê³„í•™ìŠµ ê¸°ìˆ ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.

- âœ¨ **`íŠ¹ì§•(feature)`** : ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ë³€í™˜ê¸°

- ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì€ ë³´í†µ `ë²¡í„°`ë¡œ, ì»´í“¨í„° ë¹„ì „ ë¶„ì•¼ì—ì„œëŠ” SIFT, SURF, HOG ë“±ì˜ íŠ¹ì§•ì„ ë§ì´ ì‚¬ìš©.

  ì´ëŸ° íŠ¹ì§•ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  ì§€ë„í•™ìŠµ ë°©ì‹(SVM, KNN ë“±)ìœ¼ë¡œ í•™ìŠµ

ğŸ¤·â€â™€ï¸ í•˜ì§€ë§Œ ìœ„ì™€ ê°™ì€ ê¸°ê³„í•™ìŠµ ì ‘ê·¼ë²•ì€ ì‚¬ëŒì˜ ê°œì…ì´ í•„ìˆ˜ì 



2. **`ì‹ ê²½ë§`**ì€ ëª¨ë“  ê²ƒì„ ê¸°ê³„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ ==> end-to-end machine learning



### 1.2 í›ˆë ¨ ë°ì´í„°(training data) / ì‹œí—˜ ë°ì´í„°(test data)

> ë°ì´í„° ì·¨ê¸‰ ì‹œ, ì£¼ì˜í•  ì 

- training data : ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ì°¾ìŒ

- test data : í›ˆë ¨í•œ ëª¨ë¸ í‰ê°€

  ğŸ™‹â€â™€ï¸ ì™œ ë‚˜ëˆ ì•¼í• ê¹Œ? 

  â€‹		ğŸ‘‰ ìš°ë¦¬ì˜ ëª©ì ì€ `ë²”ìš©ì  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸`. **`ë²”ìš© ëŠ¥ë ¥(ë³´ì§€ ëª»í•œ ë°ì´í„°ë¡œ ë¬¸ì œ í•´ê²°)`**ì„ í‰ê°€í•˜ê¸° ìœ„í•´ì„œëŠ” í•œë²ˆë„ ë³´ì§€ ëª»í•œ ë°ì´í„°ë¡œ í‰ê°€í•´ì•¼í•¨

  â€‹		ğŸ‘‰ ë°ì´í„°ì…‹ í•˜ë‚˜ë¡œë§Œ í•™ìŠµê³¼ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©´ í•œ ë°ì´í„°ì…‹ì—ë§Œ ìµœì í™”ëœ ìƒíƒœ(**`ì˜¤ë²„í”¼íŒ…(overfiting)`**) ì•¼ê¸°í•  ìˆ˜ ìˆìŒ.



---



## 2. ğŸŒ ì†ì‹¤ í•¨ìˆ˜(loss function)

> ì†ì‹¤ í•¨ìˆ˜ : **`ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì°¾ê¸° ìœ„í•œ ì§€í‘œ`**. ì¼ë°˜ì ìœ¼ë¡œ <u>ì˜¤ì°¨ì œê³±í•©</u>ê³¼ <u>êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨</u> ì‚¬ìš©
>
> â€‹					ğŸ‘‰ ì‹ ê²½ë§ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ë‚˜ìœê°€ì˜ ì§€í‘œ



### 2.1 ì˜¤ì°¨ì œê³±í•©(Sum of Sqaures for Error, SSE)

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.1.png" alt="e 4.1" style="zoom:50%;" /> 

```python
# y : ì‹ ê²½ë§ ì¶œë ¥ t: ì •ë‹µ ë ˆì´ë¸”
def sum_squares_error(y,t):
    return 0.5*np.sum((y-t)**2)
```



### 2.2 êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨(Cross Entropy Error, CEE)

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.2.png" alt="e 4.2" style="zoom:50%;" /> 

```python
def cross_entropy_error(y,t):
    delta =1e-7
    return -np.sum(t*np.log(y+delta))
# logì•ˆì— yê°€ 0ì¼ ë•Œ, -ë¬´í•œëŒ€ë¡œ ë¹ ì§€ê¸° ë•Œë¬¸ì— ì•„ì£¼ ì‘ì€ delta ê°’ì„ ë”í•´ì¤€ë‹¤.
# logy => y(í™•ë¥ )ì´ ì‘ì„ ìˆ˜ë¡ ê°’ì´ -ë¡œ ì»¤ì§
```

| ë¬¸ì œ ìœ í˜• | ëª©í‘œ ë ˆì´ë¸”    | ì¶œë ¥ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ | ì†ì‹¤ í•¨ìˆ˜                                      |
| --------- | -------------- | -------------------- | ---------------------------------------------- |
| íšŒê·€ ì˜ˆì¸¡ | ì—°ì†í˜•(ìˆ«ìí˜•) | í•­ë“± í•¨ìˆ˜            | í‰ê· ì œê³±ì˜¤ì°¨(MSS) = ì˜¤ì°¨ì œê³±í•©                 |
| ì´ì§„ ë¶„ë¥˜ | ì´ì‚°í˜•(ë²”ì£¼í˜•) | ì‹œê·¸ëª¨ì´ë“œ           | ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼(binary_crossentropy)        |
| ë‹¤ì¤‘ ë¶„ë¥˜ | ì´ì‚°í˜•(ë²”ì£¼í˜•) | ì†Œí”„íŠ¸ë§¥ìŠ¤           | ë²”ì£¼í˜• êµì°¨ ì—”íŠ¸ë¡œí”¼(categorical_crossentropy) |



### 2.3 ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ

> í›ˆë ¨ ë°ì´í„°ì˜ ìˆ˜ë§Œí¼ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ë“¤ì˜ í•©ì„ ì§€í‘œë¡œ ì‚¼ì•„ì•¼í•œë‹¤. 

- êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨(Nê°œì˜ ë°ì´í„°)

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.3.png" alt="e 4.3" style="zoom:50%;" /> 

ğŸ™‹â€â™€ï¸ ëª¨ë“  ë°ì´í„°ì˜ ì†ì‹¤ í•¨ìˆ˜ì˜ í•©ì„ êµ¬í• ë ¤ë©´ ë§ì€ ì‹œê°„ ì†Œìš”

â€‹		ğŸ‘‰ ì¼ë¶€ í›ˆë ¨ ë°ì´í„°ë§Œ ê³¨ë¼ í•™ìŠµ ìˆ˜í–‰ ==> ê·¸ ì¼ë¶€ê°€ **`ë¯¸ë‹ˆë°°ì¹˜(mini-batch)`**

```python
#import í•œ load_mnistì—ì„œ í›ˆë ¨ ë°ì´í„°, ì‹œí—˜ ë°ì´í„° í• ë‹¹
(x_train, t_train),(x_test, t_test)=load_mnist(normalize=True, one_hot_label=True)

# ë°ì´í„° ì‚¬ì´ì¦ˆ 60,000 í• ë‹¹
train_size=x_train.shape[0]
# ë°ì´í„° Nê°œ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë½‘ì„ ë°ì´í„° ìˆ˜
batch_size=10
# 60,000ê°œ ìˆ«ì ì¤‘ 10ê°œ ë¬´ì‘ìœ„ë¡œ ë½‘ìŒ
batch_mask=np.random.choice(train_size, batch_size)

# í›ˆë ¨ batchì— í• ë‹¹
x_batch=x_train[batch_mask]
t_batch=t_train[batch_mask]
```



### 2.4  (ë°°ì¹˜ìš©)êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ êµ¬í˜„í•˜ê¸°

```python
def cross_entropy_error(y,t):
    # y = ì‹ ê²½ë§ì˜ ì¶œë ¥
    # t = ì •ë‹µ ë ˆì´ë¸”
    
    # yê°€ 1ì°¨ì›ì´ë¼ë©´(ë°ì´í„° í•˜ë‚˜ë‹¹ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¥¼ êµ¬í•˜ëŠ” ê²½ìš°)
    # y.shape = (y.size, )
    if y.ndim ==1:
        # reshape
        t=t.reshape(1,t.size)
        y=y.reshape(1,y.size)
        # y.shape = (1, y.size)
    
    batch_size=y.shape[0]
    # ì´ë¯¸ì§€ í•œì¥ë‹¹ í‰ê· ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ ê³„ì‚°
    return -np.sum(t*np.log(y+1e-7)) /batch_size
```

- ë§Œì•½ one-hot encodingì´ ì•„ë‹ˆë¼ ìˆ«ì ë ˆì´ë¸”ì´ë¼ë©´?

```python
return -np.sum(t*np.log(y[np.arange(batch_size),t]+1e-7)) /batch_size
# ë§Œì•½ ë°°ì¹˜ ì‚¬ì´ì¦ˆê°€ 5ë¼ë©´ np.arange(batch_size)=[0,1,2,3,4]
# ì¦‰ [y[0,2],y[1,4],...,y[4,8]]ì¸ ë„˜íŒŒì´ ë°°ì—´ ìƒì„±
# y[n,m]==> ní–‰ì˜ më²ˆì§¸ ìš”ì†Œ
```



### 2.5 ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • ì´ìœ 

â“ ì™œ ì •í™•ë„ë¥¼ ì§€í‘œë¡œ í•˜ì§€ ì•ŠëŠ”ê°€â“

1. ì‹ ê²½ë§ í•™ìŠµì—ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ëŠ¥í•œ ì‘ê²Œ í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ íƒìƒ‰

2. ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„(ê¸°ìš¸ê¸°)ì„ ê³„ì‚°í•˜ê³ , ë§¤ê°œë³€ìˆ˜ ê°’ update

3. ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„ = ë§¤ê°œë³€ìˆ˜ì˜ ê°’ì„ ì¡°ê¸ˆ ë³€í™”ì‹œí‚¤ë©´ ì†ì‹¤ í•¨ìˆ˜ì˜ ë³€í™”ëŠ” ì–´ë–»ê²Œ ë˜ëŠ”ê°€

4. ì •í™•ë„ê°€ ì§€í‘œê°€ ëœë‹¤ë©´ ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„ì´ ëŒ€ë¶€ë¶„ 0ì´ ë¨ ==> ì •í™•ë„ëŠ” ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ ë°”ë€” ìˆ˜ ì—†ìŒ




---



##  3. ìˆ˜ì¹˜ ë¯¸ë¶„(numerical differentiation)

> ì•„ì£¼ ì‘ì€ *ì°¨ë¶„ìœ¼ë¡œ ë¯¸ë¶„í•˜ëŠ” ê²ƒ
>
> *ì°¨ë¶„ : ì„ì´ì˜ ë‘ ì ì—ì„œì˜ í•¨ìˆ˜ ê°’ë“¤ì˜ ì°¨ì´



### 3.1 ë¯¸ë¶„

> íŠ¹ì • ìˆœê°„ì˜ ë³€í™”ëŸ‰
>
>  ğŸ‘‰ xì˜ ì‘ì€ ë³€í™”ê°€ f(x)ë¥¼ ì–¼ë§ˆë‚˜ ë³€í™”ì‹œí‚¤ëŠ”ê°€?

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.4.png" alt="e 4.4" style="zoom:50%;" /> 

- ìœ„ì˜ ë¯¸ë¶„ì‹ì„ íŒŒì´ì„ ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì.

```python
def numerical_diff(f,x):
    
    h=1e-4
    # h=1e-50 ì˜ ê²½ìš° ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ ë¬¸ì œ(ì‘ì€ ê°’ì´ ìƒëµ)ë¥¼ ì¼ìœ¼í‚´
    
    return (f(x+h)-f(x-h))/(2*h)
```

ğŸ¤·â€â™€ï¸ ì™œ (f(x+h)-f(x))/h ì´ ì•„ë‹ê¹Œ?

- ì§„ì •í•œ ë¯¸ë¶„(xì ì—ì„œì˜ ê¸°ìš¸ê¸°)ê³¼ ìˆ˜ì¹˜ ë¯¸ë¶„(ê·¼ì‚¬ë¡œ êµ¬í•œ ì ‘ì„ )ì˜ ê°’ì€ ë‹¤ë¥´ë‹¤.
- (f(x+h)-f(x-h))/(2*h)ì€ xë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì „í›„ì˜ ì°¨ë¶„ì„ ê³„ì‚°í•œë‹¤ëŠ” ì˜ë¯¸(ì¤‘ì‹¬ ì°¨ë¶„/ì¤‘ì•™ ì°¨ë¶„)

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-5.png" alt="fig 4-5" style="zoom: 33%;" /> 



### 3.3 í¸ë¯¸ë¶„

> ë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.6.png" alt="e 4.6" style="zoom:50%;" /> 

- ìœ„ì˜ í•¨ìˆ˜ì˜ ê·¸ë˜í”„ëŠ” ì•„ë˜ì™€ ê°™ì´ 3ì°¨ì›ìœ¼ë¡œ ê·¸ë ¤ì§„ë‹¤. 

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-8.png" alt="fig 4-8" style="zoom:33%;" /> 

- ì—¬ëŸ¬ ë³€ìˆ˜ ì¤‘ ëª©í‘œ ë³€ìˆ˜ í•˜ë‚˜ì— ì´ˆì ì„ ë§ì¶”ê³  ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” ê°’ì„ ê³ ì •!

- ë§Œì•½ í¸ë¯¸ë¶„ì—ì„œ ë‘ ë³€ìˆ˜ë¥¼ í•œë²ˆì— ê³„ì‚°í•˜ê³  ì‹¶ë‹¤ë©´??

   ğŸ‘‡





## 4. ê¸°ìš¸ê¸°(gradient)

> ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ ë²¡í„°ë¡œ ì •ë¦¬í•œ ê²ƒ(ë²¡í„°ë¡œ í‘œí˜„)

- í•œ ì˜ˆë¡œ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìˆìŒ

  ```python
  def numerical_gradient(f,x):
      h=1e-4
      grad=np.zeros_like(x)
      # xì™€ í˜•ìƒì´ ê°™ì€ ë°°ì—´ (ëª¨ë‘ 0ìœ¼ë¡œ ì±„ì›Œì§)
      
      for idx in range(x.size):
      # idx = 0~x.size-1
          tmp_val=x[idx]
          
          # f(x+h)
          x[idx]=tmp_val+h
          fxh1=f(x)
          
          # f(x-h)
          x[idx]=tmp_val-h
          fxh2=f(x)
          
          grad[idx]=(fxh1-fxh2)/(2*h)
      	# xê°’ ì´ˆê¸°í™”
          x[idx]=tmp_val
          
      return grad
  ```

  - ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰ì‹œí‚¤ë©´ ì•„ë˜ì™€ ê°™ì´ ê·¸ë ¤ì§„ë‹¤.

  <img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-9.png" alt="fig 4-9" style="zoom:33%;" /> 

  - ê¸°ìš¸ê¸°ëŠ” í•¨ìˆ˜ì˜ 'ê°€ì¥ ë‚®ì€ ì¥ì†Œ(ìµœì†Ÿê°’)'ì„ ê°€ë¦¬í‚´ => ê° ì§€ì ì—ì„œ ë‚®ì•„ì§€ëŠ” ë°©í–¥ ê°€ë¦¬í‚´

    ğŸ”¥**`ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ìª½ = ê° ì¥ì†Œì—ì„œ í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ê°€ì¥ í¬ê²Œ ì¤„ì´ëŠ” ë°©í–¥`**

  - ìµœì†Ÿê°’ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ í™”ì‚´í‘œ í¬ê¸°ê°€ ì»¤ì§



### 4.1 ê²½ì‚¬ë²•(ê²½ì‚¬ í•˜ê°•ë²•)

> ìš°ë¦¬ì˜ ëª©í‘œëŠ” ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Ÿê°’ì´ ë  ë•Œì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì°¾ëŠ” ê²ƒ
>
> ê·¸ ë°©ë²•ì´ `ê²½ì‚¬ë²•`



1. í•¨ìˆ˜ì˜ ê°’ì„ ë‚®ì¶”ëŠ” ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ì§€í‘œ = ê¸°ìš¸ê¸°

> ì‹¤ì œë¡œëŠ” ê·¹ì†Ÿê°’, ìµœì†Ÿê°’, ì•ˆì¥ì (saddle point)ì—ì„œ ê¸°ìš¸ê¸°ê°€ 0ì´ë©° ë³µì¡í•˜ê³  ì°Œê·¸ëŸ¬ì§„ ëª¨ì–‘ì˜ í•¨ìˆ˜ë¼ë©´ ê³ ì›(plateau, í”Œë˜í† )ìœ¼ë¡œ íŒŒê³  ë“¤ë©´ì„œ í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠëŠ” ì •ì²´ê¸°ì— ë¹ ì§ˆìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ìˆìŒ

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/plateau.PNG" alt="plateau" style="zoom: 50%;" /> <img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/saddle point.PNG" alt="saddle point" style="zoom: 50%;" />



2. ê²½ì‚¬ë²•(ê²½ì‚¬ í•˜ê°•ë²•) : í˜„ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œ ë‹¤ìŒ ì´ë™í•œ ê³³ì—ì„œ ê¸°ìš¸ê¸° êµ¬í•´ ê·¸ ê¸°ìš¸ì–´ì§„ ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ê²ƒì„ ë°˜ë³µí•¨ => í•¨ìˆ˜ì˜ ê°’ì„ ì ì°¨ ì¤„ì´ëŠ” ê²ƒ

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.7.png" alt="e 4.7" style="zoom:50%;" /> 

> `Î·(ì—íƒ€) = ê°±ì‹ í•˜ëŠ” ì–‘ = í•™ìŠµë¥ (learning rate) = í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyper parameter)`   
>
> ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ê°±ì‹  x

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
# f = ìµœì í™”í•˜ë ¤ëŠ” í•¨ìˆ˜
# init_x = ì´ˆê¹ƒê°’
# lr = í•™ìŠµë¥  , step_num = ê²½ì‚¬ë²• ë°˜ë³µ íšŸìˆ˜

    x=init_x
    
    for i in range(step_num):
        grad=numerical_gradient(f,x)
        x-=lr*grad
    
    return x
	# ì  init_xì—ì„œ ê¸°ìš¸ê¸° ê³„ì‚° ì‹œì‘! ê·¹ì†Ÿê°’/ìµœì†Ÿê°’ì˜ x ë°˜í™˜
```





### 4.2 ì‹ ê²½ë§ì—ì„œì˜ ê¸°ìš¸ê¸°

1. ì‹ ê²½ë§ ê¸°ìš¸ê¸°

   <img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/e 4.8.png" alt="e 4.8" style="zoom:50%;" /> 

   > ìœ„ì™€ ê°™ì´ ê°€ì¤‘ì¹˜ê°€ 2 x 3ì˜ í˜•ìƒì´ë¼ ìƒê°í•´ë³´ë©´ `ê²½ì‚¬`ëŠ” dL/dWë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.
   >
   > Wë¥¼ ì¡°ê¸ˆ ë³€ê²½í–ˆì„ ë•Œ Lì´ ì–¼ë§ˆë‚˜ ë³€í™”í•˜ëŠëƒë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸

   - ê°„ë‹¨í•œ ì‹ ê²½ë§ì„ ì˜ˆë¡œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•´ë³´ì

     ```python
     # ê°„ë‹¨í•œ ì‹ ê²½ë§ êµ¬í˜„
     class simpleNet:
         
         # 2 X 3 í˜•ìƒì˜ W(ê°€ì¤‘ì¹˜) ìƒì„±
         def __init__(self):
             self.W = np.random.randn(2,3) 
             # ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”
     
         # ì˜ˆì¸¡ ê°’
         def predict(self, x):
             return np.dot(x, self.W)
     
         # ì˜¤ì°¨ë¥  , ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’
         def loss(self, x, t):
             z = self.predict(x)
             y = softmax(z)
             loss = cross_entropy_error(y, t)
     
             return loss
     
     net = simpleNet()
     
     # print(new.W)
     #[[-0.02511244 -0.96040608  1.530118  ]
     # [ 1.47644     1.7087221  -1.11852641]]
     
     # x : ì…ë ¥ ë°ì´í„°, t : ê²°ê³¼ ë ˆì´ë¸”
     x = np.array([0.6, 0.9])
     t = np.array([0, 0, 1])
     
     f = lambda w: net.loss(x, t)
     # def f(W):
     #     return net.loss(x,t)
     # dummy(ë”ë¯¸)ë¡œ f(W) ì •ì˜(numerical_gradient ë‚´ë¶€ì˜ f(x)ì‹¤í–‰ ìœ„í•´)
     # ì…ë ¥ W, ì¶œë ¥ loss
     
     dW = numerical_gradient(f, net.W)
     # wë¥¼ hë§Œí¼ ëŠ˜ë¦¬ë©´(ì¤„ì´ë©´) dw*h ë§Œí¼ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì´ ë³€í•œë‹¤.
     ```





## 5. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„í•˜ê¸°

### 5.0 ì‹ ê²½ë§ í•™ìŠµì˜ ì ˆì°¨

1. ì „ì²´ 
   - í•™ìŠµ = ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •
2. 1ë‹¨ê³„ : ë¯¸ë‹ˆë°°ì¹˜
   - í›ˆë ¨ ë°ì´í„° ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´. ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ
3.  2ë‹¨ê³„ : ê¸°ìš¸ê¸° ì‚°ì¶œ
   - ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° êµ¬í•¨. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œ
4.  3ë‹¨ê³„ : ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
   - ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ê°±ì‹ 

> ì´ë•Œ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë¬´ì‘ìœ„ ì„ ì •í•˜ê¸° ë•Œë¬¸ì— **`í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• = Stochastic Gradient Descent = SGD`**ë¼ê³  í•œë‹¤.



### 5.1 2ì¸µ ì‹ ê²½ë§ í´ë˜ìŠ¤ êµ¬í˜„í•˜ê¸°

```python
class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
    # input_size : ì…ë ¥ì¸µ ë‰´ëŸ° ìˆ˜ hidden_size : ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜ ouput_size : ì¶œë ¥ì¸µ ë‰´ëŸ° ìˆ˜
    # weight_init_std : ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì‹œ ì •ê·œë¶„í¬ ìŠ¤ì¼€ì¼
    
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.params = {} # ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size) # 0ë¡œ ì´ˆê¸°ê°’ ìƒì„±
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    # ê²°ê³¼ ì˜ˆì¸¡ í•¨ìˆ˜
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1) # í™œì„±í™” í•¨ìˆ˜ë¡œ sigmoid ì‚¬ìš©
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2) # ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ë¡œ softmax ì‚¬ìš©(ouputì„ í™•ë¥ ë¡œ ì¶”ì¶œ)
        
        return y
        
    # ì†ì‹¤í•¨ìˆ˜ ê°’ ì¶œë ¥
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t) # êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ë¡œ ê³„ì‚°
    
    # ì •í™•ë„ ê³„ì‚°(ì •ë‹µì¸ì§€ ì•„ë‹Œì§€ íŒë³„)
    def accuracy(self, x, t):
        y = self.predict(x)
        # ìµœëŒ“ê°’ì˜ index ì–»ìŒ
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy # ì •ë‹µë¥  ë°˜í™˜
        
    # ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° ê³„ì‚°    
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t) # ì…ë ¥ Wì— ëŒ€í•œ ì¶œë ¥ loss í•¨ìˆ˜
        
        grads = {} # ê¸°ìš¸ê¸° ë”•ì…”ë„ˆë¦¬
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
```

```python
# ê¸°ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° ê³„ì‚° (numerical_gradient ì„±ëŠ¥ ê°œì„ íŒ)
# ë‹¤ìŒì¥ì—ì„œ ë°°ì›€. ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads
```



### 5.2 ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ êµ¬í˜„í•˜ê¸°

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

# ì‹ ê²½ë§ ìƒì„±
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000  # ë°˜ë³µ íšŸìˆ˜
train_size = x_train.shape[0]  # train ë°ì´í„° ê°œìˆ˜ (60,000)
batch_size = 100   # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
learning_rate = 0.1  # í•™ìŠµë¥ 

train_loss_list = []

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ íšë“
    batch_mask = np.random.choice(train_size, batch_size)
    # 60,000 ì¤‘ì—ì„œ ìˆ«ì 100ê°œ ë¬´ì‘ìœ„ë¡œ ë½‘ìŒ 
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚°
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) ì„±ëŠ¥ ê°œì„ íŒ
    
    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹  gradient_descent
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    # ìµœì ì˜ ë§¤ê°œë³€ìˆ˜ ì„¤ì •
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```

![fig 4-11](ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-11.png)

> ë°ì´í„°ë¥¼ ë°˜ë³µí•´ì„œ í•™ìŠµí•¨ìœ¼ë¡œì¨ ìµœì  ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë‹¤ê°€ê°



### 5.3 ì‹œí—˜ ë°ì´í„°ë¡œ í‰ê°€í•˜ê¸°

> epoch(ì—í­) : í•™ìŠµì—ì„œ í›ˆë ¨ ë°ì´í„°ë¥¼ ëª¨ë‘ ì†Œì§„í–ˆì„ ë•Œì˜ íšŸìˆ˜
>
> ë§¤ë²ˆ ì •í™•ë„ ì¸¡ì •í•˜ë©´ ì†ë„ê°€ ëŠë ¤ì§ => ì…ë ¥ ë°ì´í„°ë¥¼ í•œë²ˆ ë‹¤ ë³¼ ë•Œë§ˆë‹¤ ì •í™•ë„ë¥¼ ê³„ì‚°í•´ë¼!

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000  # ë°˜ë³µ íšŸìˆ˜
train_size = x_train.shape[0]
batch_size = 100   # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ íšë“
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚°
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚° , í›ˆë ¨ë°ì´í„° ëª¨ë‘ ë´¤ì„ ë•Œ.
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

<img src="ë°‘ì‹œë”¥ 4. ì‹ ê²½ë§ í•™ìŠµ.assets/fig 4-12.png" alt="fig 4-12" style="zoom: 33%;" /> 

> í›ˆë ¨ë°ìì™€ ì‹œí—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê³  í‰ê°€í•œ ì •í™•ë„ê°€ ëª¨ë‘ ì¢‹ì•„ì§ => ì˜¤ë²„í”¼íŒ… x

- ë§Œì•½ ì˜¤ë²„í”¼íŒ…ì´ ì¼ì–´ë‚œë‹¤ë©´ ì–´ëŠ ìˆœê°„ë¶€í„° ì‹œí—˜ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ê°€ ì ì°¨ ë–¨ì–´ì§

  => ì´ ìˆœê°„ì„ í¬ì°©í•´ í•™ìŠµ ì¤‘ë‹¨í•˜ë©´ ì˜¤ë²„í”¼íŒ…ì„ íš¨ê³¼ì ìœ¼ë¡œ ì˜ˆë°©í•  ìˆ˜ ìˆìŒ