# 밑시딥 📂4. 신경망 학습

> 학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것



## 1. 데이터 학습

> 기계학습에서는 <u>데이터에서 패턴을 찾고 답을 찾는다.</u>



### 1.1 기계학습 접근법

<img src="밑시딥 4. 신경망 학습.assets/fig 4-2.png" alt="fig 4-2" style="zoom:50%;" />

1. 주어진 데이터를 활용하기 위해 `특징(feature)`을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다.

- ✨ **`특징(feature)`** : 입력 데이터에서 중요한 데이터를 정확하게 추출할 수 있는 변환기

- 이미지의 특징은 보통 `벡터`로, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 많이 사용.

  이런 특징들을 사용하여 이미지 데이터를 벡터로 변환하고 지도학습 방식(SVM, KNN 등)으로 학습

🤷‍♀️ 하지만 위와 같은 기계학습 접근법은 사람의 개입이 필수적



2. **`신경망`**은 모든 것을 기계 스스로 학습 ==> end-to-end machine learning



### 1.2 훈련 데이터(training data) / 시험 데이터(test data)

> 데이터 취급 시, 주의할 점

- training data : 최적의 매개변수 찾음

- test data : 훈련한 모델 평가

  🙋‍♀️ 왜 나눠야할까? 

  ​		👉 우리의 목적은 `범용적 사용 가능한 모델`. **`범용 능력(보지 못한 데이터로 문제 해결)`**을 평가하기 위해서는 한번도 보지 못한 데이터로 평가해야함

  ​		👉 데이터셋 하나로만 학습과 평가를 수행하면 한 데이터셋에만 최적화된 상태(**`오버피팅(overfiting)`**) 야기할 수 있음.



---



## 2. 🌠손실 함수(loss function)

> 손실 함수 : **`최적의 매개변수 값을 찾기 위한 지표`**. 일반적으로 <u>오차제곱합</u>과 <u>교차 엔트로피 오차</u> 사용
>
> ​					👉 신경망 성능이 얼마나 나쁜가의 지표



### 2.1 오차제곱합(Sum of Sqaures for Error, SSE)

<img src="밑시딥 4. 신경망 학습.assets/e 4.1.png" alt="e 4.1" style="zoom:50%;" /> 

```python
# y : 신경망 출력 t: 정답 레이블
def sum_squares_error(y,t):
    return 0.5*np.sum((y-t)**2)
```



### 2.2 교차 엔트로피 오차(Cross Entropy Error, CEE)

<img src="밑시딥 4. 신경망 학습.assets/e 4.2.png" alt="e 4.2" style="zoom:50%;" /> 

```python
def cross_entropy_error(y,t):
    delta =1e-7
    return -np.sum(t*np.log(y+delta))
# log안에 y가 0일 때, -무한대로 빠지기 때문에 아주 작은 delta 값을 더해준다.
# logy => y(확률)이 작을 수록 값이 -로 커짐
```

| 문제 유형 | 목표 레이블    | 출력층의 활성화 함수 | 손실 함수                                      |
| --------- | -------------- | -------------------- | ---------------------------------------------- |
| 회귀 예측 | 연속형(숫자형) | 항등 함수            | 평균제곱오차(MSS) = 오차제곱합                 |
| 이진 분류 | 이산형(범주형) | 시그모이드           | 이진 교차 엔트로피(binary_crossentropy)        |
| 다중 분류 | 이산형(범주형) | 소프트맥스           | 범주형 교차 엔트로피(categorical_crossentropy) |



### 2.3 미니배치 학습

> 훈련 데이터의 수만큼의 손실 함수 값들의 합을 지표로 삼아야한다. 

- 교차 엔트로피 오차(N개의 데이터)

<img src="밑시딥 4. 신경망 학습.assets/e 4.3.png" alt="e 4.3" style="zoom:50%;" /> 

🙋‍♀️ 모든 데이터의 손실 함수의 합을 구할려면 많은 시간 소요

​		👉 일부 훈련 데이터만 골라 학습 수행 ==> 그 일부가 **`미니배치(mini-batch)`**

```python
#import 한 load_mnist에서 훈련 데이터, 시험 데이터 할당
(x_train, t_train),(x_test, t_test)=load_mnist(normalize=True, one_hot_label=True)

# 데이터 사이즈 60,000 할당
train_size=x_train.shape[0]
# 데이터 N개 중에서 무작위로 뽑을 데이터 수
batch_size=10
# 60,000개 숫자 중 10개 무작위로 뽑음
batch_mask=np.random.choice(train_size, batch_size)

# 훈련 batch에 할당
x_batch=x_train[batch_mask]
t_batch=t_train[batch_mask]
```



### 2.4  (배치용)교차 엔트로피 오차 구현하기

```python
def cross_entropy_error(y,t):
    # y = 신경망의 출력
    # t = 정답 레이블
    
    # y가 1차원이라면(데이터 하나당 교차 엔트로피 오차를 구하는 경우)
    # y.shape = (y.size, )
    if y.ndim ==1:
        # reshape
        t=t.reshape(1,t.size)
        y=y.reshape(1,y.size)
        # y.shape = (1, y.size)
    
    batch_size=y.shape[0]
    # 이미지 한장당 평균의 교차 엔트로피 오차 계산
    return -np.sum(t*np.log(y+1e-7)) /batch_size
```

- 만약 one-hot encoding이 아니라 숫자 레이블이라면?

```python
return -np.sum(t*np.log(y[np.arange(batch_size),t]+1e-7)) /batch_size
# 만약 배치 사이즈가 5라면 np.arange(batch_size)=[0,1,2,3,4]
# 즉 [y[0,2],y[1,4],...,y[4,8]]인 넘파이 배열 생성
# y[n,m]==> n행의 m번째 요소
```



### 2.5 손실 함수 설정 이유

❓ 왜 정확도를 지표로 하지 않는가❓

1. 신경망 학습에는 손실 함수의 값을 가능한 작게 하는 매개변수 탐색

2. 매개변수의 미분(기울기)을 계산하고, 매개변수 값 update

3. 손실 함수의 미분 = 매개변수의 값을 조금 변화시키면 손실 함수의 변화는 어떻게 되는가

4. 정확도가 지표가 된다면 매개변수의 미분이 대부분 0이 됨 ==> 정확도는 연속적인 값으로 바뀔 수 없음




---



##  3. 수치 미분(numerical differentiation)

> 아주 작은 *차분으로 미분하는 것
>
> *차분 : 임이의 두 점에서의 함수 값들의 차이



### 3.1 미분

> 특정 순간의 변화량
>
>  👉 x의 작은 변화가 f(x)를 얼마나 변화시키는가?

<img src="밑시딥 4. 신경망 학습.assets/e 4.4.png" alt="e 4.4" style="zoom:50%;" /> 

- 위의 미분식을 파이선으로 구현해보자.

```python
def numerical_diff(f,x):
    
    h=1e-4
    # h=1e-50 의 경우 반올림 오차 문제(작은 값이 생략)를 일으킴
    
    return (f(x+h)-f(x-h))/(2*h)
```

🤷‍♀️ 왜 (f(x+h)-f(x))/h 이 아닐까?

- 진정한 미분(x점에서의 기울기)과 수치 미분(근사로 구한 접선)의 값은 다르다.
- (f(x+h)-f(x-h))/(2*h)은 x를 중심으로 전후의 차분을 계산한다는 의미(중심 차분/중앙 차분)

<img src="밑시딥 4. 신경망 학습.assets/fig 4-5.png" alt="fig 4-5" style="zoom: 33%;" /> 



### 3.3 편미분

> 변수가 여럿인 함수에 대한 미분

<img src="밑시딥 4. 신경망 학습.assets/e 4.6.png" alt="e 4.6" style="zoom:50%;" /> 

- 위의 함수의 그래프는 아래와 같이 3차원으로 그려진다. 

<img src="밑시딥 4. 신경망 학습.assets/fig 4-8.png" alt="fig 4-8" style="zoom:33%;" /> 

- 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정!

- 만약 편미분에서 두 변수를 한번에 계산하고 싶다면??

   👇





## 4. 기울기(gradient)

> 모든 변수의 편미분을 벡터로 정리한 것(벡터로 표현)

- 한 예로 다음과 같이 구현할 수 있음

  ```python
  def numerical_gradient(f,x):
      h=1e-4
      grad=np.zeros_like(x)
      # x와 형상이 같은 배열 (모두 0으로 채워짐)
      
      for idx in range(x.size):
      # idx = 0~x.size-1
          tmp_val=x[idx]
          
          # f(x+h)
          x[idx]=tmp_val+h
          fxh1=f(x)
          
          # f(x-h)
          x[idx]=tmp_val-h
          fxh2=f(x)
          
          grad[idx]=(fxh1-fxh2)/(2*h)
      	# x값 초기화
          x[idx]=tmp_val
          
      return grad
  ```

  - 위의 함수를 실행시키면 아래와 같이 그려진다.

  <img src="밑시딥 4. 신경망 학습.assets/fig 4-9.png" alt="fig 4-9" style="zoom:33%;" /> 

  - 기울기는 함수의 '가장 낮은 장소(최솟값)'을 가리킴 => 각 지점에서 낮아지는 방향 가리킴

    🔥**`기울기가 가리키는 쪽 = 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향`**

  - 최솟값에서 멀어질수록 화살표 크기가 커짐



### 4.1 경사법(경사 하강법)

> 우리의 목표는 손실 함수가 최솟값이 될 때의 매개변수 값을 찾는 것
>
> 그 방법이 `경사법`



1. 함수의 값을 낮추는 방안을 제시하는 지표 = 기울기

> 실제로는 극솟값, 최솟값, 안장점(saddle point)에서 기울기가 0이며 복잡하고 찌그러진 모양의 함수라면 고원(plateau, 플래토)으로 파고 들면서 학습이 진행되지 않는 정체기에 빠질수 있는 문제가 있음

<img src="밑시딥 4. 신경망 학습.assets/plateau.PNG" alt="plateau" style="zoom: 50%;" /> <img src="밑시딥 4. 신경망 학습.assets/saddle point.PNG" alt="saddle point" style="zoom: 50%;" />



2. 경사법(경사 하강법) : 현 위치에서 기울어진 방향으로 이동한 다음 이동한 곳에서 기울기 구해 그 기울어진 방향으로 나아가는 것을 반복함 => 함수의 값을 점차 줄이는 것

<img src="밑시딥 4. 신경망 학습.assets/e 4.7.png" alt="e 4.7" style="zoom:50%;" /> 

> `η(에타) = 갱신하는 양 = 학습률(learning rate) = 하이퍼파라미터(hyper parameter)`   
>
> 너무 크면 발산, 너무 작으면 갱신 x

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
# f = 최적화하려는 함수
# init_x = 초깃값
# lr = 학습률 , step_num = 경사법 반복 횟수

    x=init_x
    
    for i in range(step_num):
        grad=numerical_gradient(f,x)
        x-=lr*grad
    
    return x
	# 점 init_x에서 기울기 계산 시작! 극솟값/최솟값의 x 반환
```





### 4.2 신경망에서의 기울기

1. 신경망 기울기

   <img src="밑시딥 4. 신경망 학습.assets/e 4.8.png" alt="e 4.8" style="zoom:50%;" /> 

   > 위와 같이 가중치가 2 x 3의 형상이라 생각해보면 `경사`는 dL/dW로 나타낼 수 있다.
   >
   > W를 조금 변경했을 때 L이 얼마나 변화하느냐를 나타내기 때문

   - 간단한 신경망을 예로 기울기를 구하는 코드를 구현해보자

     ```python
     # 간단한 신경망 구현
     class simpleNet:
         
         # 2 X 3 형상의 W(가중치) 생성
         def __init__(self):
             self.W = np.random.randn(2,3) 
             # 정규분포로 초기화
     
         # 예측 값
         def predict(self, x):
             return np.dot(x, self.W)
     
         # 오차률 , 손실 함수의 값
         def loss(self, x, t):
             z = self.predict(x)
             y = softmax(z)
             loss = cross_entropy_error(y, t)
     
             return loss
     
     net = simpleNet()
     
     # print(new.W)
     #[[-0.02511244 -0.96040608  1.530118  ]
     # [ 1.47644     1.7087221  -1.11852641]]
     
     # x : 입력 데이터, t : 결과 레이블
     x = np.array([0.6, 0.9])
     t = np.array([0, 0, 1])
     
     f = lambda w: net.loss(x, t)
     # def f(W):
     #     return net.loss(x,t)
     # dummy(더미)로 f(W) 정의(numerical_gradient 내부의 f(x)실행 위해)
     # 입력 W, 출력 loss
     
     dW = numerical_gradient(f, net.W)
     # w를 h만큼 늘리면(줄이면) dw*h 만큼 손실 함수의 값이 변한다.
     ```





## 5. 학습 알고리즘 구현하기

### 5.0 신경망 학습의 절차

1. 전체 
   - 학습 = 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정
2. 1단계 : 미니배치
   - 훈련 데이터 일부를 무작위로 가져옴. 미니배치의 손실함수 값을 줄이는 것이 목표
3.  2단계 : 기울기 산출
   - 각 가중치 매개변수의 기울기 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시
4.  3단계 : 매개변수 갱신
   - 가중치 매개변수를 기울기 방향으로 갱신

> 이때 데이터를 미니배치로 무작위 선정하기 때문에 **`확률적 경사 하강법 = Stochastic Gradient Descent = SGD`**라고 한다.



### 5.1 2층 신경망 클래스 구현하기

```python
class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
    # input_size : 입력층 뉴런 수 hidden_size : 은닉층 뉴런 수 ouput_size : 출력층 뉴런 수
    # weight_init_std : 가중치 초기화 시 정규분포 스케일
    
        # 가중치 초기화
        self.params = {} # 신경망의 매개변수 딕셔너리
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size) # 0로 초기값 생성
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

    # 결과 예측 함수
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
    
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1) # 활성화 함수로 sigmoid 사용
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2) # 출력층 활성화 함수로 softmax 사용(ouput을 확률로 추출)
        
        return y
        
    # 손실함수 값 출력
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t) # 교차 엔트로피 오차로 계산
    
    # 정확도 계산(정답인지 아닌지 판별)
    def accuracy(self, x, t):
        y = self.predict(x)
        # 최댓값의 index 얻음
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy # 정답률 반환
        
    # 가중치 매개변수의 기울기 계산    
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t) # 입력 W에 대한 출력 loss 함수
        
        grads = {} # 기울기 딕셔너리
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
```

```python
# 기중치 매개변수의 기울기 계산 (numerical_gradient 성능 개선판)
# 다음장에서 배움. 오차역전파법을 사용하여 기울기를 효율적으로 계산
    def gradient(self, x, t):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        grads = {}
        
        batch_num = x.shape[0]
        
        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        # backward
        dy = (y - t) / batch_num
        grads['W2'] = np.dot(z1.T, dy)
        grads['b2'] = np.sum(dy, axis=0)
        
        da1 = np.dot(dy, W2.T)
        dz1 = sigmoid_grad(a1) * da1
        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads
```



### 5.2 미니배치 학습 구현하기

```python
# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

# 신경망 생성
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 10000  # 반복 횟수
train_size = x_train.shape[0]  # train 데이터 개수 (60,000)
batch_size = 100   # 미니배치 크기
learning_rate = 0.1  # 학습률

train_loss_list = []

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    # 60,000 중에서 숫자 100개 무작위로 뽑음 
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad = network.gradient(x_batch, t_batch) 성능 개선판
    
    # 매개변수 갱신 gradient_descent
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    # 최적의 매개변수 설정
    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```

![fig 4-11](밑시딥 4. 신경망 학습.assets/fig 4-11.png)

> 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 다가감



### 5.3 시험 데이터로 평가하기

> epoch(에폭) : 학습에서 훈련 데이터를 모두 소진했을 때의 횟수
>
> 매번 정확도 측정하면 속도가 느려짐 => 입력 데이터를 한번 다 볼 때마다 정확도를 계산해라!

```python
# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 10000  # 반복 횟수
train_size = x_train.shape[0]
batch_size = 100   # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1에폭당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1에폭당 정확도 계산 , 훈련데이터 모두 봤을 때.
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

# 그래프 그리기
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

<img src="밑시딥 4. 신경망 학습.assets/fig 4-12.png" alt="fig 4-12" style="zoom: 33%;" /> 

> 훈련데잍와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아짐 => 오버피팅 x

- 만약 오버피팅이 일어난다면 어느 순간부터 시험 데이터에 대한 정확도가 점차 떨어짐

  => 이 순간을 포착해 학습 중단하면 오버피팅을 효과적으로 예방할 수 있음