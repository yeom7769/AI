# 밑시딥2 📂2. 자연어와 단어의 분산 표현

> 컴퓨터가 우리의 말을 이해하게 만드는 것이 자연어 처리의 본질적 문제이다.
>
> 이번 장에서는 컴퓨터에 말을 이해시킨다는 것이 무슨 뜻인지, 이를 어떻게 구현하는지에 대해 설명한다.



## 1. 자연어 처리란?

- `자연어(natural language)` : 한국어, 영어 등의 평소 우리가 사용하는 언어

- `자연어 처리(Natural Language Processing, NLP)` : 언어를 컴퓨터에게 인해시키기 위한 기술
  - ex) 질의응답 시스템, IME(입력기전환), 문장 자동요약, 감정분석 ...



- 프로그래밍 언어 VS 자연어

  | 프로그래밍 언어   | 자연어        |
  | ----------------- | ------------- |
  | 기계적이고 고정적 | 유연함        |
  | 딱딱한 언어       | 부드러운 언어 |

> 따라서 컴퓨터에게 자연어를 이해시키기란 어려운 도전이다.



### 1.1 단어의 의미

> 단어 : 의미의 최소 단위

1. 시소러스(사람의 손으로 만든)를 활용한 기법
2. 통계 기반 기법(통계 정보로부터 단어를 표현)
3. 추론 기반 기법(word2vec) (신경망을 활용)



---



## 2. 시소러스

> '단어의 의미'를 나타내는 방법으로는 사람이 직접 단어의 의미를 정의하는 방식을 먼저 떠올릴 것이다.



- **`시소러스`** : 유의어 사전. 동의어/유의어가 한그룹으로 분류되어 있음

  - 단어 상의 '상위', '하위', '전체', '부분' 등의 관계를 정의하기도 함

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-2.png" alt="fig 2-2" style="zoom:50%;" />

  > ​	단어 네이트워크를 이용하면 컴퓨터에게 단어 사이의 관계를 가르칠 수 있다.



### 2.1 WordNet

> 가장 유명한 시소러스.



### 2.2 시소러스의 문제점

> 시소러스 : 수많은 단어에 대한 동의어와 계층 구조 등의 관계를 정의

1.  시대 변화에 대응하기 어렵다. 
   - 시대에 따라 언어는 생기기도하며, 사라지기도 한다. 또한 언어의 의미가 변하기도 한다.
   - 이런 변화에 대응하려면 사람이 수작업으로 끊임없이 갱신해야함
2.  사람을 쓰는 비용이 크다.
   - 방대한 단어들 모두에 대해 단어 사이의 관계를 정의해야함
3.  단어의 미묘한 차이를 표현할 수 없다.



🙄 이러한 문제점을 해결하기 위해 통계 기반 기법과 추론 기반 기법이 탄생했다.

​		👉 사람의 개입을 최소로 줄이고 텍스트 데이터만으로 원하는 결과를 얻어내는 방향으로 패러다임이 바뀌고 있음



---



## 3. 통계 기반 기법

> 사람의 지식으로 가득한 말뭉치(corpus, 대량의 텍스트 데이터)에서 자동으로(효율적으로) 핵심을 추출하는 것



### 3.1 파이선으로 말뭉치 전처리하기

> 자연어 처리에는 `다양한 말뭉치`가 사용된다.  이 덱스트 데이터(말뭉치)를 어떻게 전처리(preprocessing)하는지 알아보자.

📍 **`전처리`** : 텍스트 데이터를 단어로 분할하고 그 분할된 단어들을 단어 ID 목록으로 변환하는 일



```python
text = 'You say goodbye and I say hello.'

text = text.lower() # lower() : 모든 문자를 소문자로 변환
text = text.replace('.', ' .') # replace(a, b) : 문자열의 a를 b로 변환

print(text)
# you say goodbye and i say hello .

word = text.split() # split(a) : a를 기준으로 분할 
print(word)
# ['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '.']
```

​	🙋‍♀️  정규표현식 모듈인 re를 임포트하면 re.split('(\w+)?', text)를 호출하면 단어 단위로 분할할 수 있음



- 단어를 텍스트 그대로 조작하기란 여러 면에서 불편

  그래서 단어에 ID를 부여하고 ID의 리스트로 이용할 수 있도록 한 번 더 손질

  어떻게? => 딕셔너리 이용

  ```python
  word_to_id = {}
  id_to_word = {}
  
  for word in words:
      if word not in word_to_id:
          new_id = len(word_to_id) # word_to_id 딕셔너리의 길이로 단어의 ID지정
          word_to_id[word] = new_id
          id_to_word[new_id] = word
          
  print(word_to_id)
  print(id_to_word)
  # {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}
  # {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
  
  # 단어 목록을 단어ID 목록으로 변경
  corpus = [word_to_id[w] for w in words] # 내포 표기
  # # [0, 1, 2, 3, 4, 1, 5, 6]
  corpus = np.array(corpus)
  # [0 1 2 3 4 1 5 6]
  ```



- 🌟전처리 함수 : preprocess()

  ```python
  def preprocess(text):
      text = text.lower()
      text = text.replace('.', ' .')
      words = text.split()
  
      word_to_id = {}
      id_to_word = {}
  
      for word in words:
          if word not in word_to_id:
              new_id = len(word_to_id)  # word_to_id 딕셔너리의 길이로 단어의 ID지정
              word_to_id[word] = new_id
              id_to_word[new_id] = word
  
      corpus = [word_to_id[w] for w in words]
      corpus = np.array(corpus)
      
      return corpus, word_to_id, id_to_word
  
  # corpush : 단어ID 목록
  # word_to_id : 단어에서 단어ID로의 딕셔너리
  # id_to_word : 단어ID에서 단어로의 딕셔너리
  ```

  > 말뭉치를 다룰 준비를 마쳤으니 다음 목표로 말뭉치를 사용해 <u>'단어의 의미'</u>를 추출하는 방법인 '통계 기반 기법'을 살펴보자.



### 3.2 단어의 분산 표현

책에서는 '색'을 예로 단어의 분산표현을 설명하고있다.

색을 표현하는 방법은 두 가지가 있다. 하나는 명칭을 부여해 표현하는 방법, 다른 하나는 RGB라는 3차원 벡터로 표현하는 방법이 있다.

후자가 색을 더 정확하게 명시할 수 있으며 색끼리의 관련성도 더 쉽게 판단할 수 있다.

따라서 '단어의 의미'를 정확하게 파악할수 있는 <u>벡터 표현</u>을 구현보자. 

이를 단어의 **`분산 표현`**이라 한다.

🙋‍♀️ 분산 표현 : 단어를 고정 길이의 밀집벡터(대부분의 우너소가 0이 아닌 실수인 벡터)로 표현하는 것 



### 3.3 분포 가설

> 단어를 벡터로 표현하는 연구의 공통 가설은 '단어의 의미는 주변 단어에 의해 형선된다'는 것이다. 
>
> 이를 분포 가설(distributional hypothesis)라한다.

📍 분포 가설 의미 : 단어 자체에는 의미가 없고, 그 단어가 사용된 **`맥락`**이 의미를 형성한다.

​	👉 맥락 : 주변에 놓인 단어

​	<img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-3.png" alt="fig 2-3" style="zoom:50%;" /> 

- 맥락의 크기(주변에 단어를 몇 개 포함시킬지) = 윈도우 크기(window size)



### 3.4 동시발생 행렬

> 분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해보자. 

📍 **`통계 기반 기법`** : 어떤 단어에 주목했을 때, 그 주변에 어떤 단어가 몇 번이나 등장하는지를 세어 집계하는 방법

```python
text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(id_to_word)
# {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
```

- 윈도우 크기를 1로 하고 단어의 빈도를 표로 정리해보자.

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-7.png" alt="fig 2-7" style="zoom:50%;" /> 

  > 이 표가 행렬의 행태를 띤다는 뜻에서 동시발생 행렬(co-occurrence matrix)라고 한다.

- 위의 표를 파이썬으로 구현해보자.

  ```python
  def create_co_matrix(corpus, vocab_size, window_size = 1):
      # vocab_size = len(word_to_id)
      corpus_size = len(corpus) # 문장의 길이
      co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32) # 행렬을 0로 초기화
      
      for idx, word_id in enumerate(corpus): # 단어의 순서와 ID 값 가져옴
          for i in range(1, window_size+1): # 윈도우 사이즈만큼 체크
              left_idx = idx - i
              right_idx = idx + i
              
              if left_idx >= 0:
                  left_word_id = corpus[left_idx] # 이동한 idx에 해단되는 단어의 ID
                  co_matrix[word_id, left_word_id] += 1 # 해당 단어의 왼쪽 단어들의 ID 값 체크
                  
              if right_idx < corpus_size:
                  right_word_id = corpus[right_idx]
                  co_matrix[word_id, right_word_id] += 1
  	
      return co_matrix
  ```



### 3.5 벡터간 유사도

> 단어를 벡터로 표현한 다음에는 벡터 상의 유사도를 측정하는 방법을 살펴보자.
>
> 벡터 사이의 유사도를 측정하는 방법은 벡터의 내적, 유클리드 거리 등의 다양한 방법이 있지만 여기에서는 자주 이용하는
>
> **`코사인 유사도`**를 알아보자.

📍 **`코사인 유사도`**

<img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-1.png" alt="e 2-1" style="zoom:50%;" /> 

> 분자에는 벡터의 내적, 분모에는 벡터의 노름(norm)

🙋‍♀️ 위의 식의 핵심은 벡터를 정규화하고 내적을 구하는 것



- 코사인 유사도를 파이썬으로 구현해보자.

  ```python
  def cos_similarity(x,y, eps=1e-8): # 인수로 제로 벡터가 들어오면 분모가 0이 되기때문에 작은 수를 더해줌
      nx = x/(np.sqrt(np.sum(x**2))+eps) #eps = epsilon
      ny = y/(np.sqrt(np.sum(y**2))+eps)
  
      return np.dot(nx, ny)
  ```

  

- 코사인 유사도를 사용하여 두 단어의 유사도를 도출해보자.

  ```python
  text = 'You say goodbye and I say hello.'
  corpus, word_to_id, id_to_word = preprocess(text)
  # corpus : [0 1 2 3 4 1 5 6] 
  # word_to_id : {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6} 
  # id_to_word : {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
  
  vocab_size = len(word_to_id)
  C = create_co_matrix(corpus, vocab_size)
  
  c0 = C[word_to_id['you']]
  c1 = C[word_to_id['i']]
  cosine_simil = cos_similarity(c0, c1)
  # c0 : [0 1 0 0 0 0 0]
  # c1 : [0 1 0 1 0 0 0]
  # cosine_simil : 0.7071067691154799
  ```

  > 코사인 유사도의 값은 -1~1사이이므로 0.7의 유사도는 높은 편이다.



### 3.6 유사 단어의 랭킹 표시

> 어떤 단어가 검색어로 주어지면, 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현해보자.

🙄 어떻게 구현할 것인가?

1. 검색어의 단어 벡터를 꺼낸다.

2. 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 구한다.

3. 계산 코사인 유사도 결과를 기준으로 값이 높은 순서대로 출력한다.

   ```python
   def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):
       # 검색어를 꺼냄
       if query not in word_to_id:
           print('%s를 찾을 수 없음' %query)
           return
   
       print('[query] : %s' %query)
       query_id = word_to_id[query]
       query_vec = word_matrix[query_id]
   
       # 코사인 유사도 계산
       vocab_size = len(id_to_word)
       similarity = np.zeros(vocab_size)
   
       for i in range(vocab_size):
           similarity[i] = cos_similarity(word_matrix[i], query_vec)
   
       # 코사인 유사도를 기준으로 내림차순으로 출력
       cnt = 0
       for i in (-1*similarity).argsort(): 
           # argsort() : 오름차순으로 정렬 => 인덱스로 표현. 오름차순으로 정렬하므로 -1을 곱해줌(큰 것이 앞으로)
           
           if id_to_word[i] == query: # 같은 단어는 제외
               continue
           print('%s : %s' % (id_to_word[i], similarity[i]))
   
           cnt += 1
           if cnt >= top: # 지정한 개수 넘으면 출력x
               return
           
   most_similar('you', word_to_id, id_to_word, C)
   '''
   [query] : you
   goodbye : 0.7071067691154799
   i : 0.7071067691154799
   hello : 0.7071067691154799
   say : 0.0
   and : 0.0
   '''
   ```

   > 여기서 i는 you와 인칭대명사로 비슷하다는 건 이해.
   >
   > 하지만 다른 단어은 이해하기 어렵다 => 말뭉치의 크기가 너무 작은 것이 원인



---



## 4. 통계 기반 기법 개선하기

> 앞절에서는 단어의 동시발생 행렬을 만들어 단어를 벡터로 표현했다. 하지만 동시발생 행렬에는 개선할 점이 많음



### 4.1 상호정보량

동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타냄.

예를 들어 'the car'라는 문구에서 동시발생 횟수만 본다면 'drive'보다 'the'와의 관련성이 더 강하다고 평가된다.

따라서 빈도수로 관령성을 평가하기에는 많은 문제가 있다.

👉 이를 해결하기 위해 **`점별 상호정보량(Pointwise Mutual Information, PMI)`** 이라는 척도를 사용



- PMI : 확률 변수 x와 y에 대해 다음 식으로 정의 됨

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-2.png" alt="e 2-2" style="zoom:50%;" /> 

  > p(x) : x가 일어날 확률
  >
  > p(y) : y가 일어날 확률
  >
  > p(x, y) : x, y가 동시에 일어날 확률

  👉 PMI 가 높을수록 관련성이 높다

  

  자연어에 적용하면 p(x)는 x라는 단어가 말뭉치에 등장할 확률을 뜻함

  C : 동시발생 행렬

  C(x, y) : 단어 x와 y가 동시발생하는 횟수

  C(x) : 단어 x가 말뭉치에 등장하는 횟수

  C(y) : 단어 y가 말뭉치에 등장하는 횟수

  N : 말뭉치에 포함된 단어 수

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-3.png" alt="e 2-3" style="zoom:50%;" /> 

  ex) 'the', 'car', 'drive'의 발생 횟수는 각각 1000, 20, 10으로 car와 the, car와 drive의 동시발생 횟수는 10회, 5회로 가정해보자.

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-4.png" alt="e 2-4" style="zoom:50%;" /> 

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-5.png" alt="e 2-5" style="zoom:50%;" />  

  > 분모에 집중해보자. the의 발생횟수는 1000으로 어디든 동시발생할 가능성이 높다는 것을 말한다. 이를 참고하여 확률을 나타내기 때문에 drive의 관련성이 더 높게 나옴을 확인할 수 있다.

  🔥 단어가 단독으로 출현하는 횟수가 고려되었기 때문

  

  🙄 PMI에서도 하나의  문제점이 있다. 두 단어의 동시발생 횟수가 0이면 log0 = -∞ 가 된다는 점!!

  ​		👉 PPMI(양의 상호정보량)을 사용하자. PMI가 음수일 때 0으로 처리

  ​				<img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-6.png" alt="e 2-6" style="zoom:50%;" />

  - ```python
    def ppmi(C, verbose=False, eps = 1e-8):
        '''PPMI(양의 상호정보량) 생성
    
        :param C: 동시발생 행렬
        :param verbose: 진행 상황을 출력할지 여부
        :return:
        '''
        M = np.zeros_like(C, dtype=np.float32) # 동시행렬 크기와 같은 행렬(0으로 초기화)
        # 근삿값 구함
        N = np.sum(C) # 말뭉치에 포함된 단어 수
        S = np.sum(C, axis=0) # 각 단어가 등장한 횟수
        # axis
        # 2차원 : 0 = 열, 1 = 행
        # 3차원 : 0 = 높이, 1 = 열, 2 = 행
        total = C.shape[0] * C.shape[1]
        cnt = 0
    
        for i in range(C.shape[0]):
            for j in range(C.shape[1]):
                pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)
                M[i, j] = max(0, pmi) # PMI할당
    
                if verbose:
                    cnt += 1
                    if cnt % (total//100) == 0:
                      print('%.1f%% 완료' % (100*cnt/total))
        return M
    ```
    
    ```python
    text = 'You say goodbye and I say hello.'
    corpus, word_to_id, id_to_word = preprocess(text)
    vocab_size = len(word_to_id)
    C = create_co_matrix(corpus, vocab_size)
    W = ppmi(C)
    
    np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시
    print('동시발생 행렬')
    print(C)
    print('-'*50)
    print('PPMI')
    print(W)
    
    '''
    동시발생 행렬
    [[0 1 0 0 0 0 0]
     [1 0 1 0 1 1 0]
     [0 1 0 1 0 0 0]
     [0 0 1 0 1 0 0]
     [0 1 0 1 0 0 0]
     [0 1 0 0 0 0 1]
     [0 0 0 0 0 1 0]]
    --------------------------------------------------
    PPMI
    [[0.    1.807 0.    0.    0.    0.    0.   ]
     [1.807 0.    0.807 0.    0.807 0.807 0.   ]
     [0.    0.807 0.    1.807 0.    0.    0.   ]
     [0.    0.    1.807 0.    1.807 0.    0.   ]
     [0.    0.807 0.    1.807 0.    0.    0.   ]
     [0.    0.807 0.    0.    0.    0.    2.807]
     [0.    0.    0.    0.    0.    2.807 0.   ]]
    '''
    ```
  ```
    
    > PPMI의 문제점 : 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다.
    >
    > 또한 위의 예시에서 확인했듯이 대부분의 원소가 0이다. 이는 각 원소의 '중요도'가 낮다는 뜻이며 이런 벡터는 노이즈에 약하고 견고하지 못하다.
    >
    > 👉 때문에 이 문제를 대처하고자 <u>'벡터의 차원 감소'</u> 방법이 적용된다.
  ```



### 4.2 차원 감소

> **`차원 감소(dimensionality reduction)`**  : 벡터의 차원을 줄이는 방법

🔥  중요한 정보는 최대한 유지하면서 줄이는 것

- 데이터의 분포를 고려해 중요한 '축'을 찾는 것

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-8.png" alt="fig 2-8" style="zoom: 33%;" /> 

  왼쪽 그림은 데이터점들을 2차원 좌표에 표시한 모습.

  오른쪽은 새로운 축을 도입하여 좌표축 하나만으로 표시.

  => 새로운 축을 찾을 때, 데이터가 넓게 분포되도록 고려

  이때, 데이터점의 값은 새로운 축으로 사영된 값으로 변함

  🌟 1차원 값만으로도 데이터의 본질적인 차이를 구별할 수 있어야함!!



- 원소 대부분이 0인 행렬을 희소행렬, 희소벡터라 한다. 

  차원 감소의 핵심은 희소벡터에서 중요한 축을 찾아 더 적은 차원으로 다시 표현하는 것

  즉, 희소벡터를 밀집벡터로 변환시키는 것



📍 차원 감소시키는 방법은 여러가지지만 우리는 **`특잇값분해(singular value decomposition, SVD)`**를 이용.

-  **`특잇값분해(singular value decomposition, SVD)`**

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/e 2-7.png" alt="e 2-7" style="zoom:50%;" /> 

  > U와 V : 직교행렬(orthogomal matrix) = 역행렬이 전치행렬과 같은 정사각행렬
  >
  > S : 대각행렬(diagomal) = 대각성분 외에는 모두 0인 행렬

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-9.png" alt="fig 2-9" style="zoom:50%;" /> 

  U는 직교행렬로 어떠한 공간의 축(기저)를 형성. 즉, U 행렬은 '단어 공간'으로 취급할 수 있음.

  S는 대각행렬로 대각성분에는 '특이값(해당 축의 중요도)'이 큰 순서대로 나열되어 있음.

  <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-10.png" alt="fig 2-10" style="zoom:50%;" /> 

  만약 행렬 S에서 특잇값이 작다면 중요도가 낮다는 뜻이므로, 행렬 U에서 여분의 열벡터를 깎아내려 원래의 행렬을 근사할 수 있음.

  

### 4.3 SVD에 의한 차원 감소

> SVD를 파이썬 코드로 살펴보자. 

```python
W = ppmi(C)
# SVD
U, S, V = np.linalg.svd(W) # linalg : 선형대수(linear algebra)
# U는 SVD에 의해 변환된 밀집벡터 표현을 담음

# C[0] : [0 1 0 0 0 0 0]
# W[0] : [0.  1.807 0.   0.   0.   0.   0.  ]
# U[0] : [ 3.409e-01  0.000e+00 -3.886e-16 -1.205e-01 -9.323e-01 -1.110e-16 1.958e-17]

# 이 밀집벡터의 차원을 감소시키기 위해서는
# U[0, :2] : [ 3.409e-01  0.000e+00] 의 2차원이 됨
```

 <img src="밑시딥2 2. 자연어와 단어의 분산 표현.assets/fig 2-11.png" alt="fig 2-11" style="zoom:50%;" /> 

> 말뭉치가 아주 작아 결과를 그대로 받아들이기 어렵다. 그래서 PTB 데이터셋을 사용하여 실험을 수행해보자. 

행렬의 크기가 N이면 SVD 계산은 O(N**3)이 걸린다. 때문에 Truncated SVD(특잇값이 작은 것은 버리는 방식)을 사용한다.



### 4.4 PTB 데이터셋

> 펜 트리뱅크(Penn Treebank, PTB) 말뭉치는 품질을 측정하는 벤치마크로 자주 이용됨

회소한 단어는 <unk>라는 특수 문자로 치환, 구체적인 숫자는 'N'으로 치환하는 전처리 해둠

이 책에서는 문장의 구분없이 여러 문장을 연결한 '하나의 큰 시계열 데이터'로 간주함.

문장 끝에는 <eos>라는 특수문자 사용

```python
from dataset import ptb

corpus, word_to_id, id_to_word = ptb.load_data('train')

print('말뭉치 크기:', len(corpus))
print('corpus[:30]:', corpus[:30])
print()
print('id_to_word[0]:', id_to_word[0])
print('id_to_word[1]:', id_to_word[1])
print('id_to_word[2]:', id_to_word[2])
print()
print("word_to_id['car']:", word_to_id['car'])
print("word_to_id['happy']:", word_to_id['happy'])
print("word_to_id['lexus']:", word_to_id['lexus'])

'''
말뭉치 크기: 929589
corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

id_to_word[0]: aer
id_to_word[1]: banknote
id_to_word[2]: berlitz

word_to_id['car']: 3856
word_to_id['happy']: 4428
word_to_id['lexus']: 7426
'''
```



### 2.5 PTB 데이터셋 평가

> PTB 데이터셋에 통계 기반 기법을 적용해보자.

```python
window_size = 2 # 2차원으로 표시
wordvec_size = 100 # U를 100열만 추출

corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)
print('동시발생 수 계산 ...')
C = create_co_matrix(corpus, vocab_size, window_size)
print('PPMI 계산 ...')
W = ppmi(C, verbose=True)

print('calculating SVD ...')
try:
    # truncated SVD (빠르다!)
    from sklearn.utils.extmath import randomized_svd
    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,
                             random_state=None)
except ImportError:
    # SVD (느리다)
    U, S, V = np.linalg.svd(W)

word_vecs = U[:, :wordvec_size]

querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```
