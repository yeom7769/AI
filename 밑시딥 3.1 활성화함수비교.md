# 🌠활성화 함수 비교

> sigmoid vs ReLU vs soft max

<img src="밑시딥 3.1 활성화함수비교.assets/3.1.PNG" alt="3.1" style="zoom:80%;" /> 

<img src="밑시딥 3.1 활성화함수비교.assets/3.2.PNG" alt="3.2" style="zoom:80%;" /> 

## 1. Sigmoid

`h(x)=1/(1+exp(-x))`

#### 1.1 문제점

1. Saturated neurons 'kill' the gradients

2. Sigmoid outputs are not zero-centered

3. exp() is a bit compute expensive

   #### 1.  Saturated neurons 'kill' the gradients

   > saturated?? 시그모이드의 기울기가 0인 지점들(출력값이 0과 1에 가까워지는 지점 )

   👉 변수의 변화에도 더 이상 함수값이 변화X. 이는 ChainRule에 의해 가중치에 대한 Gradient가 0에 가까워질 것이므로 **`가중치 업데이트`**에 문제가 있는 상태  

   > 그 외의 지점에서도 Gradient가 최대 0.25로 Gradient가 곱해질 때마다 최소 0.25씩 줄어든다.

   👉 `Vanising Gradint` 문제 발생

   🔥 실제로는 이진 분류 문제의 출력 layer가 아니면 시그모이드 함수는 거의 쓰지 않음

   

   #### 2.  Sigmoid outputs are not zero-centered

   > output이 항상 양수이면 다음 뉴런의 input 값 역시 항상 양수가 됨.
   >
   > 이 경우 가중치 w의 Gradient는 항상 양수거나 음수.(mul gate 연산 방식에 의해)

   👉 w1과 w2의 그래프에서 1, 3 사분면의 방향으로만 이동하므로 2, 4 사분면으로 움직이기 위해서는 지그재그 경로로 여러 번 탐색을 해야함. ==> `비효율적인 weight update`

   



## 2. ReLU

#### 2.1 장점

1. 양의 영역에서 Saturation 발생 X
2. exp 연산 없어 경제적 ==> 빠른 연산 가능



#### 2.2 단점

1. zero-centered 출력 X
2.  x<0 구간 모두 버림 ==> Gradient가 0이 되고 가중치 업데이트 발생 X `Dead ReLU`

![3.3](밑시딥 3.1 활성화함수비교.assets/3.3.PNG)

## 4. 활성화 함수의 종류

| 문제 유형 | 목표 레이블    | 활성화 함수  | 손실 함수                                      |
| --------- | -------------- | ------------ | ---------------------------------------------- |
| 회귀 예측 | 연속형(숫자형) | 선형(linear) | 평균제곱오차(MSS)                              |
| 이진 분류 | 이산형(범주형) | 시그모이드   | 이진 교차 엔트로피(binary_crossentropy)        |
| 다중 분류 | 이산형(범주형) | 소프트맥스   | 범주형 교차 엔트로피(categorical_crossentropy) |

