# 밑시딥 📂5. 오차역전파법(backpropagation)

> 4장에서 가중치 매개변수에 대한 손실 함수의 기울기는 `수치 미분`을 사용하였다.
>
> `수치 미분`은 구현이 쉬우나 시간이 오래 걸린다는 단점이 있다. 때문에 기울기를 효율적으로 계산하는 **`오차역전파법`**을 배워보자.



## 1. 계산 그래프

> 계산 과정을 노드와 에지로 표현된 그래프로 나타냄

- 계산 그래프 흐름
  1. 계산 그래프를 구성
  2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행 ==> **``순전파(forward propagation)``** 
     - 계산을 오른쪽에서 왼쪽으로 진행 ==> **`역전파(backward propagation)`**



- 계산 그래프의 특징 : <u>국소적 계산</u>을 전파함으로써 최종 결과를 얻음

  ​									👉 자신과 직접 관계된 작은 범위 ==> 자신과 관련된 정보만으로 결과를 출력 ex) 각 노드에서 계산



- 왜 계산 그래프로 푸는가?

  1. 국소적 계산 : 전체가 복잡해도 각 노드에서는 단순한 계산에 집중해 문제 단순화

  2. 중간 계산 결과를 모두 보관할 수 있음

  3. 역전파를 통해 `미분`을 효율적으로 계산 할 수 있음

      <img src="밑시딥 5. 오차역전파법.assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />

     - 예로, 사과 가격에 대한 지분 금액의 미분 = 2.2 (사과가 1원 오르면 지불 금액은 2.2원 오른다)
     - 중간까지 구한 미분 결과를 공유할 수 있어 다수의 미분을 효율적으로 계산 할 수 있음

     

-----



## 2. 📌연쇄법칙(chain rule)

> 위의 `국소적 미분![fig 5-6](deep_learning_images/fig 5-6.png)`을 전달하는 원리는 `연쇄법칙`에 따른 것이다.



### 2.1 계산 그래프의 역전파

- 역전파 계산 절차(E : 신호)

 <img src="밑시딥 5. 오차역전파법.assets/fig 5-6.png" alt="fig 5-6" style="zoom:50%;" />



### 2.2 연쇄법칙

- 합성함수 : 여러 함수로 구성된 함수

   <img src="밑시딥 5. 오차역전파법.assets/e 5.1.png" alt="e 5.1" style="zoom:50%;" />

- 연쇄법칙 : 합성 함수의 미분에 대한 성질

  🔥**`합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다 `** 

   <img src="밑시딥 5. 오차역전파법.assets/e 5.2.png" alt="e 5.2" style="zoom:50%;" />



### 2.3 연쇄법칙과 계산 그래프

 <img src="밑시딥 5. 오차역전파법.assets/fig 5-7.png" alt="fig 5-7" style="zoom: 33%;" />

- 맨 왼쪽 역전파에서 연쇄법칙에 따르면 'x에 대한 z의 미분'이 되므로 역전파가 하는 일은 연쇄 법칙의 원리와 같다.



----



## 3. 역전파

> 연산을 예로 들어 역전파의 구조를 설명해보자.



### 3.1 덧셈 노드의 역전파

#### **`z=x+y`**

  <img src="밑시딥 5. 오차역전파법.assets/e 5.5.png" alt="e 5.5" style="zoom:50%;" /> <img src="밑시딥 5. 오차역전파법.assets/fig 5-9.png" alt="fig 5-9" style="zoom: 33%;" />

🔥 **<u>덧셈 노드의 역전파는 입력 값을 그대로 흘려보냄</u>**



### 3.2 곱셈 노드의 역전파

#### **`z=xy`**

 <img src="밑시딥 5. 오차역전파법.assets/e 5.6.png" alt="e 5.6" style="zoom:50%;" /> <img src="밑시딥 5. 오차역전파법.assets/fig 5-12.png" alt="fig 5-12" style="zoom: 33%;" />

🔥 **<u>상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보냄</u>**

​      👉 **`곱셈의 역전파`**는 순방향 입력 신호의 값이 필요 => 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해야함



### 3.3 사과 쇼핑의 예

>  이 문제에서는 '사과의 가격, 사과의 개수, 소비세'라는 변수가 최종 금액에 어떻게 영향을 주느냐를 풀고자 한다.

 <img src="밑시딥 5. 오차역전파법.assets/fig 5-14.png" alt="fig 5-14" style="zoom:50%;" /> 

- 이는 소비세와 사과 가격이 같은 양만큼 오르면 최종 금액에는 소비세가 200의 크기로, 사과 가격이 2.2 크기로 영향을 준다는 뜻



----



## 4. 단순한 계층 구현하기

> 사과 쇼핑을 파이썬으로 구현해보자.



### 4.1 곱셈 계층

```python
# 곱셈 노드
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    # 순전파
    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y
        return out

    # 역전파
    def backward(self, dout):
        # dout = 상류에서 넘어온 미분
        # x와 y를 바꾼다.
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy
```

- 위에서 구현한 곱셈 계층을 이용하여 사과 쇼핑 구현

```python
apple = 100
apple_num = 2
tax = 1.1

mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)
price = mul_tax_layer.forward(apple_price, tax)

# backward
dprice = 1
dapple_price, dtax = mul_tax_layer.backward(dprice)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print("price:", int(price))
#price: 220
print("dApple:", dapple)
#dApple: 2.2
print("dApple_num:", int(dapple_num))
#dApple_num: 110
print("dTax:", dtax)
#dTax: 200
```



### 4.2 덧셈 계층

```python
# 덧셈 노드
class AddLayer:
    # backward에서 forward의 변수 저장이 필요없기 때문에 변수 초기화 안해도 됨
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y
        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```



- 아래의 그림의 상황을 구현해보자

<img src="밑시딥 5. 오차역전파법.assets/fig 5-17.png" alt="fig 5-17" style="zoom: 33%;" /> 

```python
apple = 100 # 사과 가격
apple_num = 2 # 사과 개수
orange = 150 # 귤 가겨
orange_num = 3 # 귤 개수
tax = 1.1 # 소비세

# layer은 노드 마다 설정
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)
orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)
price = mul_tax_layer.forward(all_price, tax)  # (4)

# backward
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)
dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)

# forward와 backward 반대 순서!!

print("price:", int(price))
#price: 715
print("dApple:", dapple)
#dApple: 2.2
print("dApple_num:", int(dapple_num))
#dApple_num: 110
print("dOrange:", dorange)
#dOrange: 3.3000000000000003
print("dOrange_num:", int(dorange_num))
#dOrange_num: 165
print("dTax:", dtax)
#dTax: 650
```



---



## 5. 활성화 함수 계층 구현하기

> 계산 그래프를 신경망에 적용해 보자.



### 5.1 ReLU 계층

> 활성화 함수 ReLU의 그래프와 수식은 아래와 같다.

 <img src="밑시딥 5. 오차역전파법.assets/fig 3-9.png" alt="fig 3-9" style="zoom: 25%;" /> 

 <img src="밑시딥 5. 오차역전파법.assets/e 5.7.png" alt="e 5.7" style="zoom:50%;" /> <img src="밑시딥 5. 오차역전파법.assets/e 5.8.png" alt="e 5.8" style="zoom:50%;" />



> 계산 그래프로 나타내면 아래와 같다

<img src="밑시딥 5. 오차역전파법.assets/fig 5-18.png" alt="fig 5-18" style="zoom:50%;" /> 

- 이를 파이썬으로 구현해보자.

  ```python
  class Relu:
      def __init__(self):
          # mask(인스턴스 변수) : True/False로 구성된 넘파이 배열
          self.mask = None
  
      def forward(self, x):
          # 순전파의 입력인 x의 원소 값이 0이하인 인덱스는 True, 0보다 큰 원소는 False
          self.mask = (x <= 0)
          out = x.copy()
          # 0이하인 x 인덱스는 0, 그 외의 값은 그대로 유지
          out[self.mask] = 0
          return out
  
      def backward(self, dout):
          # mask 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정한다.
          dout[self.mask] = 0
          # 0이상인 값들은 상류에서 전파된 dout을 그대로 출력한다.
          dx = dout
          return dx
  ```

  

### 5.2 Sigmoid 계층

 <img src="밑시딥 5. 오차역전파법.assets/fig 3-7.png" alt="fig 3-7" style="zoom: 25%;" /> 

<img src="밑시딥 5. 오차역전파법.assets/e 5.9.png" alt="e 5.9" style="zoom: 50%;" /> 

 

>  Sigmoid 를 계산 그래프로 그리면 아래와 같다.

<img src="밑시딥 5. 오차역전파법.assets/fig 5-19.png" alt="fig 5-19" style="zoom:50%;" /> 

> 역전파의 흐름을 오른쪽에서 왼쪽으로 한 단계씩 짚어보자.



#### 1 단계.  **`/`** 노드

> y = 1/x 를 미분하면 아래와 같다.

 <img src="밑시딥 5. 오차역전파법.assets/e 5.10.png" alt="e 5.10" style="zoom:50%;" /> 

#### 2 단계. **`+`** 노드

> 상류의 값을 그대로 하류로 내보냄



#### 3 단계. **`exp`** 노드

> y=exp(x) 를 미분하면 아래와 같다.

 <img src="밑시딥 5. 오차역전파법.assets/e 5.11.png" alt="e 5.11" style="zoom:50%;" /> 



#### 4 단계. **`x`** 노드

> 순전파 때의 값을 서로 바꿔 곱함

<img src="밑시딥 5. 오차역전파법.assets/fig 5-20.png" alt="fig 5-20" style="zoom:50%;" />

- 최종 출력은 아래와 같이 순전파의 입력 x와 출력 y만으로 계산할 수 있음

 <img src="밑시딥 5. 오차역전파법.assets/e 5.12.png" alt="e 5.12" style="zoom:50%;" />

> 그러므로 sigmoid 계산 그래프는 아래와 같이 간소화 할 수 있다. 
>
> 간소화 => 중간 계산들을 생략할 수 있어 효율적인 계산 + 노드를 그룹화하여 세세한 내용 노출하지 않고 입력과 출력에만 집중

<img src="밑시딥 5. 오차역전파법.assets/fig 5-22.png" alt="fig 5-22" style="zoom:50%;" /> 



- sigmoid 계층을 파이썬으로 구현해보자.

```python
class Sigmoid:
	
    def __init__(self):
        self.out = None

    # 순전파의 출력을 인스턴스 변수 out에 보관
    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```



---



## 6. Affine/Softmax 계층 구현하기



### 6.1 Affine 계층

> 신경망의 순전파 대 수행하는 `행렬의 곱`은 기하학에서는 `affine transformation`이라 한다.



- 변수가 스칼라 값이 아닌 행렬이 흐를 때, `Affine 계층의 역전파`

<img src="밑시딥 5. 오차역전파법.assets/fig 5-25.png" alt="fig 5-25" style="zoom: 33%;" /> 

🔥 여기서 조심해야할 부분은 dot 노드에서 원소들을 서로 바꿔 곱하는 단계에서 `행렬은 전치행렬`로 곱해줌

​		👉 행렬의 형상에 주의!



### 6.2 배치용 Affine 계층

> 데이터 N개를 묶어 순전파하는 경우

<img src="밑시딥 5. 오차역전파법.assets/fig 5-27.png" alt="fig 5-27" style="zoom: 33%;" /> 



- 편향을 더할 때 주의 ! => 편향은 각 데이터에 더해짐 => 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함

```python
# affine 계층 구현
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        # 가중치와 편향 매개변수의 미분
        self.dW = None
        self.db = None
        # 텐서 대응 위한 x.shape을 저장하는 인스턴스 변수
        self.original_x_shape = None

    def forward(self, x):
        # 텐서 대응
        self.original_x_shape = x.shape
        # 1차원으로 reshape
        x = x.reshape(x.shape[0], -1)
        
        self.x = x
        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        # .T => 전치행렬
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        
        # 입력 데이터 모양 변경(텐서 대응)
        dx = dx.reshape(*self.original_x_shape)
        return dx
```



### 6.3 Softmax-with-Loss 계층

> 출력층의 소프트맥스 함수는 입력 값을 정규화하여 출력함

- Softmax-with-Loss 계층 : 손실함수인 교차 엔트로피 오차 포함한 계층

<img src="밑시딥 5. 오차역전파법.assets/fig 5-29.png" alt="fig 5-29" style="zoom:50%;" />

<img src="밑시딥 5. 오차역전파법.assets/fig 5-30.png" alt="fig 5-30" style="zoom: 33%;" /> 



```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None # 손실함수
        self.y = None    # softmax의 출력
        self.t = None    # 정답 레이블(원-핫 인코딩 형태)
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때
            dx = (self.y - self.t) / batch_size
            # 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        return dx
```



---



## 7. 오차역전파법 구현하기

> 앞서 구현한 계층을 조합하여 신경망을 구축해보자.



### 7.1 신경망 학습의 전체 그림

1. 전체 
   - 학습 = 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정
2. 1단계 : 미니배치
   - 훈련 데이터 일부를 무작위로 가져옴. 미니배치의 손실함수 값을 줄이는 것이 목표
3. 2단계 : 기울기 산출
   - 각 가중치 매개변수의 기울기 구함. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시
4. 3단계 : 매개변수 갱신
   - 가중치 매개변수를 기울기 방향으로 갱신

> 2단계의 기울기산출에서 우리는 수치 미분을 사용하였다. 하지만 수치 미분은 속도가 느리다는 단점이 있었고 오차역전파법을 이용하면 기울기를 효율적이고 빠르게 구할 수 있음



### 7.2 오차역전파법을 적용한 신경망 구현하기

> 2층 신경망을 구현해보자.

```python
class TwoLayerNet:

    # 가중치 초기화
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        # input_size : 입력층 뉴런 수 hidden_size : 은닉층 뉴런 수 ouput_size : 출력층 뉴런 수
    	# weight_init_std : 가중치 초기화 시 정규분포 스케일
        self.params = {} # 신경망의 매개변수 딕셔너리
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        # 가우시안 표준 정균 분포 범위(평균0,분산1)에서 난수 matrix array생성
        self.params['b1'] = np.zeros(hidden_size) # 0로 초기값 생성
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) 
        self.params['b2'] = np.zeros(output_size)

        # 계층 생성
        self.layers = OrderedDict() # OrderedDict : 순서 있는 딕셔너리 / 3.6버전 부터 안써도 됨
        # 순전파 때는 추가한 순서대로 각 계층의 메서드 호출
        # 역전파 때는 계층을 반대 순서로 호출
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()
        
    # 결과 예측 메서드
    def predict(self, x):
        for layer in self.layers.values():
            # 계층 순서대로 forward 진행
            # affine1 -> relu1 -> affine2
            x = layer.forward(x)
        
        return x
        
    # x : 입력 데이터, t : 정답 레이블
    # 손실함수 값 출력
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    # 정확도 계산(정답인지 아닌지 판별)
    def accuracy(self, x, t):
        y = self.predict(x)
        # 최댓값의 index 얻음
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy # 정답률 반환
        
    # 가중치 매개변수의 기울기 계산(수치 미분)    
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    # 가중치 매개변수의 기울기 계산(오차역전파법)  
    def gradient(self, x, t):
        
        # forward Loss
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        # affine1 -> relu -> affine2 
        layers.reverse()
        # affine2 -> relu -> affine1
        for layer in layers:
            dout = layer.backward(dout)

        # 결과 저장
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads

```



### 7.3 오차역전파법으로 구한 기울기 검증하기

> 기울기를 구한느 방법은 두가지가 있다. `수치 미분`과 `오차역전파법`이다.  
>
> 오차역전파법은 매개변수가 많아도 효율적으로 계산할 수 있기때문에 오차역전파법을 주로 사용한다.
>
> 수치미분은 구현이 쉽기때문에 오차역전파법을 정확히 구현했는지 확인하기 위해 필요하다.

- 기울기 확인 : 두 방식으로 구한 기울기가 일치함을 확인하는 작업

```python
# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 데이터 3개만 확인
x_batch = x_train[:3]
t_batch = t_train[:3]

# 수치 미분
grad_numerical = network.numerical_gradient(x_batch, t_batch)
# 오차역전파법
grad_backprop = network.gradient(x_batch, t_batch)

# 각 가중치의 절대 오차의 평균을 구한다.
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))
    print(key + ":" + str(diff))

# W1:3.8569612445837085e-10
# b1:2.596604584942223e-09
# W2:5.251747585074162e-09
# b2:1.4031324106628106e-07
```



### 7.4 오차역전파법을 사용한 학습 구현하기

```python
# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 10000 # 반복 횟수
train_size = x_train.shape[0]
batch_size = 100 # 미니배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 에폭 횟수
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # 미니배치만큼 batch 할당
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식
    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)
    
    # 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1에폭당 정확도 계산 , 훈련데이터 모두 봤을 때.
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)
```

```python
0.0829 0.0845
0.9061833333333333 0.9098
0.9230333333333334 0.9258
0.9347666666666666 0.9337
0.94415 0.9433
0.94965 0.9468
0.9559333333333333 0.9528
0.9597333333333333 0.9562
0.96455 0.9599
0.9665333333333334 0.9604
0.9702 0.9627
0.9718833333333333 0.9623
0.9738 0.9647
0.9754166666666667 0.9669
0.9777666666666667 0.9675
0.9780333333333333 0.9679
0.97965 0.9681
```
