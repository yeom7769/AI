# ë°‘ì‹œë”¥ ğŸ“‚5. ì˜¤ì°¨ì—­ì „íŒŒë²•(backpropagation)

> 4ì¥ì—ì„œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ëŠ” `ìˆ˜ì¹˜ ë¯¸ë¶„`ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
>
> `ìˆ˜ì¹˜ ë¯¸ë¶„`ì€ êµ¬í˜„ì´ ì‰¬ìš°ë‚˜ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë•Œë¬¸ì— ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” **`ì˜¤ì°¨ì—­ì „íŒŒë²•`**ì„ ë°°ì›Œë³´ì.



## 1. ê³„ì‚° ê·¸ë˜í”„

> ê³„ì‚° ê³¼ì •ì„ ë…¸ë“œì™€ ì—ì§€ë¡œ í‘œí˜„ëœ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ëƒ„

- ê³„ì‚° ê·¸ë˜í”„ íë¦„
  1. ê³„ì‚° ê·¸ë˜í”„ë¥¼ êµ¬ì„±
  2. ê·¸ë˜í”„ì—ì„œ ê³„ì‚°ì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì§„í–‰ ==> **``ìˆœì „íŒŒ(forward propagation)``** 
     - ê³„ì‚°ì„ ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì§„í–‰ ==> **`ì—­ì „íŒŒ(backward propagation)`**



- ê³„ì‚° ê·¸ë˜í”„ì˜ íŠ¹ì§• : <u>êµ­ì†Œì  ê³„ì‚°</u>ì„ ì „íŒŒí•¨ìœ¼ë¡œì¨ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ìŒ

  â€‹									ğŸ‘‰ ìì‹ ê³¼ ì§ì ‘ ê´€ê³„ëœ ì‘ì€ ë²”ìœ„ ==> ìì‹ ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œìœ¼ë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥ ex) ê° ë…¸ë“œì—ì„œ ê³„ì‚°



- ì™œ ê³„ì‚° ê·¸ë˜í”„ë¡œ í‘¸ëŠ”ê°€?

  1. êµ­ì†Œì  ê³„ì‚° : ì „ì²´ê°€ ë³µì¡í•´ë„ ê° ë…¸ë“œì—ì„œëŠ” ë‹¨ìˆœí•œ ê³„ì‚°ì— ì§‘ì¤‘í•´ ë¬¸ì œ ë‹¨ìˆœí™”

  2. ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ë¥¼ ëª¨ë‘ ë³´ê´€í•  ìˆ˜ ìˆìŒ

  3. ì—­ì „íŒŒë¥¼ í†µí•´ `ë¯¸ë¶„`ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚° í•  ìˆ˜ ìˆìŒ

      <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-5.png" alt="fig 5-5" style="zoom:50%;" />

     - ì˜ˆë¡œ, ì‚¬ê³¼ ê°€ê²©ì— ëŒ€í•œ ì§€ë¶„ ê¸ˆì•¡ì˜ ë¯¸ë¶„ = 2.2 (ì‚¬ê³¼ê°€ 1ì› ì˜¤ë¥´ë©´ ì§€ë¶ˆ ê¸ˆì•¡ì€ 2.2ì› ì˜¤ë¥¸ë‹¤)
     - ì¤‘ê°„ê¹Œì§€ êµ¬í•œ ë¯¸ë¶„ ê²°ê³¼ë¥¼ ê³µìœ í•  ìˆ˜ ìˆì–´ ë‹¤ìˆ˜ì˜ ë¯¸ë¶„ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚° í•  ìˆ˜ ìˆìŒ

     

-----



## 2. ğŸ“Œì—°ì‡„ë²•ì¹™(chain rule)

> ìœ„ì˜ `êµ­ì†Œì  ë¯¸ë¶„![fig 5-6](deep_learning_images/fig 5-6.png)`ì„ ì „ë‹¬í•˜ëŠ” ì›ë¦¬ëŠ” `ì—°ì‡„ë²•ì¹™`ì— ë”°ë¥¸ ê²ƒì´ë‹¤.



### 2.1 ê³„ì‚° ê·¸ë˜í”„ì˜ ì—­ì „íŒŒ

- ì—­ì „íŒŒ ê³„ì‚° ì ˆì°¨(E : ì‹ í˜¸)

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-6.png" alt="fig 5-6" style="zoom:50%;" />



### 2.2 ì—°ì‡„ë²•ì¹™

- í•©ì„±í•¨ìˆ˜ : ì—¬ëŸ¬ í•¨ìˆ˜ë¡œ êµ¬ì„±ëœ í•¨ìˆ˜

   <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.1.png" alt="e 5.1" style="zoom:50%;" />

- ì—°ì‡„ë²•ì¹™ : í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ëŒ€í•œ ì„±ì§ˆ

  ğŸ”¥**`í•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ `** 

   <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.2.png" alt="e 5.2" style="zoom:50%;" />



### 2.3 ì—°ì‡„ë²•ì¹™ê³¼ ê³„ì‚° ê·¸ë˜í”„

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-7.png" alt="fig 5-7" style="zoom: 33%;" />

- ë§¨ ì™¼ìª½ ì—­ì „íŒŒì—ì„œ ì—°ì‡„ë²•ì¹™ì— ë”°ë¥´ë©´ 'xì— ëŒ€í•œ zì˜ ë¯¸ë¶„'ì´ ë˜ë¯€ë¡œ ì—­ì „íŒŒê°€ í•˜ëŠ” ì¼ì€ ì—°ì‡„ ë²•ì¹™ì˜ ì›ë¦¬ì™€ ê°™ë‹¤.



----



## 3. ì—­ì „íŒŒ

> ì—°ì‚°ì„ ì˜ˆë¡œ ë“¤ì–´ ì—­ì „íŒŒì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ë³´ì.



### 3.1 ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒ

#### **`z=x+y`**

  <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.5.png" alt="e 5.5" style="zoom:50%;" /> <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-9.png" alt="fig 5-9" style="zoom: 33%;" />

ğŸ”¥ **<u>ë§ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒëŠ” ì…ë ¥ ê°’ì„ ê·¸ëŒ€ë¡œ í˜ë ¤ë³´ëƒ„</u>**



### 3.2 ê³±ì…ˆ ë…¸ë“œì˜ ì—­ì „íŒŒ

#### **`z=xy`**

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.6.png" alt="e 5.6" style="zoom:50%;" /> <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-12.png" alt="fig 5-12" style="zoom: 33%;" />

ğŸ”¥ **<u>ìƒë¥˜ì˜ ê°’ì— ìˆœì „íŒŒ ë•Œì˜ ì…ë ¥ ì‹ í˜¸ë“¤ì„ 'ì„œë¡œ ë°”ê¾¼ ê°’'ì„ ê³±í•´ì„œ í•˜ë¥˜ë¡œ ë³´ëƒ„</u>**

â€‹      ğŸ‘‰ **`ê³±ì…ˆì˜ ì—­ì „íŒŒ`**ëŠ” ìˆœë°©í–¥ ì…ë ¥ ì‹ í˜¸ì˜ ê°’ì´ í•„ìš” => ê³±ì…ˆ ë…¸ë“œë¥¼ êµ¬í˜„í•  ë•ŒëŠ” ìˆœì „íŒŒì˜ ì…ë ¥ ì‹ í˜¸ë¥¼ ë³€ìˆ˜ì— ì €ì¥í•´ì•¼í•¨



### 3.3 ì‚¬ê³¼ ì‡¼í•‘ì˜ ì˜ˆ

>  ì´ ë¬¸ì œì—ì„œëŠ” 'ì‚¬ê³¼ì˜ ê°€ê²©, ì‚¬ê³¼ì˜ ê°œìˆ˜, ì†Œë¹„ì„¸'ë¼ëŠ” ë³€ìˆ˜ê°€ ìµœì¢… ê¸ˆì•¡ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ì£¼ëŠëƒë¥¼ í’€ê³ ì í•œë‹¤.

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-14.png" alt="fig 5-14" style="zoom:50%;" /> 

- ì´ëŠ” ì†Œë¹„ì„¸ì™€ ì‚¬ê³¼ ê°€ê²©ì´ ê°™ì€ ì–‘ë§Œí¼ ì˜¤ë¥´ë©´ ìµœì¢… ê¸ˆì•¡ì—ëŠ” ì†Œë¹„ì„¸ê°€ 200ì˜ í¬ê¸°ë¡œ, ì‚¬ê³¼ ê°€ê²©ì´ 2.2 í¬ê¸°ë¡œ ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ëœ»



----



## 4. ë‹¨ìˆœí•œ ê³„ì¸µ êµ¬í˜„í•˜ê¸°

> ì‚¬ê³¼ ì‡¼í•‘ì„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì.



### 4.1 ê³±ì…ˆ ê³„ì¸µ

```python
# ê³±ì…ˆ ë…¸ë“œ
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None

    # ìˆœì „íŒŒ
    def forward(self, x, y):
        self.x = x
        self.y = y                
        out = x * y
        return out

    # ì—­ì „íŒŒ
    def backward(self, dout):
        # dout = ìƒë¥˜ì—ì„œ ë„˜ì–´ì˜¨ ë¯¸ë¶„
        # xì™€ yë¥¼ ë°”ê¾¼ë‹¤.
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy
```

- ìœ„ì—ì„œ êµ¬í˜„í•œ ê³±ì…ˆ ê³„ì¸µì„ ì´ìš©í•˜ì—¬ ì‚¬ê³¼ ì‡¼í•‘ êµ¬í˜„

```python
apple = 100
apple_num = 2
tax = 1.1

mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)
price = mul_tax_layer.forward(apple_price, tax)

# backward
dprice = 1
dapple_price, dtax = mul_tax_layer.backward(dprice)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)

print("price:", int(price))
#price: 220
print("dApple:", dapple)
#dApple: 2.2
print("dApple_num:", int(dapple_num))
#dApple_num: 110
print("dTax:", dtax)
#dTax: 200
```



### 4.2 ë§ì…ˆ ê³„ì¸µ

```python
# ë§ì…ˆ ë…¸ë“œ
class AddLayer:
    # backwardì—ì„œ forwardì˜ ë³€ìˆ˜ ì €ì¥ì´ í•„ìš”ì—†ê¸° ë•Œë¬¸ì— ë³€ìˆ˜ ì´ˆê¸°í™” ì•ˆí•´ë„ ë¨
    def __init__(self):
        pass

    def forward(self, x, y):
        out = x + y
        return out

    def backward(self, dout):
        dx = dout * 1
        dy = dout * 1
        return dx, dy
```



- ì•„ë˜ì˜ ê·¸ë¦¼ì˜ ìƒí™©ì„ êµ¬í˜„í•´ë³´ì

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-17.png" alt="fig 5-17" style="zoom: 33%;" /> 

```python
apple = 100 # ì‚¬ê³¼ ê°€ê²©
apple_num = 2 # ì‚¬ê³¼ ê°œìˆ˜
orange = 150 # ê·¤ ê°€ê²¨
orange_num = 3 # ê·¤ ê°œìˆ˜
tax = 1.1 # ì†Œë¹„ì„¸

# layerì€ ë…¸ë“œ ë§ˆë‹¤ ì„¤ì •
mul_apple_layer = MulLayer()
mul_orange_layer = MulLayer()
add_apple_orange_layer = AddLayer()
mul_tax_layer = MulLayer()

# forward
apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)
orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)
all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)
price = mul_tax_layer.forward(all_price, tax)  # (4)

# backward
dprice = 1
dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)
dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)
dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)

# forwardì™€ backward ë°˜ëŒ€ ìˆœì„œ!!

print("price:", int(price))
#price: 715
print("dApple:", dapple)
#dApple: 2.2
print("dApple_num:", int(dapple_num))
#dApple_num: 110
print("dOrange:", dorange)
#dOrange: 3.3000000000000003
print("dOrange_num:", int(dorange_num))
#dOrange_num: 165
print("dTax:", dtax)
#dTax: 650
```



---



## 5. í™œì„±í™” í•¨ìˆ˜ ê³„ì¸µ êµ¬í˜„í•˜ê¸°

> ê³„ì‚° ê·¸ë˜í”„ë¥¼ ì‹ ê²½ë§ì— ì ìš©í•´ ë³´ì.



### 5.1 ReLU ê³„ì¸µ

> í™œì„±í™” í•¨ìˆ˜ ReLUì˜ ê·¸ë˜í”„ì™€ ìˆ˜ì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤.

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 3-9.png" alt="fig 3-9" style="zoom: 25%;" /> 

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.7.png" alt="e 5.7" style="zoom:50%;" /> <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.8.png" alt="e 5.8" style="zoom:50%;" />



> ê³„ì‚° ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-18.png" alt="fig 5-18" style="zoom:50%;" /> 

- ì´ë¥¼ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì.

  ```python
  class Relu:
      def __init__(self):
          # mask(ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜) : True/Falseë¡œ êµ¬ì„±ëœ ë„˜íŒŒì´ ë°°ì—´
          self.mask = None
  
      def forward(self, x):
          # ìˆœì „íŒŒì˜ ì…ë ¥ì¸ xì˜ ì›ì†Œ ê°’ì´ 0ì´í•˜ì¸ ì¸ë±ìŠ¤ëŠ” True, 0ë³´ë‹¤ í° ì›ì†ŒëŠ” False
          self.mask = (x <= 0)
          out = x.copy()
          # 0ì´í•˜ì¸ x ì¸ë±ìŠ¤ëŠ” 0, ê·¸ ì™¸ì˜ ê°’ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
          out[self.mask] = 0
          return out
  
      def backward(self, dout):
          # mask ì›ì†Œê°€ Trueì¸ ê³³ì—ëŠ” ìƒë¥˜ì—ì„œ ì „íŒŒëœ doutì„ 0ìœ¼ë¡œ ì„¤ì •í•œë‹¤.
          dout[self.mask] = 0
          # 0ì´ìƒì¸ ê°’ë“¤ì€ ìƒë¥˜ì—ì„œ ì „íŒŒëœ doutì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•œë‹¤.
          dx = dout
          return dx
  ```

  

### 5.2 Sigmoid ê³„ì¸µ

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 3-7.png" alt="fig 3-7" style="zoom: 25%;" /> 

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.9.png" alt="e 5.9" style="zoom: 50%;" /> 

 

>  Sigmoid ë¥¼ ê³„ì‚° ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-19.png" alt="fig 5-19" style="zoom:50%;" /> 

> ì—­ì „íŒŒì˜ íë¦„ì„ ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ í•œ ë‹¨ê³„ì”© ì§šì–´ë³´ì.



#### 1 ë‹¨ê³„.  **`/`** ë…¸ë“œ

> y = 1/x ë¥¼ ë¯¸ë¶„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.10.png" alt="e 5.10" style="zoom:50%;" /> 

#### 2 ë‹¨ê³„. **`+`** ë…¸ë“œ

> ìƒë¥˜ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ í•˜ë¥˜ë¡œ ë‚´ë³´ëƒ„



#### 3 ë‹¨ê³„. **`exp`** ë…¸ë“œ

> y=exp(x) ë¥¼ ë¯¸ë¶„í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.11.png" alt="e 5.11" style="zoom:50%;" /> 



#### 4 ë‹¨ê³„. **`x`** ë…¸ë“œ

> ìˆœì „íŒŒ ë•Œì˜ ê°’ì„ ì„œë¡œ ë°”ê¿” ê³±í•¨

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-20.png" alt="fig 5-20" style="zoom:50%;" />

- ìµœì¢… ì¶œë ¥ì€ ì•„ë˜ì™€ ê°™ì´ ìˆœì „íŒŒì˜ ì…ë ¥ xì™€ ì¶œë ¥ yë§Œìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŒ

 <img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/e 5.12.png" alt="e 5.12" style="zoom:50%;" />

> ê·¸ëŸ¬ë¯€ë¡œ sigmoid ê³„ì‚° ê·¸ë˜í”„ëŠ” ì•„ë˜ì™€ ê°™ì´ ê°„ì†Œí™” í•  ìˆ˜ ìˆë‹¤. 
>
> ê°„ì†Œí™” => ì¤‘ê°„ ê³„ì‚°ë“¤ì„ ìƒëµí•  ìˆ˜ ìˆì–´ íš¨ìœ¨ì ì¸ ê³„ì‚° + ë…¸ë“œë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì„¸ì„¸í•œ ë‚´ìš© ë…¸ì¶œí•˜ì§€ ì•Šê³  ì…ë ¥ê³¼ ì¶œë ¥ì—ë§Œ ì§‘ì¤‘

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-22.png" alt="fig 5-22" style="zoom:50%;" /> 



- sigmoid ê³„ì¸µì„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì.

```python
class Sigmoid:
	
    def __init__(self):
        self.out = None

    # ìˆœì „íŒŒì˜ ì¶œë ¥ì„ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ outì— ë³´ê´€
    def forward(self, x):
        out = sigmoid(x)
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```



---



## 6. Affine/Softmax ê³„ì¸µ êµ¬í˜„í•˜ê¸°



### 6.1 Affine ê³„ì¸µ

> ì‹ ê²½ë§ì˜ ìˆœì „íŒŒ ëŒ€ ìˆ˜í–‰í•˜ëŠ” `í–‰ë ¬ì˜ ê³±`ì€ ê¸°í•˜í•™ì—ì„œëŠ” `affine transformation`ì´ë¼ í•œë‹¤.



- ë³€ìˆ˜ê°€ ìŠ¤ì¹¼ë¼ ê°’ì´ ì•„ë‹Œ í–‰ë ¬ì´ íë¥¼ ë•Œ, `Affine ê³„ì¸µì˜ ì—­ì „íŒŒ`

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-25.png" alt="fig 5-25" style="zoom: 33%;" /> 

ğŸ”¥ ì—¬ê¸°ì„œ ì¡°ì‹¬í•´ì•¼í•  ë¶€ë¶„ì€ dot ë…¸ë“œì—ì„œ ì›ì†Œë“¤ì„ ì„œë¡œ ë°”ê¿” ê³±í•˜ëŠ” ë‹¨ê³„ì—ì„œ `í–‰ë ¬ì€ ì „ì¹˜í–‰ë ¬`ë¡œ ê³±í•´ì¤Œ

â€‹		ğŸ‘‰ í–‰ë ¬ì˜ í˜•ìƒì— ì£¼ì˜!



### 6.2 ë°°ì¹˜ìš© Affine ê³„ì¸µ

> ë°ì´í„° Nê°œë¥¼ ë¬¶ì–´ ìˆœì „íŒŒí•˜ëŠ” ê²½ìš°

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-27.png" alt="fig 5-27" style="zoom: 33%;" /> 



- í¸í–¥ì„ ë”í•  ë•Œ ì£¼ì˜ ! => í¸í–¥ì€ ê° ë°ì´í„°ì— ë”í•´ì§ => ì—­ì „íŒŒ ë•ŒëŠ” ê° ë°ì´í„°ì˜ ì—­ì „íŒŒ ê°’ì´ í¸í–¥ì˜ ì›ì†Œì— ëª¨ì—¬ì•¼ í•¨

```python
# affine ê³„ì¸µ êµ¬í˜„
class Affine:
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.x = None
        # ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„
        self.dW = None
        self.db = None
        # í…ì„œ ëŒ€ì‘ ìœ„í•œ x.shapeì„ ì €ì¥í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
        self.original_x_shape = None

    def forward(self, x):
        # í…ì„œ ëŒ€ì‘
        self.original_x_shape = x.shape
        # 1ì°¨ì›ìœ¼ë¡œ reshape
        x = x.reshape(x.shape[0], -1)
        
        self.x = x
        out = np.dot(self.x, self.W) + self.b
        return out

    def backward(self, dout):
        # .T => ì „ì¹˜í–‰ë ¬
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)
        
        # ì…ë ¥ ë°ì´í„° ëª¨ì–‘ ë³€ê²½(í…ì„œ ëŒ€ì‘)
        dx = dx.reshape(*self.original_x_shape)
        return dx
```



### 6.3 Softmax-with-Loss ê³„ì¸µ

> ì¶œë ¥ì¸µì˜ ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ëŠ” ì…ë ¥ ê°’ì„ ì •ê·œí™”í•˜ì—¬ ì¶œë ¥í•¨

- Softmax-with-Loss ê³„ì¸µ : ì†ì‹¤í•¨ìˆ˜ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ í¬í•¨í•œ ê³„ì¸µ

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-29.png" alt="fig 5-29" style="zoom:50%;" />

<img src="ë°‘ì‹œë”¥ 5. ì˜¤ì°¨ì—­ì „íŒŒë²•.assets/fig 5-30.png" alt="fig 5-30" style="zoom: 33%;" /> 



```python
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None # ì†ì‹¤í•¨ìˆ˜
        self.y = None    # softmaxì˜ ì¶œë ¥
        self.t = None    # ì •ë‹µ ë ˆì´ë¸”(ì›-í•« ì¸ì½”ë”© í˜•íƒœ)
        
    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)
        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size: # ì •ë‹µ ë ˆì´ë¸”ì´ ì›-í•« ì¸ì½”ë”© í˜•íƒœì¼ ë•Œ
            dx = (self.y - self.t) / batch_size
            # ì „íŒŒí•˜ëŠ” ê°’ì„ ë°°ì¹˜ì˜ ìˆ˜ë¡œ ë‚˜ëˆ ì„œ ë°ì´í„° 1ê°œë‹¹ ì˜¤ì°¨ë¥¼ ì• ê³„ì¸µìœ¼ë¡œ ì „íŒŒ
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size
        return dx
```



---



## 7. ì˜¤ì°¨ì—­ì „íŒŒë²• êµ¬í˜„í•˜ê¸°

> ì•ì„œ êµ¬í˜„í•œ ê³„ì¸µì„ ì¡°í•©í•˜ì—¬ ì‹ ê²½ë§ì„ êµ¬ì¶•í•´ë³´ì.



### 7.1 ì‹ ê²½ë§ í•™ìŠµì˜ ì „ì²´ ê·¸ë¦¼

1. ì „ì²´ 
   - í•™ìŠµ = ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •
2. 1ë‹¨ê³„ : ë¯¸ë‹ˆë°°ì¹˜
   - í›ˆë ¨ ë°ì´í„° ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´. ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ
3. 2ë‹¨ê³„ : ê¸°ìš¸ê¸° ì‚°ì¶œ
   - ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° êµ¬í•¨. ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥ì„ ì œì‹œ
4. 3ë‹¨ê³„ : ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
   - ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ê°±ì‹ 

> 2ë‹¨ê³„ì˜ ê¸°ìš¸ê¸°ì‚°ì¶œì—ì„œ ìš°ë¦¬ëŠ” ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆê³  ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì´ìš©í•˜ë©´ ê¸°ìš¸ê¸°ë¥¼ íš¨ìœ¨ì ì´ê³  ë¹ ë¥´ê²Œ êµ¬í•  ìˆ˜ ìˆìŒ



### 7.2 ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì ìš©í•œ ì‹ ê²½ë§ êµ¬í˜„í•˜ê¸°

> 2ì¸µ ì‹ ê²½ë§ì„ êµ¬í˜„í•´ë³´ì.

```python
class TwoLayerNet:

    # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):
        # input_size : ì…ë ¥ì¸µ ë‰´ëŸ° ìˆ˜ hidden_size : ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜ ouput_size : ì¶œë ¥ì¸µ ë‰´ëŸ° ìˆ˜
    	# weight_init_std : ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ì‹œ ì •ê·œë¶„í¬ ìŠ¤ì¼€ì¼
        self.params = {} # ì‹ ê²½ë§ì˜ ë§¤ê°œë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        # ê°€ìš°ì‹œì•ˆ í‘œì¤€ ì •ê·  ë¶„í¬ ë²”ìœ„(í‰ê· 0,ë¶„ì‚°1)ì—ì„œ ë‚œìˆ˜ matrix arrayìƒì„±
        self.params['b1'] = np.zeros(hidden_size) # 0ë¡œ ì´ˆê¸°ê°’ ìƒì„±
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) 
        self.params['b2'] = np.zeros(output_size)

        # ê³„ì¸µ ìƒì„±
        self.layers = OrderedDict() # OrderedDict : ìˆœì„œ ìˆëŠ” ë”•ì…”ë„ˆë¦¬ / 3.6ë²„ì „ ë¶€í„° ì•ˆì¨ë„ ë¨
        # ìˆœì „íŒŒ ë•ŒëŠ” ì¶”ê°€í•œ ìˆœì„œëŒ€ë¡œ ê° ê³„ì¸µì˜ ë©”ì„œë“œ í˜¸ì¶œ
        # ì—­ì „íŒŒ ë•ŒëŠ” ê³„ì¸µì„ ë°˜ëŒ€ ìˆœì„œë¡œ í˜¸ì¶œ
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()
        
    # ê²°ê³¼ ì˜ˆì¸¡ ë©”ì„œë“œ
    def predict(self, x):
        for layer in self.layers.values():
            # ê³„ì¸µ ìˆœì„œëŒ€ë¡œ forward ì§„í–‰
            # affine1 -> relu1 -> affine2
            x = layer.forward(x)
        
        return x
        
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    # ì†ì‹¤í•¨ìˆ˜ ê°’ ì¶œë ¥
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    # ì •í™•ë„ ê³„ì‚°(ì •ë‹µì¸ì§€ ì•„ë‹Œì§€ íŒë³„)
    def accuracy(self, x, t):
        y = self.predict(x)
        # ìµœëŒ“ê°’ì˜ index ì–»ìŒ
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy # ì •ë‹µë¥  ë°˜í™˜
        
    # ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° ê³„ì‚°(ìˆ˜ì¹˜ ë¯¸ë¶„)    
    # x : ì…ë ¥ ë°ì´í„°, t : ì •ë‹µ ë ˆì´ë¸”
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
        
    # ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸° ê³„ì‚°(ì˜¤ì°¨ì—­ì „íŒŒë²•)  
    def gradient(self, x, t):
        
        # forward Loss
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        # affine1 -> relu -> affine2 
        layers.reverse()
        # affine2 -> relu -> affine1
        for layer in layers:
            dout = layer.backward(dout)

        # ê²°ê³¼ ì €ì¥
        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads

```



### 7.3 ì˜¤ì°¨ì—­ì „íŒŒë²•ìœ¼ë¡œ êµ¬í•œ ê¸°ìš¸ê¸° ê²€ì¦í•˜ê¸°

> ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œëŠ ë°©ë²•ì€ ë‘ê°€ì§€ê°€ ìˆë‹¤. `ìˆ˜ì¹˜ ë¯¸ë¶„`ê³¼ `ì˜¤ì°¨ì—­ì „íŒŒë²•`ì´ë‹¤.  
>
> ì˜¤ì°¨ì—­ì „íŒŒë²•ì€ ë§¤ê°œë³€ìˆ˜ê°€ ë§ì•„ë„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê¸°ë•Œë¬¸ì— ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.
>
> ìˆ˜ì¹˜ë¯¸ë¶„ì€ êµ¬í˜„ì´ ì‰½ê¸°ë•Œë¬¸ì— ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì •í™•íˆ êµ¬í˜„í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤.

- ê¸°ìš¸ê¸° í™•ì¸ : ë‘ ë°©ì‹ìœ¼ë¡œ êµ¬í•œ ê¸°ìš¸ê¸°ê°€ ì¼ì¹˜í•¨ì„ í™•ì¸í•˜ëŠ” ì‘ì—…

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# ë°ì´í„° 3ê°œë§Œ í™•ì¸
x_batch = x_train[:3]
t_batch = t_train[:3]

# ìˆ˜ì¹˜ ë¯¸ë¶„
grad_numerical = network.numerical_gradient(x_batch, t_batch)
# ì˜¤ì°¨ì—­ì „íŒŒë²•
grad_backprop = network.gradient(x_batch, t_batch)

# ê° ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· ì„ êµ¬í•œë‹¤.
for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))
    print(key + ":" + str(diff))

# W1:3.8569612445837085e-10
# b1:2.596604584942223e-09
# W2:5.251747585074162e-09
# b2:1.4031324106628106e-07
```



### 7.4 ì˜¤ì°¨ì—­ì „íŒŒë²•ì„ ì‚¬ìš©í•œ í•™ìŠµ êµ¬í˜„í•˜ê¸°

```python
# ë°ì´í„° ì½ê¸°
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
iters_num = 10000 # ë°˜ë³µ íšŸìˆ˜
train_size = x_train.shape[0]
batch_size = 100 # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# ì—í­ íšŸìˆ˜
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ë¯¸ë‹ˆë°°ì¹˜ë§Œí¼ batch í• ë‹¹
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # ê¸°ìš¸ê¸° ê³„ì‚°
    #grad = network.numerical_gradient(x_batch, t_batch) # ìˆ˜ì¹˜ ë¯¸ë¶„ ë°©ì‹
    grad = network.gradient(x_batch, t_batch) # ì˜¤ì°¨ì—­ì „íŒŒë²• ë°©ì‹(í›¨ì”¬ ë¹ ë¥´ë‹¤)
    
    # ê°±ì‹ 
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚° , í›ˆë ¨ë°ì´í„° ëª¨ë‘ ë´¤ì„ ë•Œ.
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)
```

```python
0.0829 0.0845
0.9061833333333333 0.9098
0.9230333333333334 0.9258
0.9347666666666666 0.9337
0.94415 0.9433
0.94965 0.9468
0.9559333333333333 0.9528
0.9597333333333333 0.9562
0.96455 0.9599
0.9665333333333334 0.9604
0.9702 0.9627
0.9718833333333333 0.9623
0.9738 0.9647
0.9754166666666667 0.9669
0.9777666666666667 0.9675
0.9780333333333333 0.9679
0.97965 0.9681
```
